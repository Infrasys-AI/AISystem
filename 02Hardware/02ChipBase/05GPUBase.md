# GPU 基础

GPU是Graphics Processing Unit（图形处理器）的简称，它是计算机系统中负责处理图形和图像相关任务的核心组件。GPU的发展历史可以追溯到对计算机图形处理需求的不断增长，以及对图像渲染速度和质量的不断追求。从最初的简单图形处理功能到如今的高性能计算和深度学习加速器，GPU经历了一系列重要的技术突破和发展转折。

在接下来的内容中，我们还将探讨GPU与CPU的区别，了解它们在设计、架构和用途上存在显著差异。此外，我们还将简短介绍一下AI发展和GPU的联系，并探讨GPU在各种领域的应用场景。

除了图形处理和人工智能，GPU在科学计算、数据分析、加密货币挖矿等领域也有着广泛的应用。深入了解这些应用场景有助于我们更好地发挥GPU的潜力，解决各种复杂计算问题。现在让我们深入了解GPU的发展历史、与CPU的区别、AI所需的重要性以及其广泛的应用领域。

## GPU 发展历史

=======后续要注意二级目录和一级目录的区别

在GPU发展史上，第一代GPU可追溯至1999年之前。这一时期的GPU在图形处理领域进行了一定的创新，部分功能开始从CPU中分离出来，实现了针对图形处理的硬件加速。其中，最具代表性的是几何处理引擎，即GEOMETRY ENGINE。该引擎主要用于加速3D图像处理，但相较于后来的GPU，它并不具备软件编程特性。这意味着它的功能相对受限，只能执行预定义的图形处理任务，而无法像现代GPU那样灵活地适应不同的软件需求。

然而，尽管功能有限，第一代GPU的出现为图形处理领域的硬件加速打下了重要的基础，奠定了后续GPU技术发展的基石。

========尽可能避免一个段落太长，记得分段

![第一代GPU](images/gpu/05GPUBase01.png)

=======图片哪里引用到了？图片是第一代GPU还是第二代GPU？文章要加上如下图所示这种自眼哈

第二代GPU的发展跨越了1999年到2005年这段时期，其间取得了显著的进展。1999年，英伟达发布了GeForce256图像处理芯片，这款芯片专为执行复杂的数学和几何计算而设计。与此前的GPU相比，GeForce256将更多的晶体管用于执行单元，而不是像CPU那样用于复杂的控制单元和缓存。它成功地将诸如变换与光照（TRANSFORM AND LIGHTING）等功能从CPU中分离出来，实现了图形快速变换，标志着GPU的真正出现。

随着时间的推移，GPU技术迅速发展。从2000年到2005年，GPU的运算速度迅速超越了CPU。在2001年，英伟达和ATI分别推出了GeForce3和Radeon 8500，这些产品进一步推动了图形硬件的发展。图形硬件的流水线被定义为流处理器，顶点级可编程性开始出现，同时像素级也具有了有限的编程性。

尽管如此，第二代GPU的整体编程性仍然相对有限，与现代GPU相比仍有一定差距。然而，这一时期的GPU发展为后续的技术进步奠定了基础，为图形处理和计算领域的发展打下了坚实的基础。

![GPU2](images/gpu/05GPUBase02.png)

=======图片哪里引用到了？文章要加上如下图所示这种自眼哈，两张图并列的尽可能合并成一张图

从长远看，NVIDIA的GPU在一开始就选择了正确的方向MIMD，通过G80 Series，Fermi，Kepler和Maxwell四代（下一章节会有解析）大跨步进化，形成了完善和复杂的储存层次结构和指令派发/执行管线。ATI/AMD在一开始选择了VLIW5/4，即SIMD，通过GCN向MIMD靠拢，但是进化不够完全（GCN一开始就落后于Kepler），所以图形性能和GPGPU效率低于对手。

NVIDIA 和 ATI之争本质上是shader管线与其他纹理，ROP单元配置比例之争，A认为计算用shader越多越好，计算性能强大，N认为纹理单元由于结构更简单电晶体更少，单位面积配置起来更划算，至于游戏则是越后期需要计算的比例越重。

第三代GPU的发展从2006年开始，带来了方便的编程环境创建，使得用户可以直接编写程序来利用GPU的并行计算能力。在2006年，英伟达和ATI分别推出了CUDA（Compute Unified Device Architecture）和CTM（CLOSE TO THE METAL）编程环境。

这一举措打破了GPU仅限于图形语言的局限，将GPU变成了真正的并行数据处理超级加速器。CUDA和CTM的推出使得开发者可以更灵活地利用GPU的计算能力，为科学计算、数据分析等领域提供了更多可能性。

2008年，苹果公司推出了一个通用的并行计算编程平台OPENCL（Open Computing Language）。与CUDA不同，OPENCL并不与特定的硬件绑定，而是与具体的计算设备无关，这使得它迅速成为移动端GPU的编程环境业界标准。OPENCL的出现进一步推动了GPU在各种应用领域的普及和应用，为广大开发者提供了更广阔的创新空间。

第三代GPU的到来不仅提升了GPU的计算性能，更重要的是为其提供了更便捷、灵活的编程环境，使得GPU在科学计算、深度学习等领域的应用得以广泛推广，成为现代计算领域不可或缺的重要组成部分。

以下为Nvidia和AMD的工具链架构示意图：

![GPU4](images/gpu/05GPUBase03.png)

以及二者的对比图：

![GPU6](images/gpu/05GPUBase04.jpg)

=======需要加上文字解释图中内容哈

## GPU vs CPU

现在探讨一下 CPU 和 GPU 在架构方面的主要区别， CPU 即中央处理单元（Central Processing Unit），负责处理操作系统和应用程序运行所需的各类计算任务，需要很强的通用性来处理各种不同的数据类型，同时逻辑判断又会引入大量的分支跳转和中断的处理，使得 CPU 的内部结构异常复杂。

GPU 即图形处理单元（Graphics Processing Unit），可以更高效地处理并行运行时复杂的数学运算，最初用于处理游戏和动画中的图形渲染任务，现在的用途已远超于此。两者具有相似的内部组件，包括核心、内存和控制单元。

![GPU7](images/gpu/05GPUBase05.png)

GPU 和 CPU 在架构方面的主要区别包括以下几点：

1. **并行处理能力**： CPU 拥有少量的强大计算单元（ALU），更适合处理顺序执行的任务，可以在很少的时钟周期内完成算术运算，时钟周期的频率很高，复杂的控制逻辑单元（Control）可以在程序有多个分支的情况下提供分支预测能力，因此 CPU 擅长逻辑控制和串行计算，流水线技术通过多个部件并行工作来缩短程序执行时间。GPU 控制单元可以把多个访问合并成，采用了数量众多的计算单元（ALU）和线程（Thread），大量的 ALU 可以实现非常大的计算吞吐量，超配的线程可以很好地平衡内存延时问题，因此可以同时处理多个任务，专注于大规模高度并行的计算任务。

2. **内存架构**： CPU 被缓存 Cache 占据了大量空间，大量缓存可以保存之后可能需要访问的数据，可以降低延时； GPU 缓存很少且为线程（Thread）服务，如果很多线程需要访问一个相同的数据，缓存会合并这些访问之后再去访问 DRMA，获取数据之后由 Cache 分发到数据对应的线程。 GPU 更多的寄存器可以支持大量 Thread。

3. **指令集**： CPU 的指令集更加通用，适合执行各种类型的任务； GPU 的指令集主要用于图形处理和通用计算，如 CUDA 和 OpenCL。 

4. **功耗和散热**：CPU 的功耗相对较低，散热要求也相对较低；由于 GPU 的高度并行特性，其功耗通常较高，需要更好的散热系统来保持稳定运行。

因此，CPU 更适合处理顺序执行的任务，如操作系统、数据分析等；而 GPU 适合处理需要计算密集型 (Compute-intensive) 程序和大规模并行计算的任务，如图形处理、深度学习等。在异构系统中， GPU 和 CPU 经常会结合使用，以发挥各自的优势。

## AI 发展与 GPU

GPU与人工智能（AI）的发展密不可分。2012年的一系列重要事件标志着GPU在AI计算中的崭露头角。Hinton和Alex Krizhevsky设计的AlexNet是一个重要的突破，他们利用两块英伟达GTX 580 GPU训练了两周，将计算机图像识别的正确率提升了一个数量级，并赢得了2012年ImageNet竞赛冠军。这一成就充分展示了GPU在加速深度学习模型训练中的巨大潜力。

![GPU9](images/gpu/05GPUBase07.png)

同时，谷歌和吴恩达等团队的工作也进一步强调了GPU在AI计算中的重要性。谷歌利用1000台CPU服务器完成了猫狗识别任务，而吴恩达等则只用了3台GTX680-GPU服务器，取得了同样的成果。这一对比显示了GPU在深度学习任务中的显著加速效果，进一步激发了对GPU在AI领域的广泛应用。

从2005/2006年开始，一些研究人员开始尝试使用GPU进行AI计算，但直到2012/2013年，GPU才被更广泛地接受。随着深度学习网络层次越来越深、网络规模越来越大，GPU的加速效果越来越显著。这得益于GPU相比CPU拥有更多的独立大吞吐量计算通道，以及较少的控制单元，使其在高度并行的计算任务中表现出色。

因此，GPU在AI发展中的作用愈发凸显，它为深度学习等复杂任务提供了强大的计算支持，并成为了AI计算的标配。从学术界到互联网头部厂商，都开始广泛采用GPU，将其引入到各自的生产研发环境中，为AI技术的快速发展和应用提供了关键支持。

![GPU10](images/gpu/05GPUBase08.png)

## GPU 其他应用场景

1. 游戏设备：GPU大体决定了游戏分辨率、特效能开多高，对于用户的游戏体验起到关键性作用。

2. 消费电子：目前智能手机市场占据了全球GPU市场份额的主导地位，此外，智能音箱、智能手环/手表、VR/AR眼镜等移动消费电子都是GPU潜在的市场。

3. 云端AI服务器：AI服务器通常搭载GPU、FPGA、ASIC等加速芯片，利用CPU与加速芯片的组合可以满足高吞吐量互联的需求，为自然语言处理、计算机视觉、语音交互等人工智能应用场景提供强大的算力支持，支撑AI算法训练和推理过程。

4. 自动驾驶：GPU兼具技术成本优势，已成为自动驾驶领域主流。

5. 边缘计算：在边缘计算场景，AI芯片主要承担推断任务，通过将终端设备上的传感器（麦克风阵列、摄像头等）收集的数据代入训练好的模型推理得出推断结果。

6. 智慧安防：安防摄像头发展经历了由模拟向数字化、数字化高清到现在的数字化智能方向的发展，最新的智能摄像头除了实现简单的录、存功能外，还可以实现结构化图像数据分析。

7. 加密货币：比特币等加密货币的行情火爆带动矿卡GPU需求，矿机算力的大小决定挖矿的速度，算力越大，挖矿越快。除了主流的ASIC矿机，加密货币挖矿用的最多大概是GPU矿机了。

8. 医疗影像设备：近年来，在深度学习和GPU加速运算快速发展之下，人工智能成为满足医疗影像需求日益增长的推手。

=======这段挺好的，加上自己喜欢的内容和理解的内容，多加自己理解和内容就对了

## 小结

=======最好加一段个人总结

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=527094407&bvid=BV1sM411T72Q&cid=1082002375&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

## 引用

=======注意标准格式哟

[1]. [A卡和N卡的架构有什么区别](https://www.zhihu.com/question/267104699/answer/320361801)
[2]. [一文看完GPU八大应用场景，抢食千亿美元市场](https://zhuanlan.zhihu.com/p/442395604)
