1
00:00:10,800 --> 00:00:12,266
我是 ZOMI

2
00:00:22,400 --> 00:00:23,866
做一些简单的思考

3
00:00:36,800 --> 00:00:38,800
看看 Groq 成功背后

4
00:01:06,600 --> 00:01:07,300
我们会看

5
00:01:09,566 --> 00:01:12,833
然后呢做一些简单的市场的预测

6
00:01:14,700 --> 00:01:17,966
我们现在来看看 Groq 的一个整体的效果

7
00:01:21,400 --> 00:01:23,300
确实非常的惊人

8
00:01:29,300 --> 00:01:31,966
在 artificial analysis 里面呢

9
00:01:33,600 --> 00:01:34,800
下面的这一堆呢

10
00:01:40,100 --> 00:01:43,600
而使用 Groq 的 Mistral 里面呢

11
00:01:58,300 --> 00:02:00,466
可以看到呢除了 Groq 推理性能以外啊

12
00:02:06,266 --> 00:02:08,333
越往上呢 Tokens 每秒越大

13
00:02:18,600 --> 00:02:18,900
下面

14
00:02:27,200 --> 00:02:28,500
第二个呢就是吞吐

15
00:02:47,700 --> 00:02:48,866
那正常情况下呢

16
00:02:51,466 --> 00:02:54,466
但是人类的阅读速度是有限的

17
00:03:53,600 --> 00:03:53,966
意味着

18
00:03:56,666 --> 00:03:58,033
更多的用户访问呢

19
00:04:05,800 --> 00:04:08,300
大模型推理的具体的阶段

20
00:04:18,500 --> 00:04:20,200
第二个呢叫做 Decoding

21
00:04:35,000 --> 00:04:36,466
进行一个预填充

22
00:05:07,466 --> 00:05:08,566
执行前向

23
00:05:08,566 --> 00:05:09,766
然后获取一个

24
00:05:12,700 --> 00:05:15,600
那第二个呢就比较明确了

25
00:05:27,466 --> 00:05:29,266
或者输进去一个 prompt 之后呢

26
00:05:45,266 --> 00:05:47,033
每次呢生成一个 token

27
00:05:49,300 --> 00:05:52,900
也最影响性能的一个具体的阶段

28
00:06:04,400 --> 00:06:05,566
决定整体的时

29
00:06:05,566 --> 00:06:07,466
而基本上决定整体时延呢

30
00:06:14,600 --> 00:06:16,066
性能提升的指标

31
00:06:34,400 --> 00:06:35,366
其实差很多

32
00:06:35,366 --> 00:06:36,866
那它的峰值算力呢

33
00:06:46,300 --> 00:06:47,700
就是节点类的内存

34
00:06:47,700 --> 00:06:49,166
也就是所谓的显存了

35
00:06:59,366 --> 00:07:02,133
和一些参数指标呢对比起 H100 呢

36
00:07:04,066 --> 00:07:05,133
很非常的错

37
00:07:07,800 --> 00:07:09,000
为什么它能够做到

38
00:07:10,800 --> 00:07:12,800
那么多那这个问题呢

39
00:07:18,000 --> 00:07:20,800
也希望大家呢带着这个问题去了解

40
00:07:23,600 --> 00:07:25,100
我们现在来到了第二个内容

41
00:07:25,100 --> 00:07:27,166
Groq 的一个整体的背景和概况

42
00:07:28,900 --> 00:07:32,400
首先呢 Groq 的创立呢是在 2016 年呢

43
00:07:57,866 --> 00:07:59,533
和软件定义的问题

44
00:08:01,800 --> 00:08:02,700
在后续里面呢

45
00:08:05,866 --> 00:08:07,133
在 YouTube 上面

46
00:08:13,366 --> 00:08:14,933
给他一个降龙十八掌

47
00:08:16,000 --> 00:08:17,566
Groq 在两周前的出圈呢

48
00:08:21,266 --> 00:08:23,433
他用了 500 多 tokens 每秒

49
00:08:57,166 --> 00:08:59,366
确定性的指令来去执行的

50
00:09:00,466 --> 00:09:01,466
这三条很重要

51
00:09:29,200 --> 00:09:31,766
包括我们现在的 GPU 和 NPU 啊

52
00:09:37,366 --> 00:09:39,333
会有很多严重的问题

53
00:09:40,666 --> 00:09:41,366
在概况里面呢

54
00:09:42,266 --> 00:09:44,433
Groq 的一个整体的产品形态呢

55
00:09:45,900 --> 00:09:47,400
我们先有一块芯片

56
00:10:05,000 --> 00:10:06,200
那这个时候说实话

57
00:10:16,166 --> 00:10:18,433
特别是软件定义硬件

58
00:10:20,266 --> 00:10:23,266
现在我们来看看整个 TSP 的一个芯片

59
00:10:24,800 --> 00:10:27,200
最左下角呢有一个 PCIe Gen4

60
00:10:44,566 --> 00:10:47,733
这里面呢这一块就是 SRAM 左右对称

61
00:10:49,366 --> 00:10:52,666
也就是没有 HBM 这种大内存的

62
00:10:52,666 --> 00:10:54,766
那整体的制造工艺呢是 14 纳米

63
00:11:03,266 --> 00:11:04,233
用到纳米级别

64
00:11:15,200 --> 00:11:16,100
芯片间呢

65
00:11:23,266 --> 00:11:24,333
这个 C2C 呢

66
00:11:28,066 --> 00:11:28,666
那现在呢

67
00:11:35,000 --> 00:11:38,366
主传输我们的具体的指令一共有 20 条

68
00:11:40,400 --> 00:11:41,200
这里面下面呢

69
00:11:44,666 --> 00:11:47,733
那这个 ICU 呢非彼 ICU

70
00:11:56,100 --> 00:11:59,400
我们可以执行 40 万次整数的乘法呢

71
00:12:14,566 --> 00:12:17,266
左边跟右边是完全的对称的

72
00:12:17,566 --> 00:12:19,233
这个数据流动呢也非常的明确

73
00:12:27,566 --> 00:12:29,533
这个对我们的 shape 进行转换

74
00:12:36,900 --> 00:12:39,500
数据呢是这么不断的循环去执行的

75
00:12:39,500 --> 00:12:41,600
所以它的数据流呢是非常的有规律

76
00:12:43,400 --> 00:12:44,000
那这里面呢

77
00:12:46,100 --> 00:12:47,666
一个是 VXM

78
00:12:47,666 --> 00:12:48,166
我们向量

79
00:12:51,766 --> 00:12:53,066
而整个 MXM 呢

80
00:12:53,066 --> 00:12:55,166
位于我们的芯片的两边呢

81
00:13:02,200 --> 00:13:03,566
我们现在来看看软件呢

82
00:13:08,200 --> 00:13:09,800
实际上整体的吞吐时延呢

83
00:13:09,800 --> 00:13:12,500
是由具体的编译器来去决定的

84
00:13:19,200 --> 00:13:21,100
来去执行每一个超长指令

85
00:13:32,500 --> 00:13:33,466
我们现在来重点呢

86
00:13:38,200 --> 00:13:39,100
这 Unit 做什么呢

87
00:13:40,066 --> 00:13:41,133
我们把整个 Unit 呢

88
00:13:43,966 --> 00:13:44,933
有四个呢

89
00:14:00,200 --> 00:14:00,300
都

90
00:14:00,300 --> 00:14:03,766
把我们刚才的 320B 的一个超长指令字

91
00:14:08,900 --> 00:14:11,200
那可以看到 VXM 呢放在中间

92
00:14:16,100 --> 00:14:18,900
然后进行一个矩阵的运算之后存回来

93
00:14:21,800 --> 00:14:23,000
我们的软件已经定好

94
00:14:23,000 --> 00:14:24,800
我们的硬件的具体的形态了

95
00:14:29,000 --> 00:14:29,866
那很明显的

96
00:14:44,266 --> 00:14:47,666
我们的指令呢是流失性的向上传递的

97
00:14:51,466 --> 00:14:52,466
实行我们的计算

98
00:14:52,466 --> 00:14:54,433
同步去执行的

99
00:15:02,400 --> 00:15:03,500
那这里面流动的时候呢

100
00:15:13,700 --> 00:15:15,800
既然谈到一个很重要的概念呢

101
00:15:15,800 --> 00:15:17,400
刚才讲到的就是一个 Superlane

102
00:15:20,500 --> 00:15:21,166
实际上呢

103
00:15:32,600 --> 00:15:35,200
我们刚才讲到了有好多个单元呢

104
00:15:39,566 --> 00:15:41,566
一个在中间的向量处理单元

105
00:15:41,566 --> 00:15:43,733
和两个数据交互的单元

106
00:15:45,700 --> 00:15:47,300
就是像上面这个图展开

107
00:15:47,300 --> 00:15:49,500
了解完这几个单元之后呢

108
00:15:52,300 --> 00:15:54,100
数据呢在流水线上面呢

109
00:15:57,700 --> 00:15:58,500
指令的时候

110
00:16:01,300 --> 00:16:03,100
从 Superlane0 呢进行流入

111
00:16:22,166 --> 00:16:24,266
发射一条通知的指令

112
00:16:24,266 --> 00:16:26,933
给到整体的 Superlane 去执行

113
00:16:28,400 --> 00:16:30,500
接着我们打开一个 superlane 的

114
00:16:31,766 --> 00:16:32,833
刚才讲到了 superlane

115
00:16:38,866 --> 00:16:40,333
每条 superlane 的位宽呢

116
00:16:41,900 --> 00:16:45,400
总体的数据流的位宽呢是 512B 的

117
00:16:47,700 --> 00:16:50,200
就是我们只看右边的这个模块

118
00:16:50,200 --> 00:16:51,400
可以看到了

119
00:16:55,500 --> 00:16:57,000
不同的时间段呢

120
00:17:05,000 --> 00:17:09,500
原来的 Superlane 就在执行下一个指令组

121
00:17:19,100 --> 00:17:20,666
哦那这个创始呢

122
00:17:43,100 --> 00:17:44,266
又没有太多的缓存

123
00:17:46,066 --> 00:17:47,633
它怎么去保证执行的

124
00:18:01,066 --> 00:18:02,666
所以它需要明确的去安排

125
00:18:06,700 --> 00:18:09,300
那这个取指呢更多是指人工取指

126
00:18:13,000 --> 00:18:15,266
就变成了软件上面对编译器的优化

127
00:18:15,266 --> 00:18:18,133
编译器对芯片的执行性能的优化了

128
00:18:31,800 --> 00:18:34,666
首先呢我们打开整个 Superlane 呢

129
00:18:46,166 --> 00:18:47,566
专门做矩阵运算的

130
00:19:32,800 --> 00:19:34,666
我们权重呢会预先的存放码

131
00:19:36,566 --> 00:19:38,666
display 到各个单元里面

132
00:19:54,766 --> 00:19:56,066
VXM 放在中间

133
00:20:04,700 --> 00:20:05,700
对齐的 4B 呢

134
00:20:06,866 --> 00:20:10,133
来执行 32B 的一个具体的计算

135
00:20:15,700 --> 00:20:16,700
浮点的运算

136
00:20:21,466 --> 00:20:23,866
这么一个计算单元的

137
00:20:45,866 --> 00:20:48,166
很小啊也就是一个 Superlane

138
00:20:48,166 --> 00:20:49,433
里面的一个 Superlane 呢

139
00:20:53,666 --> 00:20:54,566
不是指两面呢

140
00:20:54,566 --> 00:20:55,933
因为它是对称的

141
00:21:07,600 --> 00:21:09,900
组合起来有 110MB

142
00:21:23,266 --> 00:21:23,733
那注意呢

143
00:21:30,166 --> 00:21:31,233
通过流式的寄存器呢

144
00:21:49,800 --> 00:21:50,966
啊因为一个是向量

145
00:21:51,766 --> 00:21:53,933
我们数据存的时候存该存

146
00:22:05,900 --> 00:22:06,866
那整体呢

147
00:22:13,700 --> 00:22:14,900
怎么去用我们的网络模型

148
00:22:22,900 --> 00:22:25,000
它根据硬件呢会有具体的命名

149
00:22:39,100 --> 00:22:42,400
还有 Address offset 具体的地址的偏移

150
00:22:42,400 --> 00:22:44,700
那对应的就是右下角这个内容了

151
00:22:51,866 --> 00:22:53,133
刚才其实简单的讲了讲

152
00:22:56,200 --> 00:22:57,766
中间的一个灰色的位置

153
00:23:02,100 --> 00:23:04,066
主要是用来保存我们的操作数据的

154
00:23:11,700 --> 00:23:14,500
就是 ECC 的一个具体的校验

155
00:23:25,400 --> 00:23:27,200
其实都有对应的指令

156
00:23:27,200 --> 00:23:30,566
那最核心的还是中间的这四个

157
00:23:39,200 --> 00:23:40,000
那这个呢

158
00:23:58,166 --> 00:24:00,733
只是它一个微核架构的一个概念

159
00:24:06,600 --> 00:24:07,600
整体的端到端的

160
00:24:10,666 --> 00:24:12,166
都到了微秒级别

161
00:24:15,866 --> 00:24:18,566
是我们的寄存器去做控制的

162
00:24:18,566 --> 00:24:21,333
硬件呢实际上是没有流露太多的 buffer

163
00:24:22,200 --> 00:24:23,700
我们是要集群呢

164
00:24:23,700 --> 00:24:25,066
集群也需要进行一个

165
00:24:28,100 --> 00:24:30,100
需要考虑每一个物理电路上面的

166
00:24:30,100 --> 00:24:31,900
所有的信号的带宽和延迟

167
00:24:54,200 --> 00:24:56,900
单个芯片里面是没有全局时钟的

168
00:25:23,500 --> 00:25:24,800
是同步的

169
00:25:26,500 --> 00:25:28,166
就是为了保持我们通讯正确性的

170
00:25:28,166 --> 00:25:30,033
所以他会有带时间戳的

171
00:25:44,666 --> 00:25:45,433
把所有东西呢

172
00:25:47,700 --> 00:25:48,866
那在整个硬件

173
00:25:52,066 --> 00:25:54,433
因此呢会加入一些硬件的对接机器呢

174
00:25:58,700 --> 00:26:00,566
那了解完这些信息之后呢

175
00:26:06,600 --> 00:26:08,066
硬件就好解决了

176
00:26:10,800 --> 00:26:12,266
鸟瞰图可以看到

177
00:26:14,866 --> 00:26:16,633
里面执行的是一个全连接

178
00:26:20,200 --> 00:26:21,800
就通过我们刚才的 C2C 呢

179
00:26:24,200 --> 00:26:26,500
我们基本上形态呢就变成这样了

180
00:26:26,500 --> 00:26:28,000
一个全连接的电路的中间呢

181
00:26:28,000 --> 00:26:29,066
是非常的多的

182
00:26:46,966 --> 00:26:48,866
用的是 Dragonfly

183
00:26:52,100 --> 00:26:53,400
总体的拓扑形态呢

184
00:26:55,066 --> 00:26:56,566
但是呢遵循呢 dragonfly

185
00:26:58,300 --> 00:27:00,000
变成一个 rack 之后呢

186
00:27:01,900 --> 00:27:03,100
基本上组网的方式呢

187
00:27:03,100 --> 00:27:04,600
就变成这一种了

188
00:27:07,700 --> 00:27:08,600
带宽了小

189
00:27:15,900 --> 00:27:18,300
当 TSP 的节点越来越多的时候呢整

190
00:27:24,100 --> 00:27:25,700
其实降的很低

191
00:27:31,366 --> 00:27:34,333
TPU 的一个集群里面通讯带宽这么低

192
00:27:37,300 --> 00:27:38,866
大模型的推理需要

193
00:27:38,866 --> 00:27:41,366
大集群需要很多张卡去配合

194
00:27:48,800 --> 00:27:50,566
能够比英伟达超过 10 倍呢

195
00:27:51,466 --> 00:27:53,266
你讲到这我听不懂

196
00:27:56,600 --> 00:27:57,500
有了新的内容

197
00:27:57,500 --> 00:28:00,466
就性能吊打 NV 的一个具体的总结

198
00:28:02,700 --> 00:28:03,766
还有集群组网啊

199
00:28:05,400 --> 00:28:06,600
现在呢来了解一下

200
00:28:10,000 --> 00:28:12,000
我们首先还是回到了刚才

201
00:28:13,766 --> 00:28:16,833
看我们的吞吐跟时延的性能的曲线

202
00:28:20,966 --> 00:28:21,933
在大语言模型里面

203
00:28:22,700 --> 00:28:25,200
是更好的吞吐更低的时延

204
00:28:25,200 --> 00:28:26,300
所以我们一般来说呢

205
00:28:26,300 --> 00:28:27,566
看一下左上角

206
00:28:27,566 --> 00:28:29,633
尽可能的往左上角靠

207
00:28:42,100 --> 00:28:44,666
也可以去到 ZOMI 的 deep learning system

208
00:28:46,066 --> 00:28:47,333
去了解一下

209
00:28:51,600 --> 00:28:53,900
特别是 AI 芯片的一些基础

210
00:28:53,900 --> 00:28:55,300
了解一下相关的概念

211
00:28:55,300 --> 00:28:57,800
你就知道这条曲线代表的是什么

212
00:28:57,800 --> 00:28:58,500
那么现在呢

213
00:28:59,666 --> 00:29:01,633
在整个曲线的过程当中呢

214
00:29:05,366 --> 00:29:07,533
我们假设它的推理的时间呢

215
00:29:10,166 --> 00:29:12,833
随着我们系统里面的数据越来越多

216
00:29:30,000 --> 00:29:32,366
那我们刚才讲到的时延是固定的

217
00:29:41,300 --> 00:29:44,266
我们的是在我们峰值算力增加的前提

218
00:29:44,266 --> 00:29:47,233
接着呢我们看一下另外一种情况啊

219
00:29:49,666 --> 00:29:50,533
我们每次呢

220
00:29:56,500 --> 00:29:58,600
也就是一次生成一个 token

221
00:29:59,066 --> 00:30:00,266
我把一个当前的 token 可能

222
00:30:03,266 --> 00:30:05,133
可能给到下一个大语言模型

223
00:30:20,866 --> 00:30:22,066
搬来搬进

224
00:30:27,900 --> 00:30:29,966
也就是我们的带宽的 bound 了

225
00:30:37,400 --> 00:30:38,366
采用自回归的方式

226
00:30:43,400 --> 00:30:45,466
我们的时延往右边偏

227
00:30:49,900 --> 00:30:52,466
就是增加我们的片内的带宽

228
00:30:55,200 --> 00:30:58,666
就是 Groq 的一个非常成功的关键

229
00:30:58,666 --> 00:30:59,733
也是它的推理时间

230
00:31:26,966 --> 00:31:28,333
比它高了很多

231
00:31:34,900 --> 00:31:37,000
做到的时延非常的低

232
00:31:49,100 --> 00:31:50,900
同样的我们对 Groq 这颗芯片呢

233
00:32:04,200 --> 00:32:05,366
快得离谱

234
00:32:06,200 --> 00:32:07,500
那第二呢就是 Groq 呢

235
00:32:07,500 --> 00:32:10,900
这个芯片呢真正意义做到了 software defined

236
00:32:23,500 --> 00:32:24,600
2020 年的芯片呢

237
00:32:27,300 --> 00:32:30,200
这个比大部分 DSA 都要强无数倍

238
00:32:30,200 --> 00:32:32,000
你别带那么多芯片干嘛

239
00:32:34,600 --> 00:32:36,000
你芯片不可获取吗

240
00:32:36,000 --> 00:32:36,700
在国内那

241
00:32:40,000 --> 00:32:41,866
而且很重要的就是 Groq 架构了

242
00:32:48,800 --> 00:32:51,200
最后通过软件的编译成了

243
00:32:58,666 --> 00:32:59,866
有很多的要求

244
00:33:01,700 --> 00:33:02,700
各种各样的东西呢

245
00:33:04,200 --> 00:33:05,566
或者 Wifi 的工程

246
00:33:05,566 --> 00:33:08,266
寻址整体工程量是很大的

247
00:33:11,966 --> 00:33:13,666
实际上端侧很多产品呢

248
00:33:15,900 --> 00:33:17,400
所以编译精细化的一个设计

249
00:33:23,566 --> 00:33:25,333
我们的 DSA 架构呢

250
00:33:28,300 --> 00:33:30,266
所以也可以非常值得的借鉴

251
00:33:32,200 --> 00:33:34,500
就是每一片的 SRAM 容量呢

252
00:33:34,500 --> 00:33:35,866
为 220 MB 呢

253
00:33:39,200 --> 00:33:41,300
其实也需要很大的内存的

254
00:33:49,500 --> 00:33:51,166
Scaling low 的一个具体的发展呢

255
00:33:52,700 --> 00:33:55,300
希望 Groq 可能能够出第二代呀

256
00:34:07,600 --> 00:34:09,500
ZOMI 呢不是非常的认同

257
00:34:10,966 --> 00:34:11,433
那这里面呢

258
00:34:28,366 --> 00:34:29,533
做了各种各样的测算

259
00:34:30,466 --> 00:34:31,533
也可以了解一下

260
00:34:36,166 --> 00:34:37,733
这里面的 ZOMI 就有一个疑问了

261
00:34:39,200 --> 00:34:41,666
有 30%-40%呢是 HBM

262
00:34:43,100 --> 00:34:44,666
那 Groq 的大部分都是 SRAM

263
00:34:46,400 --> 00:34:48,400
其他的内容呢要贵很多

264
00:34:48,400 --> 00:34:50,700
所以它的张卡的成本呢是可以预测的

265
00:34:50,700 --> 00:34:51,600
500 张 Groq

266
00:34:55,066 --> 00:34:56,033
但是 60 万美金

267
00:34:59,066 --> 00:34:59,733
两台英伟达

268
00:35:02,700 --> 00:35:03,800
肯定不行啊

269
00:35:17,666 --> 00:35:18,766
嗯 mackler

270
00:35:19,700 --> 00:35:22,466
而且分析了一个非常详细的一个报告

271
00:35:22,466 --> 00:35:23,433
啊就知乎

272
00:35:25,166 --> 00:35:27,033
或者一个公众号转载啊

273
00:35:32,900 --> 00:35:35,966
一个 prefill 跟 decode 的一个使用的阶段

274
00:35:36,566 --> 00:35:39,433
确实你每个角度都有问题

275
00:35:46,100 --> 00:35:48,900
我们现在 AI 手机 AIPC 其实是越来越火了

276
00:35:49,966 --> 00:35:52,666
要求我们的时延必须做到 2 毫秒

277
00:35:57,800 --> 00:35:59,000
所以在这种情况以下

278
00:36:04,800 --> 00:36:06,266
H100 Groq 都能满足

279
00:36:12,400 --> 00:36:14,500
ZOMI 呢就做了一个简单的总结

280
00:36:18,700 --> 00:36:20,000
大部分都会对时延

281
00:36:38,066 --> 00:36:38,333
这么

282
00:36:53,066 --> 00:36:54,266
因为两者之间呢

283
00:36:56,500 --> 00:36:59,366
还有集群组网的形态完全都不一样

284
00:37:00,866 --> 00:37:01,833
一个用 4 纳米

285
00:37:05,800 --> 00:37:07,700
然后我们看看芯片的尺寸呢

286
00:37:12,766 --> 00:37:14,566
Groq 一个小片出来

287
00:37:26,400 --> 00:37:27,600
不是便宜

288
00:37:32,200 --> 00:37:34,100
也别这么去算成本

289
00:37:35,300 --> 00:37:36,866
不过呢 Groq 技术发展路线呢

290
00:37:36,866 --> 00:37:37,766
其实也有很多问题

291
00:37:40,166 --> 00:37:43,133
说实话 20 年图片的芯片

292
00:37:45,100 --> 00:37:46,200
还在搞软件

293
00:37:52,100 --> 00:37:52,666
这个呢

294
00:37:52,666 --> 00:37:55,333
功耗和散热可能是个很大的问题啊

295
00:38:11,500 --> 00:38:12,000
那最后呢

296
00:38:19,800 --> 00:38:23,366
不会看好 Groq 这种的 DSA domain specific architecture

297
00:38:23,366 --> 00:38:24,566
但是长期来说呢

298
00:38:26,200 --> 00:38:27,400
特别是在 AI-Agent 呢

299
00:38:30,200 --> 00:38:31,900
就是主要的矛盾呢

300
00:38:31,900 --> 00:38:34,766
并不是 LLM 的一个 prefill 跟 decoding 的矛盾

301
00:39:02,366 --> 00:39:03,766
桌面觉得很有可能

302
00:39:05,666 --> 00:39:08,233
就是大家都在吐槽这个 Groq 的

303
00:39:14,800 --> 00:39:15,800
而不是 HBM

304
00:39:27,200 --> 00:39:28,666
老在吐槽人家

305
00:39:45,500 --> 00:39:47,400
因为推理服务的提供商呢提

306
00:39:48,766 --> 00:39:49,533
那这个时候呢

307
00:39:52,100 --> 00:39:54,300
如果买 NV 来提供推理服务呢

308
00:39:56,866 --> 00:39:58,166
到底想当卖铲子呢

309
00:40:03,400 --> 00:40:04,200
谢谢各位

310
00:05:47,000 --> 00:05:49,300
这个就是大语言模型最常用

311
00:12:29,500 --> 00:12:30,700
就给到 MXM

312
00:13:44,900 --> 00:13:47,666
一个是 MXM 专门算矩阵的

313
00:16:40,300 --> 00:16:41,900
大概是 328B

314
00:17:47,600 --> 00:17:49,700
是一个确定性的计算呢

315
00:18:18,100 --> 00:18:19,000
那接下来呢

316
00:20:10,100 --> 00:20:12,300
那具体的计算它用来什么作用呢

317
00:21:31,200 --> 00:21:33,466
去对我们的数据呢进行一个缓存

318
00:25:30,000 --> 00:25:31,466
一个发送和接收

319
00:26:16,600 --> 00:26:18,700
以及 Full Mesh 的一个状态

320
00:28:47,300 --> 00:28:48,466
结构体系的

321
00:29:01,600 --> 00:29:03,900
我们假设固定的时延

322
00:30:59,700 --> 00:31:01,400
能够比英伟达超过十倍的

323
00:34:31,500 --> 00:34:33,866
Semianalysis 的一个具体分析

324
00:34:59,700 --> 00:35:02,700
的 H100 能跑出 500 多 token 每秒吗

325
00:36:38,300 --> 00:36:40,700
还需要等待大模型真正的商业化落地

326
00:37:01,800 --> 00:37:03,266
你咋打嘛

327
00:37:43,100 --> 00:37:45,100
到了 23 年 24 年呢

328
00:00:06,866 --> 00:00:08,666
酱香拿铁来一杯

329
00:00:14,900 --> 00:00:17,800
地表最强的推理芯片 Groq

330
00:00:17,800 --> 00:00:19,666
它整体的一个技术分析

331
00:00:19,666 --> 00:00:22,433
然后呢再看看它的整体的产业洞察

332
00:00:25,400 --> 00:00:27,300
可能时间就会有点长了

333
00:00:27,300 --> 00:00:28,766
我们将会分开 4 个内容

334
00:00:28,766 --> 00:00:31,033
跟大家去分享和介绍

335
00:00:31,600 --> 00:00:34,966
就是推理的两个很重要的指标

336
00:00:35,000 --> 00:00:36,800
延迟和吞吐

337
00:00:50,666 --> 00:00:52,166
特别是软件定义

338
00:00:55,900 --> 00:00:58,066
最后呢还会进行一个 scale

339
00:00:58,066 --> 00:01:00,433
l 变成一个集群的解读

340
00:01:00,500 --> 00:01:02,366
在最后我们肯定忘不了

341
00:01:02,366 --> 00:01:04,433
对整个产业和技术

342
00:01:07,300 --> 00:01:09,566
推理算力的一个具体的发展情况

343
00:01:14,100 --> 00:01:14,700
好了事不宜迟

344
00:01:17,966 --> 00:01:21,266
从这个效果我们可以看到 Groq 整体的出字的速率呢

345
00:01:27,266 --> 00:01:29,333
那整体的推理速度的对比呢

346
00:01:34,800 --> 00:01:40,100
基本上每 Tokens 每秒呢是低于 150 左右的

347
00:01:43,600 --> 00:01:47,066
整体的推理的性能呢去到了 500Tokens 每秒

348
00:01:47,066 --> 00:01:49,233
效果呢夸张的非常的多

349
00:01:51,100 --> 00:01:52,400
十倍有一些

350
00:01:52,400 --> 00:01:53,866
那整体的推理速度呢

351
00:01:53,866 --> 00:01:55,766
确实在很多的测评当中呢

352
00:01:55,766 --> 00:01:58,066
Groq 也是遥遥领先呢

353
00:02:02,900 --> 00:02:03,866
那右边的这个呢

354
00:02:03,866 --> 00:02:06,266
就是越往右呢就是价格越贵

355
00:02:08,300 --> 00:02:10,300
可以看到 Groq 的整体的形态呢

356
00:02:10,300 --> 00:02:11,566
不管是推理的价格

357
00:02:11,566 --> 00:02:12,566
推理的性能呢

358
00:02:12,566 --> 00:02:16,333
也是非常的嗯比较有性价比的

359
00:02:18,900 --> 00:02:21,466
我们马上进入到第一个很重要的内容

360
00:02:21,466 --> 00:02:23,933
就说真正影响大模型推理性能的指标

361
00:02:26,300 --> 00:02:27,200
我们叫 latency

362
00:02:28,500 --> 00:02:29,800
我们叫做 throughtput

363
00:02:30,100 --> 00:02:32,066
现在我们逐个指标的来去看一下

364
00:02:32,066 --> 00:02:33,033
所谓的延迟呢

365
00:02:34,600 --> 00:02:36,166
给到大语言模型之后呢

366
00:02:36,166 --> 00:02:38,366
获得一个结果或者答案呃

367
00:02:38,366 --> 00:02:40,066
的一个具体的时间

368
00:02:40,066 --> 00:02:40,933
那这个时间呢

369
00:02:44,100 --> 00:02:45,866
这里面呢就是 tokens 每秒

370
00:02:45,866 --> 00:02:47,733
作为一个延迟的指标

371
00:02:48,866 --> 00:02:51,466
其实我们希望延迟越低越好嘛

372
00:02:58,366 --> 00:03:01,833
应该是大于 10-15 tokens 每秒比较合理

373
00:03:02,066 --> 00:03:04,166
H100 呢其实现在做的比较好

374
00:03:04,166 --> 00:03:05,566
或者经过优化之后呢

375
00:03:05,566 --> 00:03:08,266
大概能到 50Tokens 每秒

376
00:03:08,266 --> 00:03:10,333
甚至到 100Tokens 每秒

377
00:03:12,700 --> 00:03:14,266
针对输入的文本

378
00:03:14,266 --> 00:03:16,333
然后给出一个具体的答案

379
00:03:20,266 --> 00:03:22,933
接着我们看一下第二个指标就是 Throughput

380
00:03:24,866 --> 00:03:27,333
那它主要是指 LLM 这大语言模型

381
00:03:28,666 --> 00:03:30,566
够处理的数据量

382
00:03:30,566 --> 00:03:33,166
具体的单位呢就是每秒的查询数

383
00:03:33,166 --> 00:03:35,533
Query/second 实际上呢

384
00:03:37,166 --> 00:03:39,866
在实际应用当中的效率和成本

385
00:03:39,866 --> 00:03:41,266
这两个呢是最核心的

386
00:03:41,266 --> 00:03:43,833
用户体验呢更多的是时延引起的

387
00:03:44,600 --> 00:03:46,800
我们希望吞吐肯定越大越好嘛

388
00:03:46,800 --> 00:03:48,900
但是呢在有限的资源下面呢

389
00:03:49,166 --> 00:03:50,033
我们一般情况下呢

390
00:03:51,866 --> 00:03:53,633
因为我们的吞吐大了

391
00:03:53,966 --> 00:03:56,666
我们可以支持更多并发的用户的访问

392
00:03:58,000 --> 00:03:59,900
我们每秒查询的数率呢

393
00:03:59,900 --> 00:04:02,200
就会变得更满更大

394
00:04:02,600 --> 00:04:04,666
我们现在讲完两个指标之外呢

395
00:04:04,666 --> 00:04:05,833
我们还要了解

396
00:04:08,300 --> 00:04:10,066
你得了解大模型推理

397
00:04:10,066 --> 00:04:12,866
你才知道为什么它的推理性能这么好

398
00:04:12,866 --> 00:04:16,033
那大模型推理呢实际上分为两个步骤

399
00:04:17,200 --> 00:04:18,500
我们叫做预填充

400
00:04:20,200 --> 00:04:22,166
我们叫做解码的阶段

401
00:04:22,166 --> 00:04:24,466
那现在我们简单的把这两个阶段呢

402
00:04:24,466 --> 00:04:26,633
分开来去了解一下

403
00:04:28,500 --> 00:04:30,466
实际上呢是执行一次前项

404
00:04:30,466 --> 00:04:32,566
那执行一次前项的过程当中呢

405
00:04:32,566 --> 00:04:35,033
我们就会对整个大语言模型呢

406
00:04:36,466 --> 00:04:37,433
所谓的预填充呢

407
00:04:39,500 --> 00:04:41,400
我们经常呢会有一些 prompt

408
00:04:41,400 --> 00:04:44,266
prompt 是提前输进去我们的网络模型的

409
00:04:44,266 --> 00:04:45,266
例如我是你老爸

410
00:04:45,266 --> 00:04:46,466
你是儿子

411
00:04:46,466 --> 00:04:47,833
说话放尊重一点

412
00:04:49,766 --> 00:04:50,366
那这个呢

413
00:04:50,366 --> 00:04:53,666
是我们给到大语言模型的一个引导词

414
00:04:53,666 --> 00:04:56,333
我们希望大语言模型回答问题呢

415
00:04:59,666 --> 00:05:01,166
不要太过于强势

416
00:05:01,166 --> 00:05:02,833
不要用其他的回答

417
00:05:04,800 --> 00:05:06,466
对这种提示词进行预处理呢

418
00:05:06,466 --> 00:05:07,466
我们叫做 perfill

419
00:05:09,766 --> 00:05:12,466
kv cache key 跟 value 的缓存

420
00:05:15,600 --> 00:05:18,566
decoding 的阶段也要生成第一个 token

421
00:05:18,566 --> 00:05:20,633
之后呢采用自回归的方式

422
00:05:22,366 --> 00:05:23,633
什么叫 decoding

423
00:05:25,700 --> 00:05:27,466
就是我们输进去一段话

424
00:05:30,866 --> 00:05:33,633
接着呢我们把下一个单词连在一起

425
00:05:33,800 --> 00:05:35,000
输给我们的大圆模型

426
00:05:35,000 --> 00:05:36,600
再输出下一个单词

427
00:05:36,600 --> 00:05:37,066
接着呢

428
00:05:37,066 --> 00:05:39,566
我们把之前的所有单词连在一起

429
00:05:39,566 --> 00:05:41,266
输给我们的大圆模型

430
00:05:41,266 --> 00:05:43,233
再输出下一个单词

431
00:05:44,266 --> 00:05:45,266
叫做自回归

432
00:05:54,900 --> 00:05:55,800
了解到这里为止呢

433
00:05:55,800 --> 00:05:59,400
我们就知道大语言模型的推理的时延呢

434
00:05:59,400 --> 00:06:01,500
主要是由 perfill 和 decoding 阶段

435
00:06:01,500 --> 00:06:02,900
来来去决定的

436
00:06:03,566 --> 00:06:04,433
而基本上呢

437
00:06:07,466 --> 00:06:10,833
是由 decoding 阶段输出的序列的 token 数呢

438
00:06:13,100 --> 00:06:14,600
而这里面的 Groq 呢

439
00:06:16,066 --> 00:06:18,333
或者性能提升的重点呢是在这里

440
00:06:21,400 --> 00:06:22,900
而是 PK 行不行

441
00:06:22,900 --> 00:06:24,766
不就这么一次的问题吗

442
00:06:25,166 --> 00:06:28,466
现在我们来看一下整个 Google 的情况

443
00:06:28,600 --> 00:06:30,000
特别是芯片的一些内容

444
00:06:30,000 --> 00:06:32,200
可以看到 Groq 呢整体的制造工艺呢

445
00:06:33,200 --> 00:06:34,400
对比起 100 的 4 纳米呢

446
00:06:36,866 --> 00:06:40,166
最后 FP16 呢只能到达 188 Tflops

447
00:06:40,200 --> 00:06:42,866
H100 能够去到 989 Tflops

448
00:06:42,866 --> 00:06:44,633
节点内带宽大概都差不多

449
00:06:49,166 --> 00:06:51,866
GOP 只有 1.76GB

450
00:06:51,866 --> 00:06:55,033
而 H100 呢有 640GB

451
00:06:55,666 --> 00:06:56,633
从下面这个表了

452
00:07:02,100 --> 00:07:04,066
说实话矬了很多

453
00:07:05,100 --> 00:07:06,400
那我们有一个疑问呢

454
00:07:06,400 --> 00:07:07,800
就是推理的实验呢

455
00:07:09,000 --> 00:07:10,800
比英伟达还要高出十倍

456
00:07:12,800 --> 00:07:14,900
我们将会在后面的所有的视频里面呢

457
00:07:14,900 --> 00:07:16,300
会继续解读的时候呢

458
00:07:16,300 --> 00:07:18,000
跟大家一起回答的

459
00:07:27,166 --> 00:07:28,933
简单的去过一下

460
00:07:32,400 --> 00:07:34,700
由谷歌的 TPU 的架构师呢

461
00:07:34,700 --> 00:07:37,166
ross 呢跟他的团队经理创立的

462
00:07:37,166 --> 00:07:38,333
Groq 为什么叫 Groq

463
00:07:39,100 --> 00:07:40,300
名字呢奇奇怪怪

464
00:07:40,300 --> 00:07:42,900
但是呢实际上他经过了四年之后呢

465
00:07:42,900 --> 00:07:44,400
才发表他第一篇文章

466
00:07:44,400 --> 00:07:47,000
也就 2020 年的时候呢发表了两篇

467
00:07:47,300 --> 00:07:47,500
然后呢

468
00:07:47,500 --> 00:07:50,566
分别是关于芯片的和一个 data four fold

469
00:07:50,566 --> 00:07:52,166
在 2022 年的时候呢

470
00:07:52,166 --> 00:07:55,666
其实啊这个时候已经经历了六年了

471
00:07:55,666 --> 00:07:57,866
他发表了他的一个 scale out 的问题

472
00:07:59,500 --> 00:08:01,800
直到 23 年的第七年的时候呢

473
00:08:02,700 --> 00:08:04,666
做了一个简单的分享报告

474
00:08:04,666 --> 00:08:05,866
大家也可以去看一下

475
00:08:08,066 --> 00:08:11,066
这次真正出圈的是 8 年后了

476
00:08:11,066 --> 00:08:13,366
创业的 8 年后确实坚持很不容易

477
00:08:17,566 --> 00:08:20,433
是因为在整体在 Mistral-MOE7*8B

478
00:08:23,400 --> 00:08:27,300
性能时间时延非常的高啊

479
00:08:28,700 --> 00:08:29,000
于是呢

480
00:08:29,000 --> 00:08:31,066
ZOMI 继续翻了一下 Groq 整体的特性

481
00:08:31,066 --> 00:08:31,433
说实话

482
00:08:33,500 --> 00:08:34,800
但是 ZOMI 觉得最牛逼

483
00:08:34,800 --> 00:08:37,666
最重要的就是他的 SRAM 的一个 memory 啊

484
00:08:37,666 --> 00:08:40,433
能够去到 80TB 每秒的一个具体带宽

485
00:08:42,766 --> 00:08:44,533
那在整个 Groq 定义里面呢

486
00:08:46,900 --> 00:08:47,700
那首先第一条呢

487
00:08:47,700 --> 00:08:49,466
就是硬件极致的简化

488
00:08:49,466 --> 00:08:51,566
也是进件尽可能的少东西

489
00:08:51,566 --> 00:08:54,466
把所有东西呢交给我们的软件编译器

490
00:08:54,466 --> 00:08:55,066
那最后呢

491
00:08:55,066 --> 00:08:56,033
硬件呢执行的

492
00:09:01,466 --> 00:09:02,933
大部分呢都是我们的软件

493
00:09:04,400 --> 00:09:07,666
是 software defined Tensor streaming multipleprocessor

494
00:09:07,666 --> 00:09:08,333
那这里面呢

495
00:09:08,700 --> 00:09:13,066
很突出的一点就是 software 软件定义了

496
00:09:13,066 --> 00:09:15,333
我们将它后面呢详细的去看看

497
00:09:17,000 --> 00:09:17,600
不过呢

498
00:09:17,600 --> 00:09:19,900
在了解所谓的软件定义之前呢

499
00:09:19,900 --> 00:09:21,700
我们还是要了解一个问题啊

500
00:09:21,700 --> 00:09:23,600
我们怎么去让计算的过程

501
00:09:23,600 --> 00:09:26,066
完全由软件去定义和调度呢

502
00:09:26,066 --> 00:09:29,233
其实硬件里面有非常多的不确定性的

503
00:09:31,766 --> 00:09:35,066
基本上执行的都是非确定性计算的

504
00:09:35,066 --> 00:09:37,366
执行确定性计算确实会慢很多

505
00:09:41,366 --> 00:09:42,266
我们再看一下

506
00:09:44,400 --> 00:09:45,900
可以看到很明确的肯定

507
00:09:47,400 --> 00:09:48,800
所以叫做 GroqChip

508
00:09:48,800 --> 00:09:49,300
接着呢

509
00:09:49,300 --> 00:09:51,866
把一块芯片呢封装成具体的产品

510
00:09:51,866 --> 00:09:53,666
我们叫做 GroqCard

511
00:09:53,700 --> 00:09:54,466
最后呢把

512
00:09:54,466 --> 00:09:56,966
多单 GroqCard 插在一块板子上面呢

513
00:09:58,800 --> 00:10:02,966
把 9 个 GroqNode 插在一个机架上面哦

514
00:10:02,966 --> 00:10:05,033
我们叫做 GroqRack

515
00:10:06,200 --> 00:10:08,300
这个名字我整不明白

516
00:10:08,800 --> 00:10:10,666
整不懂不管怎么样呢

517
00:10:10,666 --> 00:10:13,033
我们现在来到了第三个内容

518
00:10:27,200 --> 00:10:28,166
也就是第四代啊

519
00:10:28,166 --> 00:10:29,733
现在呢都已经出到第六代了

520
00:10:30,300 --> 00:10:32,066
它是个 2020 年的芯片了

521
00:10:32,066 --> 00:10:34,133
所以它用上一代很正常

522
00:10:36,000 --> 00:10:38,300
主要是连在我们的 host 的 CPU 的

523
00:10:38,300 --> 00:10:40,766
整个 TPU 呢主要是负责计算

524
00:10:40,766 --> 00:10:44,566
那片上呢一共有 222 兆的一个 SRAM 嘛

525
00:10:47,700 --> 00:10:49,366
那没有低源的一个控制接口

526
00:10:54,766 --> 00:10:58,033
整个芯片的面积呢是 725 平方毫米

527
00:10:58,466 --> 00:10:59,866
也就是 25 乘以 29

528
00:10:59,866 --> 00:11:03,266
里面呢一共有 268 亿个晶体管呢

529
00:11:04,200 --> 00:11:05,366
其实晶体管很多

530
00:11:05,366 --> 00:11:07,533
那值得注意的就是我们的外围呢

531
00:11:11,600 --> 00:11:13,200
也是 chip to chip 加速芯片

532
00:11:13,200 --> 00:11:15,200
跟加速芯片之间互联支持

533
00:11:16,100 --> 00:11:19,500
用 320B 的一个向量进行一个传输的

534
00:11:19,500 --> 00:11:22,900
那向量传输呢则是指我们的 VXM 呢

535
00:11:24,300 --> 00:11:26,400
还提供一个 3.84 Tb 每秒的

536
00:11:26,400 --> 00:11:28,066
一个片外互联

537
00:11:28,666 --> 00:11:30,466
我们再看看下一个指标里面呢

538
00:11:30,466 --> 00:11:32,333
一共有 20 个 Superlane

539
00:11:34,200 --> 00:11:35,000
每一条呢是

540
00:11:38,366 --> 00:11:40,433
那其中呢会多一条冗余的

541
00:11:41,200 --> 00:11:44,666
有一个叫做 ICU instruct control unit

542
00:11:48,566 --> 00:11:49,166
这个 ICU 呢

543
00:11:49,166 --> 00:11:50,433
主要是把我们的指令呢

544
00:11:51,700 --> 00:11:52,500
然后去执行

545
00:11:52,500 --> 00:11:54,600
最后呢输出我们的数据

546
00:11:54,600 --> 00:11:56,100
那每个时钟周期内呢

547
00:11:59,400 --> 00:12:03,766
累加可以处理 FP16 跟 INT8 FP32 这种这样的

548
00:12:03,766 --> 00:12:04,466
反正呢

549
00:12:04,466 --> 00:12:06,633
就把我们的计算单元呢拼在一起

550
00:12:07,466 --> 00:12:08,933
整个的芯片的数据流动呢

551
00:12:11,066 --> 00:12:12,133
我们可以看到啊

552
00:12:21,500 --> 00:12:22,766
流动到我们的 VSM

553
00:12:22,766 --> 00:12:23,966
VSM 计算完之后呢

554
00:12:23,966 --> 00:12:25,533
给到我们的下一个 memory

555
00:12:30,700 --> 00:12:33,000
然后返回给来存进来

556
00:12:33,000 --> 00:12:34,666
接着我们再给下一个指令

557
00:12:44,000 --> 00:12:46,100
刚才讲到了有两个很核心的概念

558
00:12:48,166 --> 00:12:51,766
计算单元呢位于整个芯片的正中央

559
00:12:55,166 --> 00:12:58,433
体积为一个 320*320 的一个具体的小矩阵

560
00:12:59,100 --> 00:13:02,200
了解完整个 TSP 的芯片的形态之后呢

561
00:13:03,566 --> 00:13:06,166
因为他说软件定义我们的硬件

562
00:13:06,166 --> 00:13:08,233
所以说 TSP 的一个硬件的性能呢

563
00:13:13,400 --> 00:13:15,200
编译器里面呢就协调了

564
00:13:15,200 --> 00:13:19,200
必须要以 144B 为宽的一个超长指令字呢

565
00:13:21,100 --> 00:13:22,066
字呢我们看一下

566
00:13:22,066 --> 00:13:25,133
右边呢用的是 SIMD 的一个 unit

567
00:13:27,766 --> 00:13:30,733
最底下的是我们的指令的 dispatch

568
00:13:31,166 --> 00:13:32,533
哎了解完右边这个图之后呢

569
00:13:33,466 --> 00:13:36,033
来打开一下整个 SIMD 的 Unit

570
00:13:39,100 --> 00:13:40,066
不知道因此呢

571
00:13:41,100 --> 00:13:43,966
变成具体的执行的单元

572
00:13:49,966 --> 00:13:51,566
还有一个 SXM

573
00:13:51,566 --> 00:13:53,933
专门对我们的数据的进行 reshape

574
00:13:57,200 --> 00:14:00,200
这四个单元呢都是 SIMD 的单元

575
00:14:04,966 --> 00:14:06,633
接着把刚才的一个芯片呢

576
00:14:11,200 --> 00:14:13,466
那我们数据流动呢是这么循环的嘛

577
00:14:13,466 --> 00:14:14,366
算完一个之后呢

578
00:14:14,366 --> 00:14:16,133
存下来然后做数字转换

579
00:14:24,800 --> 00:14:27,000
接着我们往下看一看具体的指令

580
00:14:27,000 --> 00:14:28,200
我们怎么分发数据

581
00:14:28,200 --> 00:14:29,000
我们怎么留

582
00:14:29,866 --> 00:14:32,433
我们最底下的是有一个 ICU 的

583
00:14:34,700 --> 00:14:36,700
把我们具体的指令呢

584
00:14:36,700 --> 00:14:39,300
给到每一个单元去执行

585
00:14:39,300 --> 00:14:41,066
而在整体的指令呢

586
00:14:41,066 --> 00:14:42,033
从下到上

587
00:14:43,800 --> 00:14:44,266
因此呢

588
00:14:48,400 --> 00:14:49,166
通过同步的指令呢

589
00:14:49,166 --> 00:14:51,466
分发到所有的 SIMD 单元里面

590
00:14:55,666 --> 00:14:57,033
那么肯定要有数据嘛

591
00:14:57,800 --> 00:15:00,800
实际上是左右的跨不同的执行单元呢

592
00:15:00,800 --> 00:15:02,400
去流动我们的数据

593
00:15:03,500 --> 00:15:05,066
就靠一个很重要的东西

594
00:15:05,066 --> 00:15:06,566
也就是流式的计算器

595
00:15:06,566 --> 00:15:12,233
我们叫做 SR 或者 SRF stream Registers

596
00:15:17,400 --> 00:15:18,866
我们现在来打开看一下

597
00:15:18,866 --> 00:15:20,233
TSP 的一个 Superlane

598
00:15:21,166 --> 00:15:23,666
TSP 是由 20 条 Superlane 组成的

599
00:15:23,666 --> 00:15:24,766
每一条 Superlane

600
00:15:24,766 --> 00:15:26,633
我们可以看一下右边的这个图呢

601
00:15:29,400 --> 00:15:30,400
那流水线里面呢

602
00:15:30,400 --> 00:15:32,200
主要是发射指令的

603
00:15:32,200 --> 00:15:32,600
可以看到

604
00:15:35,200 --> 00:15:37,766
两个矩阵乘法的单元 MXM

605
00:15:38,200 --> 00:15:39,566
两个内存的单元

606
00:15:44,966 --> 00:15:45,733
那基本上呢

607
00:15:49,500 --> 00:15:50,866
我们详细来进来看看

608
00:15:50,866 --> 00:15:52,333
这个主要的逻辑图

609
00:15:54,100 --> 00:15:55,766
实际上或者在我们 Superlane 里面呢

610
00:15:55,766 --> 00:15:57,733
是左右东西流动的

611
00:15:58,500 --> 00:16:01,300
ICU 啊最底下的一个控制器呢

612
00:16:03,100 --> 00:16:04,666
然后一级一级往上流

613
00:16:04,666 --> 00:16:07,466
一级一级执行完不同的硬件指令

614
00:16:08,600 --> 00:16:10,066
不过值得注意的就是

615
00:16:10,066 --> 00:16:11,833
每一个计算单元呢

616
00:16:13,500 --> 00:16:14,800
都是不一样的

617
00:16:14,800 --> 00:16:15,266
因此呢

618
00:16:15,266 --> 00:16:17,133
我们把这所有的组合起来

619
00:16:19,066 --> 00:16:21,233
一个超长指令字的架构

620
00:16:30,500 --> 00:16:31,766
一个具体的模块

621
00:16:32,800 --> 00:16:34,066
是分开左右的

622
00:16:34,066 --> 00:16:36,133
是里面呢叫做东西啊

623
00:16:45,400 --> 00:16:47,700
那我们现在打开其中一个

624
00:16:51,466 --> 00:16:52,533
数据实际上呢

625
00:16:57,000 --> 00:17:00,500
交给不同的单元去做一个执行

626
00:17:00,500 --> 00:17:01,800
每次执行完之后呢

627
00:17:01,800 --> 00:17:04,300
就会给到下一个 Superlane 进行执行

628
00:17:04,300 --> 00:17:05,000
这个时候呢

629
00:17:09,566 --> 00:17:11,866
这种南北向的 Superlane

630
00:17:11,866 --> 00:17:13,333
东西向的数据流

631
00:17:15,500 --> 00:17:19,100
有氧类似于谷歌 TPU 里面的脉动阵列

632
00:17:20,666 --> 00:17:22,466
是主导谷歌的 TPU 的开发

633
00:17:22,466 --> 00:17:23,433
用的脉动阵列

634
00:17:23,700 --> 00:17:26,700
这里面呢也同样引用了这种思想

635
00:17:29,100 --> 00:17:31,366
哎这里面 ZOMI 就有个问题了

636
00:17:31,366 --> 00:17:34,466
数据在不同的我们的 SIMD 上面去流动

637
00:17:34,466 --> 00:17:35,633
不同的时间段呢

638
00:17:38,900 --> 00:17:40,366
这不是很容易引起

639
00:17:40,366 --> 00:17:42,333
非确定性计算的问题吗

640
00:17:44,266 --> 00:17:44,866
分析预测

641
00:17:44,866 --> 00:17:46,066
还有很多逻辑

642
00:17:50,066 --> 00:17:51,433
这个就很有意思啊

643
00:17:53,200 --> 00:17:53,866
所以编译器呢

644
00:17:53,866 --> 00:17:55,166
必须要去了解我们的指令流

645
00:17:55,166 --> 00:17:56,933
数据流还有优化整体的利

646
00:17:58,800 --> 00:18:01,066
时间都要记录下来的

647
00:18:02,666 --> 00:18:04,433
我们的数据的移动位置和管理

648
00:18:04,666 --> 00:18:06,733
单元还有取指的问题

649
00:18:09,300 --> 00:18:11,666
那这个时候所有的负荷

650
00:18:11,666 --> 00:18:13,033
所有的优化点

651
00:18:19,000 --> 00:18:23,266
我们就引入了第二个具体的核心的点

652
00:18:24,100 --> 00:18:25,600
微内核的模块

653
00:18:25,600 --> 00:18:26,866
打开具体的执行模块

654
00:18:26,866 --> 00:18:27,833
也是我们对应的

655
00:18:34,666 --> 00:18:37,633
Superlane 刚才讲到了有两个 MXM 呢

656
00:18:38,600 --> 00:18:41,266
有一个中间垂直的 VXM 呢

657
00:18:41,266 --> 00:18:43,233
和两个数据交换来去组成

658
00:18:44,800 --> 00:18:46,166
第一个呢就是 MXM

659
00:18:47,566 --> 00:18:49,833
有点类似于英伟达的 tensor core 啊

660
00:18:53,700 --> 00:18:55,400
也就是参观了一下操作

661
00:18:56,900 --> 00:18:59,800
这 320 个 Mac 有非常多的 Mac 呢

662
00:18:59,800 --> 00:19:01,700
又分成 20 个超级单元

663
00:19:01,700 --> 00:19:03,466
我们叫做 supercell

664
00:19:03,466 --> 00:19:07,066
那既然是 320 个 Mac 分成 20 个

665
00:19:07,066 --> 00:19:09,233
那这里面呢就是 16 个 Mac 呢

666
00:19:12,400 --> 00:19:13,166
每个 Mac 啊

667
00:19:13,166 --> 00:19:14,633
打开里面很小的一个模块呢

668
00:19:16,300 --> 00:19:18,266
和两个 32B 的累加寄存器

669
00:19:19,500 --> 00:19:21,200
那我们把它打开来看呢

670
00:19:21,200 --> 00:19:22,366
就长这个样子

671
00:19:22,366 --> 00:19:24,933
其中一个呢就是长这个方式的

672
00:19:27,700 --> 00:19:28,700
也就是大部分呢

673
00:19:28,700 --> 00:19:29,900
我们会来处理

674
00:19:29,900 --> 00:19:32,800
FP16 的一个具体的计算单元

675
00:19:34,666 --> 00:19:36,566
因此呢会通过左边的这一个

676
00:19:38,666 --> 00:19:41,233
只有呢我们真正的一些银布的 data 呢

677
00:19:43,800 --> 00:19:45,200
然后进行一个累加操作

678
00:19:45,200 --> 00:19:46,666
可以看到它的电路设计呢

679
00:19:46,666 --> 00:19:47,666
是有个回环的

680
00:19:47,666 --> 00:19:49,533
也非常的精妙

681
00:19:50,300 --> 00:19:50,900
这往下呢

682
00:19:50,900 --> 00:19:52,300
我们来了解第二个内容

683
00:19:52,300 --> 00:19:54,766
就是向量的执行单元 VXM

684
00:19:56,066 --> 00:19:58,233
那 VXM 实际上呢有 16 个 ALU

685
00:19:59,266 --> 00:20:01,166
可以看到这边有 16 个 ALU

686
00:20:01,166 --> 00:20:02,933
这行 4 个竖 4 个

687
00:20:05,700 --> 00:20:06,866
作为我们的操作数

688
00:20:12,300 --> 00:20:13,500
其实很明确

689
00:20:13,700 --> 00:20:15,700
常规对我们向量的加减乘除

690
00:20:16,700 --> 00:20:17,966
格式的转化归一化

691
00:20:17,966 --> 00:20:19,066
要激活这种呢

692
00:20:19,066 --> 00:20:21,466
我们是非常依赖于 VXM

693
00:20:25,200 --> 00:20:27,766
我们现在了解完了两个很重要的单元

694
00:20:27,766 --> 00:20:29,133
一个呢就是 VXM

695
00:20:30,866 --> 00:20:32,433
两个很核心的计算

696
00:20:35,100 --> 00:20:37,300
MEM 我们的内存单元

697
00:20:37,300 --> 00:20:37,966
内存单元呢

698
00:20:37,966 --> 00:20:39,733
分布在每一个 Superlane 里面呢

699
00:20:41,366 --> 00:20:44,033
但是每个 Superlane 里面的内存单元呢

700
00:20:49,400 --> 00:20:51,466
只有 5.5MB 那这里面呢

701
00:20:51,466 --> 00:20:53,666
我们只是指单的一个面呢

702
00:20:55,900 --> 00:20:58,600
里面呢就分开 44 个切片

703
00:20:58,600 --> 00:21:00,800
从 M0 到 M43

704
00:21:00,800 --> 00:21:03,000
每个切片呢一共有 128K

705
00:21:03,200 --> 00:21:04,666
组合起来了近 5.5NB

706
00:21:04,666 --> 00:21:05,733
因为有 20 个 Superlane

707
00:21:09,900 --> 00:21:11,700
整一块芯片呢一共是

708
00:21:11,700 --> 00:21:16,066
220MB 的一个具体的显存大小

709
00:21:16,066 --> 00:21:18,133
很小啊这么小

710
00:21:19,800 --> 00:21:22,766
确实用的真的是不够用

711
00:21:23,700 --> 00:21:25,366
这里面呢有一些白色的

712
00:21:25,366 --> 00:21:26,833
就我们刚才讲到的

713
00:21:34,700 --> 00:21:36,900
最后呢我们来讲第四个单元

714
00:21:36,900 --> 00:21:39,866
叫做数据交换单元 SXM

715
00:21:39,866 --> 00:21:41,333
那它主要的动作呢

716
00:21:43,900 --> 00:21:46,166
把旋转平移各种各样的也好呢

717
00:21:46,166 --> 00:21:49,833
为的就是从 VXM 到 MXM 里面的数据呢

718
00:21:50,966 --> 00:21:51,766
一个是张量嘛

719
00:21:53,900 --> 00:21:55,766
但是呢该 reshape reshape

720
00:21:55,766 --> 00:21:57,133
这个呢也是符合我们 AI 的

721
00:21:58,700 --> 00:22:00,766
不过很有意思的就是 SXM 呢

722
00:22:00,766 --> 00:22:05,866
它可以划 Superlane 进行一个向下的通讯

723
00:22:06,866 --> 00:22:08,533
我们还是需要打开一下

724
00:22:10,000 --> 00:22:10,900
因为存储结构呢

725
00:22:10,900 --> 00:22:11,500
很多时候呢

726
00:22:11,500 --> 00:22:13,700
会决定我们编译器

727
00:22:14,900 --> 00:22:18,466
或我们的张量进行一个具体的排布

728
00:22:18,466 --> 00:22:20,533
那这里面呢就分开四级啊

729
00:22:25,000 --> 00:22:27,100
这对软件呢会有具体的抽象

730
00:22:27,100 --> 00:22:28,766
那这里面的内存的寻址呢

731
00:22:28,766 --> 00:22:32,066
就按照 rank5 五层的方式呢进行寻址

732
00:22:32,066 --> 00:22:34,166
首先呢我们去寻找 device

733
00:22:34,166 --> 00:22:36,166
然后寻找到哪两个面

734
00:22:36,166 --> 00:22:38,533
然后哪个具体的内存的切片

735
00:22:47,066 --> 00:22:48,933
了解完大面的一个程序结构之后呢

736
00:22:50,000 --> 00:22:51,200
一个流寄存器呢

737
00:22:51,200 --> 00:22:51,866
放在哪里

738
00:22:53,100 --> 00:22:54,100
我们的流寄存器器呢

739
00:22:54,100 --> 00:22:56,200
放在一条 Superlane 里面的 MEM

740
00:22:57,766 --> 00:23:00,766
那这个位置呢其实含有类似的 SRF 呢

741
00:23:00,766 --> 00:23:02,133
就是我们的流寄存器呢

742
00:23:04,066 --> 00:23:07,833
结果的 45 个 SRF 的横跨一个 Superlane

743
00:23:10,700 --> 00:23:11,700
下面有一个头呢

744
00:23:14,500 --> 00:23:15,700
那整体的功能单元呢

745
00:23:15,700 --> 00:23:17,000
刚才讲完了

746
00:23:17,000 --> 00:23:21,300
一共最核心的有 4 个呢 MEM VXM MXM SXM

747
00:23:21,300 --> 00:23:23,000
那刚才 C2C 呢就 chip to chip

748
00:23:23,200 --> 00:23:25,400
还有 ICU 我们的指令发式呢

749
00:23:33,500 --> 00:23:34,966
现在我们来到了 3.3

750
00:23:34,966 --> 00:23:37,133
Groq 的一个技术的架构

751
00:23:40,000 --> 00:23:43,366
就是我们怎么把 TSP 呢扩展成 LPU 啊

752
00:23:43,366 --> 00:23:46,066
它现在最新呢叫自己也叫 LPU 呢

753
00:23:46,066 --> 00:23:48,566
以前呢叫自己叫 TSP

754
00:23:48,866 --> 00:23:52,133
哎这个小甜甜跟牛夫人的区别啊

755
00:23:54,300 --> 00:23:54,800
它现在呢

756
00:23:54,800 --> 00:23:57,566
说它专门用来处理大语言模型的

757
00:23:57,566 --> 00:23:58,166
而 TSP 呢

758
00:24:00,700 --> 00:24:02,366
嗯可以看到它的 scale 呢

759
00:24:02,366 --> 00:24:03,766
是不断的规模增大了

760
00:24:03,766 --> 00:24:06,233
从一片呢到 8 片变成一个节点

761
00:24:07,600 --> 00:24:10,666
时延呢确实也是做的非常的低的

762
00:24:12,266 --> 00:24:13,333
但硬件的芯片呢

763
00:24:15,000 --> 00:24:15,866
确定性的计算呢

764
00:24:21,700 --> 00:24:22,200
那这里面呢

765
00:24:25,066 --> 00:24:26,066
确定性的网络呢

766
00:24:26,066 --> 00:24:28,133
确定性网络呢是指我们的编译器呢

767
00:24:31,900 --> 00:24:33,400
因此呢 TSP 当中呢

768
00:24:33,400 --> 00:24:35,666
要显示的去进行一个调度

769
00:24:35,666 --> 00:24:37,066
那具体调度或者细节呢

770
00:24:37,066 --> 00:24:37,766
下面有很多

771
00:24:37,766 --> 00:24:39,466
我就不逐一的去介绍了

772
00:24:39,466 --> 00:24:40,333
很重要的一点呢

773
00:24:41,666 --> 00:24:44,266
就是单个 TSP 的指令的执行呢

774
00:24:44,266 --> 00:24:45,766
是确定性的这个理解

775
00:24:45,900 --> 00:24:47,266
但是 TSP 之间的 C2C

776
00:24:47,266 --> 00:24:49,833
是有可能会引起非确定性计算的

777
00:24:52,466 --> 00:24:54,233
例如刚才讲到的 TSP

778
00:24:56,966 --> 00:24:58,766
没有全局时钟怎么办呀

779
00:24:58,766 --> 00:25:00,466
然后单个网络的执行的时候

780
00:25:00,466 --> 00:25:02,366
它不会做事时时间漂移吗

781
00:25:02,366 --> 00:25:03,933
我们怎么去处理啊

782
00:25:04,066 --> 00:25:04,966
而且编译性呢

783
00:25:04,966 --> 00:25:07,233
它需要精确的评估列入的言辞端

784
00:25:10,700 --> 00:25:12,166
因此呢为什么就是 TSP

785
00:25:12,166 --> 00:25:13,366
他要花那么多时间

786
00:25:13,366 --> 00:25:14,833
去开发软件的问题

787
00:25:16,466 --> 00:25:17,966
20 年的时候已经出来了

788
00:25:17,966 --> 00:25:20,733
他要做非常多的软硬件的协同呢

789
00:25:25,200 --> 00:25:26,100
那主要有 3 种啊

790
00:25:26,100 --> 00:25:26,500
第一种呢

791
00:25:31,466 --> 00:25:32,566
来进行一个通讯的

792
00:25:32,566 --> 00:25:33,633
每一个发送和接收呢

793
00:25:35,266 --> 00:25:36,466
那有了时间戳之外呢

794
00:25:36,466 --> 00:25:38,833
我们还会公开一些硬件的状态信息

795
00:25:39,900 --> 00:25:42,200
在计算图里面的显示的感知

796
00:25:42,200 --> 00:25:42,866
那第三个呢

797
00:25:42,866 --> 00:25:44,666
就通过指定级的调度

798
00:25:45,400 --> 00:25:47,700
变成我们具体指令呢来做调度的

799
00:25:48,866 --> 00:25:51,866
上面呢其实也需要进行一个同步的

800
00:25:54,400 --> 00:25:55,766
还有测试我们整体的状态

801
00:25:55,766 --> 00:25:58,633
还有做一些时间的重同步

802
00:26:00,566 --> 00:26:02,666
我们基本上能够做到确定性的计算

803
00:26:02,666 --> 00:26:04,966
确定性的网络通信软件

804
00:26:04,966 --> 00:26:06,633
上面解决了这些问题之后呢

805
00:26:08,366 --> 00:26:10,833
那右边的这个就是它的一个节点的了

806
00:26:12,266 --> 00:26:14,866
右边对应的就下面的一个小圆圈

807
00:26:19,366 --> 00:26:20,233
卡跟卡之间呢

808
00:26:21,800 --> 00:26:23,500
进行一个连接

809
00:26:23,500 --> 00:26:24,200
那最后呢

810
00:26:29,066 --> 00:26:31,066
然后呢有两个 CPU 进行一个控制

811
00:26:31,066 --> 00:26:33,533
每个 CPU 呢控制 4 块 TSP

812
00:26:36,200 --> 00:26:37,366
有两个 CPU

813
00:26:37,366 --> 00:26:39,766
每个 CPU 呢就挂 4 个 TSP

814
00:26:39,766 --> 00:26:41,366
就是我们的 Groq 整体的形态

815
00:26:41,366 --> 00:26:44,033
那刚才讲到了一个节点就是 GroqNode

816
00:26:48,866 --> 00:26:51,233
我们的一个拓扑哦

817
00:26:53,400 --> 00:26:55,066
其实也是非常复杂

818
00:26:56,566 --> 00:26:58,333
所以大家呢可以看一下右边的

819
00:27:00,000 --> 00:27:01,800
或者变成一个超级点之后呢

820
00:27:04,600 --> 00:27:07,700
那我们其实更关心的是它的一个性能

821
00:27:08,600 --> 00:27:10,066
于是用 TSP 的时候呢

822
00:27:10,066 --> 00:27:11,366
就小型系统的时候呢

823
00:27:11,366 --> 00:27:12,133
它基本上呢

824
00:27:15,500 --> 00:27:15,900
但是呢

825
00:27:18,300 --> 00:27:19,500
体的带宽呢是下降的

826
00:27:19,500 --> 00:27:20,866
到了 50GB 每秒

827
00:27:20,866 --> 00:27:22,533
超过 264 之后呢

828
00:27:27,266 --> 00:27:29,533
哎呦 ZOMI 老师你讲完这一堆之后

829
00:27:34,300 --> 00:27:37,066
一块芯片里面的 SRAM 这么小

830
00:27:41,366 --> 00:27:45,266
要切 TP 把它都切系在每张卡去训练

831
00:27:45,300 --> 00:27:47,400
这个这么挫的芯片

832
00:27:47,600 --> 00:27:48,800
他怎么推理时延

833
00:27:50,566 --> 00:27:51,466
你都没讲

834
00:27:55,000 --> 00:27:56,600
对所以我们在 3.4 里面呢

835
00:28:00,466 --> 00:28:02,733
刚才我们只是讲了一个软件定义硬件

836
00:28:03,766 --> 00:28:05,433
他有很多的细节

837
00:28:06,600 --> 00:28:09,400
为什么性能能够吊打英伟达

838
00:28:09,400 --> 00:28:10,000
那这里面呢

839
00:28:12,000 --> 00:28:13,766
两个很重要的指标

840
00:28:16,800 --> 00:28:18,500
我们的行坐标呢是时延

841
00:28:18,500 --> 00:28:20,200
纵坐标呢是吞吐

842
00:28:21,900 --> 00:28:22,700
推理的目标呢

843
00:28:29,600 --> 00:28:33,800
时延低吞吐大是我们的目标

844
00:28:34,000 --> 00:28:35,300
接着有了这个之后呢

845
00:28:35,300 --> 00:28:37,066
我们还是左边的带宽

846
00:28:37,066 --> 00:28:39,566
跟计算的一个具体的曲线

847
00:28:39,566 --> 00:28:41,833
如果大家对这个曲线不了解呢

848
00:28:44,666 --> 00:28:46,066
这个系列里面呢

849
00:28:48,466 --> 00:28:51,633
AI chip 里面很重要的一个模块啊

850
00:28:58,500 --> 00:28:59,666
回归正传呢

851
00:29:03,900 --> 00:29:05,366
也就是右边的纵坐标

852
00:29:07,500 --> 00:29:09,100
都是相同的

853
00:29:09,466 --> 00:29:10,166
那这个时候呢

854
00:29:12,800 --> 00:29:14,966
对应的就是 batch size 的增加了

855
00:29:14,966 --> 00:29:17,833
而系统的 bound 将会从我们的带宽的 bound

856
00:29:20,266 --> 00:29:20,633
这个时候呢

857
00:29:22,466 --> 00:29:24,466
也就可能从 A100 到 H100

858
00:29:24,466 --> 00:29:26,266
假设我们的带宽都是不变的

859
00:29:26,266 --> 00:29:27,766
那这个时候我们往右边看看

860
00:29:27,766 --> 00:29:30,033
整体的指标会往哪个方向走

861
00:29:32,366 --> 00:29:34,033
于是呢我们可能在当前

862
00:29:36,266 --> 00:29:38,933
之后呢整体的吞吐呢就增加了

863
00:29:41,000 --> 00:29:41,300
就是

864
00:29:47,366 --> 00:29:49,666
假设我们的模型的 batch size 不变

865
00:29:50,500 --> 00:29:52,466
都是输进去一个比较小的值

866
00:29:52,466 --> 00:29:54,533
因为大语言模型我们刚才讲到了

867
00:30:00,266 --> 00:30:01,833
给到一个大语言模型

868
00:30:05,100 --> 00:30:06,600
再把 3 个核心的 token 可能

869
00:30:06,600 --> 00:30:09,466
输给我们这个大语言模型

870
00:30:09,466 --> 00:30:12,166
那这种方式我们叫做自回归的方式

871
00:30:12,166 --> 00:30:13,366
一次生成一个 token

872
00:30:13,366 --> 00:30:15,566
可循环的丢到我们的网络模型里面的

873
00:30:15,566 --> 00:30:16,833
因此这个时候呢

874
00:30:19,166 --> 00:30:20,866
我们要对数据不断的搬来搬出

875
00:30:22,166 --> 00:30:23,666
这个时候呢推理的

876
00:30:23,666 --> 00:30:26,033
服务就会像我们左边这个图呢

877
00:30:30,166 --> 00:30:31,766
为了使我们的性能更好了

878
00:30:31,766 --> 00:30:33,633
我们还是看回右边的这个图

879
00:30:36,466 --> 00:30:37,433
是因为大语言模型呢

880
00:30:38,366 --> 00:30:41,366
是我们需要很多的数据的搬运

881
00:30:41,366 --> 00:30:43,433
那这个时候呢为了提升 latency

882
00:30:45,466 --> 00:30:47,333
也就是时延呢从这个点

883
00:30:47,500 --> 00:30:48,500
然后听到这个点

884
00:30:48,500 --> 00:30:49,900
我们唯一能做的

885
00:30:52,466 --> 00:30:55,133
那这个时候增加片内的带宽

886
00:31:01,400 --> 00:31:05,000
因为它取消了 L1 L2 L3 L U 各种各样的 cache

887
00:31:05,000 --> 00:31:07,900
把所有的数据呢都放到 SRAM 里面

888
00:31:07,966 --> 00:31:11,033
SRAM 里面呢具体的就是 stream register file

889
00:31:13,900 --> 00:31:14,966
非常的惊人哦

890
00:31:14,966 --> 00:31:16,466
达到了 80TB 每秒

891
00:31:16,466 --> 00:31:20,533
而因为达到 H100 啊只能做到 3.35TB 每秒

892
00:31:22,200 --> 00:31:25,100
你要真的说吞吐吞吐肯定是 H100 高的

893
00:31:25,100 --> 00:31:26,966
因为它单节点里面的内存呢

894
00:31:29,000 --> 00:31:32,066
所以呢鱼与熊掌不能兼得

895
00:31:32,066 --> 00:31:34,933
Groq 呢主要的就是在一个低吞吐下呢

896
00:31:37,066 --> 00:31:38,733
而英伟达呢没有管时延

897
00:31:41,266 --> 00:31:43,666
所以我们说呢他两者之间呢没法比

898
00:31:43,666 --> 00:31:44,833
看这条曲线呢

899
00:31:50,900 --> 00:31:51,900
做一个技术的总结

900
00:31:51,900 --> 00:31:55,200
首先呢它取消了我们的 L123 的 cache

901
00:31:55,200 --> 00:31:56,300
把所有数据呢都放在

902
00:31:56,300 --> 00:31:58,766
SRAM 实现了一个内存的带宽呢

903
00:31:58,766 --> 00:31:59,566
非常的高

904
00:31:59,566 --> 00:32:01,766
从而提升我们整体的一个时延

905
00:32:01,766 --> 00:32:04,233
也就是 Tokens 每秒非常的快哦

906
00:32:05,366 --> 00:32:06,233
真的离谱

907
00:32:11,266 --> 00:32:13,433
这个对我们国内厂商其实很有益的

908
00:32:15,700 --> 00:32:17,266
连续性是受限的

909
00:32:17,266 --> 00:32:18,433
它这个很牛逼的

910
00:32:19,900 --> 00:32:22,466
实现了硬件的一代打三代啊

911
00:32:22,466 --> 00:32:23,533
非常的夸张

912
00:32:24,600 --> 00:32:27,300
在 2024 年居然还可以大放光彩啊

913
00:32:32,000 --> 00:32:34,600
还不如真正的把软件做好对不对

914
00:32:36,700 --> 00:32:39,166
这个对国内厂商都是很有接近意义的

915
00:32:41,866 --> 00:32:44,033
一开始是为 CNN 的卷积网络模型

916
00:32:44,500 --> 00:32:46,366
因为那时候还是 2020 年呢

917
00:32:46,366 --> 00:32:48,733
不是为 transformer 去设计的

918
00:32:51,200 --> 00:32:51,600
成为了

919
00:32:51,600 --> 00:32:54,566
地表最快 token 输出速度的一个芯片

920
00:32:54,566 --> 00:32:56,566
还是非常的惊人的

921
00:32:56,566 --> 00:32:58,666
不过呢这个时候对我们的软件呢

922
00:32:59,866 --> 00:33:01,733
我们需要把人工神经网络

923
00:33:02,700 --> 00:33:04,200
翻译成我们具体的一些指定

924
00:33:09,166 --> 00:33:10,366
最后一个技术总结点呢

925
00:33:10,366 --> 00:33:11,966
就是对于端侧的产品

926
00:33:13,666 --> 00:33:14,866
会受限于面积啊

927
00:33:14,866 --> 00:33:15,933
功耗因素呢

928
00:33:17,400 --> 00:33:19,466
数据流也就是我们做到了

929
00:33:19,466 --> 00:33:21,266
一个软件的确定性计算

930
00:33:21,266 --> 00:33:23,566
而软件的确定性通讯以后呢

931
00:33:25,300 --> 00:33:28,300
肯定未来的是一个非常好的一个趋势

932
00:33:30,566 --> 00:33:32,233
不过呢有个最大的 bug

933
00:33:35,866 --> 00:33:37,133
整体的 KV cache 的占用呢

934
00:33:41,300 --> 00:33:43,200
因此需要一个很大的规模的机型

935
00:33:43,200 --> 00:33:45,400
大模型现在都模型很大

936
00:33:45,400 --> 00:33:48,566
所以说它有可能 2020 年的芯片呢

937
00:33:48,566 --> 00:33:49,533
确实不适用于

938
00:33:51,166 --> 00:33:52,433
所以我们有望

939
00:33:57,766 --> 00:33:59,833
不知不觉我们进来的最后一个内容

940
00:34:02,666 --> 00:34:04,933
ZOMI 呢发表了一下自己的个人观点

941
00:34:05,266 --> 00:34:07,633
我们可以看到业界的一个通用的看法

942
00:34:09,500 --> 00:34:10,966
也非常不苟同啊

943
00:34:11,400 --> 00:34:13,100
我们可以看到贾扬清

944
00:34:13,100 --> 00:34:15,200
他发表了一大串文字

945
00:34:15,200 --> 00:34:17,300
说白了就是贵

946
00:34:17,300 --> 00:34:18,900
太贵了你这个 Groq 了

947
00:34:18,900 --> 00:34:20,100
不划算没有性价比

948
00:34:20,100 --> 00:34:20,900
没有市场机会

949
00:34:20,900 --> 00:34:24,266
你不行那这是贾扬清的一个看法

950
00:34:24,600 --> 00:34:25,700
那最后呢

951
00:34:25,700 --> 00:34:26,900
Semianalysis 呢

952
00:34:26,900 --> 00:34:28,366
根据贾扬清的这种看法呢

953
00:34:29,500 --> 00:34:30,466
当然大家有兴趣呢

954
00:34:35,400 --> 00:34:36,166
成本核算呢

955
00:34:37,700 --> 00:34:39,200
说实话 GPU 里面的成本呢

956
00:34:41,666 --> 00:34:43,066
也是我们的显存

957
00:34:44,666 --> 00:34:46,433
因为 SRAM 推比起它的芯片

958
00:34:51,600 --> 00:34:55,066
总体的成本呢大概是 60 万美金

959
00:34:56,000 --> 00:34:58,600
我们能采购两台英伟达的 H100 吗

960
00:35:03,800 --> 00:35:05,366
一台才 50 多

961
00:35:05,366 --> 00:35:06,633
两台怎么跑啊

962
00:35:09,466 --> 00:35:12,566
所以呢 ZOMI 并不是很认同哦

963
00:35:13,600 --> 00:35:16,066
我们看看一个在知乎上面的一个大名

964
00:35:16,066 --> 00:35:17,666
叫做 mackler 啊

965
00:35:18,766 --> 00:35:19,733
他里面说了一堆

966
00:35:23,400 --> 00:35:25,166
然后被很多的自媒体

967
00:35:27,000 --> 00:35:28,300
反正他的看法呢

968
00:35:28,300 --> 00:35:29,066
ZOMI 总结一句

969
00:35:29,066 --> 00:35:31,166
不妥很不妥啊

970
00:35:31,166 --> 00:35:32,933
非常不符合大语言模型的

971
00:35:39,400 --> 00:35:41,800
那 ZOMI 呢就发表一下自己的观点

972
00:35:41,800 --> 00:35:44,000
首先呢是针对市场策略的一个观点

973
00:35:44,000 --> 00:35:46,100
我们可以看到这个呢

974
00:35:48,900 --> 00:35:49,966
假设某个场景呢

975
00:35:52,666 --> 00:35:53,366
一个 Token

976
00:35:53,366 --> 00:35:54,166
那这个时候呢

977
00:35:54,166 --> 00:35:56,633
H100 因为它是主要针对于大吞吐的

978
00:35:59,000 --> 00:36:01,300
我们只能使用了一个 Groq 了

979
00:36:02,200 --> 00:36:04,800
如果这个场景对时间要求不是很高

980
00:36:06,266 --> 00:36:08,633
那我们呢这个时候才考虑性价比

981
00:36:11,900 --> 00:36:12,400
因此呢

982
00:36:14,500 --> 00:36:17,666
首先呢推理端肯定是 c 端去用的

983
00:36:17,666 --> 00:36:18,733
c 端的用户啊

984
00:36:20,000 --> 00:36:21,466
有个比较明确的要求

985
00:36:21,466 --> 00:36:23,366
是用户的感受很重要

986
00:36:23,366 --> 00:36:24,033
那这个时候呢

987
00:36:25,066 --> 00:36:26,533
还是非常的大的

988
00:36:27,166 --> 00:36:28,166
不以时延为重点

989
00:36:28,166 --> 00:36:29,266
那个推理的芯片呢

990
00:36:29,266 --> 00:36:31,066
我觉得都是在耍流氓

991
00:36:31,166 --> 00:36:33,166
但是有个很大的问题就是以

992
00:36:33,166 --> 00:36:35,533
时延为优先的应用还没有起来啊

993
00:36:40,700 --> 00:36:43,366
咱都说 Groq 这款芯片呢

994
00:36:43,366 --> 00:36:45,133
真正的能够大量的出货

995
00:36:47,300 --> 00:36:48,766
ZOMI 的最后的观点就是

996
00:36:48,766 --> 00:36:51,833
我们不能够把 GPU 的 H100 跟 Groq 呢

997
00:36:54,266 --> 00:36:56,533
从整体芯片的形态和封装的形态

998
00:36:59,366 --> 00:37:00,866
一个用 14 纳米

999
00:37:03,400 --> 00:37:05,800
14 纳米便宜那么多对吧

1000
00:37:07,700 --> 00:37:09,266
小了这么多

1001
00:37:09,866 --> 00:37:12,766
一片晶圆 Groq 可以切 900 片

1002
00:37:14,600 --> 00:37:18,700
一片晶圆 H100 基本上就这么三四个

1003
00:37:18,800 --> 00:37:20,066
差别那么大

1004
00:37:20,066 --> 00:37:22,533
而且访存速度完全不是一个量级

1005
00:37:24,300 --> 00:37:26,400
因为 Groq 主打是快

1006
00:37:27,600 --> 00:37:29,100
不是大吞吐

1007
00:37:29,100 --> 00:37:30,866
别把它两个拿在一起比

1008
00:37:30,866 --> 00:37:32,233
这么比是不合理的

1009
00:37:38,100 --> 00:37:40,166
就是软件生态的问题啊

1010
00:37:46,200 --> 00:37:48,600
搞软件已经开发了那么多年了

1011
00:37:48,600 --> 00:37:50,100
所以说软件的生态啊

1012
00:37:50,100 --> 00:37:52,100
可能开发起来还是很慢的

1013
00:37:55,300 --> 00:37:57,766
我们可以看到现在买卡呢

1014
00:37:57,766 --> 00:37:59,233
基本上是核心成本

1015
00:38:01,966 --> 00:38:04,633
3-10 年内的重要的成本呢是电力

1016
00:38:07,066 --> 00:38:09,266
所以说以后我们的基金怎么组网啊

1017
00:38:09,266 --> 00:38:11,533
也是一个很大很头痛的问题

1018
00:38:12,000 --> 00:38:14,300
我们对 Groq 进行一个简单的洞

1019
00:38:14,300 --> 00:38:15,500
察和分析

1020
00:38:15,900 --> 00:38:16,600
短期内呢

1021
00:38:16,600 --> 00:38:17,300
大语言模型为

1022
00:38:17,300 --> 00:38:19,400
现在还没有明确的商业化落地的路线

1023
00:38:19,400 --> 00:38:19,800
所以呢

1024
00:38:24,566 --> 00:38:26,233
大模型的应用落地呢

1025
00:38:27,400 --> 00:38:29,566
这个是很好的一个方向

1026
00:38:29,666 --> 00:38:30,233
第二点呢

1027
00:38:36,566 --> 00:38:38,733
mackler 的一个具体的想法

1028
00:38:40,100 --> 00:38:40,500
那最后呢

1029
00:38:40,500 --> 00:38:42,400
就是对整个产业的动态思考

1030
00:38:42,400 --> 00:38:44,366
那刚才讲到了展望整个 AI Agent 呢

1031
00:38:44,366 --> 00:38:45,766
它确实是大语言模型

1032
00:38:45,766 --> 00:38:47,233
落地的一个很好的节点

1033
00:38:49,100 --> 00:38:50,666
将会成为 LLM 大语言模型

1034
00:38:50,666 --> 00:38:51,766
或者 LLM 大语言模型的

1035
00:38:51,766 --> 00:38:53,233
一个主流的芯片方案

1036
00:38:55,700 --> 00:38:57,600
就出了 LVP 嘛

1037
00:38:57,600 --> 00:38:59,900
或者 LPU 那以后的多模态呢

1038
00:38:59,900 --> 00:39:00,700
或者缩码这种啊

1039
00:39:00,700 --> 00:39:01,500
会不会出现呢

1040
00:39:01,500 --> 00:39:02,366
VPU 这种啊

1041
00:39:05,300 --> 00:39:05,666
第二点呢

1042
00:39:08,200 --> 00:39:10,066
整体的芯片的内存的不足

1043
00:39:10,066 --> 00:39:10,966
那内存不足呢

1044
00:39:10,966 --> 00:39:13,366
我们可以通过下一代芯片来解决呢

1045
00:39:15,800 --> 00:39:16,400
那这个时候呢

1046
00:39:16,400 --> 00:39:18,466
我们可以在极低的时延下面呢

1047
00:39:18,466 --> 00:39:20,833
来提升我们芯片的吞吐

1048
00:39:20,900 --> 00:39:23,066
哎这不是把我们的芯片装上去呢

1049
00:39:23,066 --> 00:39:24,933
你别把现在的眼光

1050
00:39:26,300 --> 00:39:27,200
20 的眼光

1051
00:39:29,700 --> 00:39:30,200
第三个呢

1052
00:39:30,200 --> 00:39:31,366
就是服务商啊

1053
00:39:31,366 --> 00:39:35,833
你到底想做一个卖铲人还是铲沙子呢

1054
00:39:36,566 --> 00:39:38,433
其实对于百度家昆仑芯啊

1055
00:39:39,466 --> 00:39:40,766
阿里+含光啊

1056
00:39:40,766 --> 00:39:44,733
其实都有一个很好的启发

1057
00:39:47,400 --> 00:39:48,766
供自有的芯片

1058
00:39:49,500 --> 00:39:50,566
芯片的成本呢

1059
00:39:50,566 --> 00:39:52,133
就是推理硬件的成本

1060
00:39:54,300 --> 00:39:56,200
售价就是成本的说明

1061
00:39:56,200 --> 00:39:56,866
服务提供商呢

1062
00:39:58,166 --> 00:40:00,566
还是想当铲沙子呢

1063
00:40:01,700 --> 00:40:02,666
那今天的分享呢

1064
00:40:02,666 --> 00:40:03,433
到这里为止

1065
00:00:05,700 --> 00:00:06,866
哈喽大家好

1066
00:00:31,000 --> 00:00:31,600
第一个呢

1067
00:00:41,700 --> 00:00:46,100
接着我们来了解一下 Groq 整体的背景与概况

1068
00:00:46,466 --> 00:00:50,666
在最核心的内容呢是这个技术架构啊 Groq 技术架构

1069
00:00:52,166 --> 00:00:55,733
硬件和 TSP 的一个内核的解读

1070
00:01:04,400 --> 00:01:06,600
进行一个思考和洞察

1071
00:01:25,300 --> 00:01:27,266
它快了好多好多啊

1072
00:01:49,200 --> 00:01:51,100
基本上呢涨了五倍左右或

1073
00:02:23,900 --> 00:02:26,300
有两个第一个呢就是延迟

1074
00:02:33,000 --> 00:02:34,600
就是我输进去一个文本

1075
00:02:40,900 --> 00:02:44,000
是以我们输出的每一个单词为例子

1076
00:02:56,866 --> 00:02:58,366
输出的速率呢

1077
00:03:10,300 --> 00:03:12,700
那这个呢就是我们所谓的延迟

1078
00:03:16,300 --> 00:03:19,400
答案吐出来的字的一个吐字时间

1079
00:03:22,900 --> 00:03:24,866
吞吐吞吐量都行

1080
00:03:27,300 --> 00:03:28,666
在单位时间内能

1081
00:03:35,500 --> 00:03:37,166
吞吐会影响我们大语言模型

1082
00:03:43,800 --> 00:03:44,600
一般来说呢

1083
00:03:50,000 --> 00:03:51,866
是希望吞吐越大越好

1084
00:04:16,000 --> 00:04:17,200
第一个呢就是 prefill

1085
00:04:26,600 --> 00:04:28,200
第一个呢就是 prefill

1086
00:04:37,400 --> 00:04:39,500
就是因为在大语言模型里面呢

1087
00:04:47,800 --> 00:04:49,766
回答问题前呢都用尊称

1088
00:04:56,300 --> 00:04:59,666
都用一种比较尊重的态度去回答

1089
00:05:02,800 --> 00:05:04,800
方式因此呢

1090
00:05:20,600 --> 00:05:22,366
依次生成一个 took

1091
00:05:23,600 --> 00:05:25,700
decoding 它有很多次哦

1092
00:05:43,200 --> 00:05:44,266
那这种方式呢

1093
00:06:10,800 --> 00:06:13,100
就决定了它整体的时延

1094
00:06:18,300 --> 00:06:21,400
在网上呢有很多人吐槽他的 Prefill 不行

1095
00:06:32,200 --> 00:06:33,200
是 14 纳米

1096
00:06:44,600 --> 00:06:46,300
但是很大的一个问题呢

1097
00:06:56,600 --> 00:06:59,366
我们可以看到 Groq 整体的芯片的指标

1098
00:07:38,300 --> 00:07:39,100
其实我不知道啊

1099
00:08:20,400 --> 00:08:21,266
的时候呢

1100
00:08:31,400 --> 00:08:33,500
他有很多说是自己很牛逼的特性

1101
00:08:40,400 --> 00:08:42,766
那这个带宽的指标呢是非常惊人的

1102
00:08:44,500 --> 00:08:46,900
它用的思路呢比较纯粹

1103
00:08:56,000 --> 00:08:57,166
就是根据一些具体的

1104
00:09:02,900 --> 00:09:04,400
因此呢贯称为它的这样做呢

1105
00:09:08,300 --> 00:09:08,700
很重要

1106
00:09:15,300 --> 00:09:17,000
为什么叫做软件定义

1107
00:10:13,000 --> 00:10:16,166
真正的去看看 Groq 的一个技术的架构

1108
00:10:29,700 --> 00:10:30,300
不过没关系

1109
00:10:34,100 --> 00:10:36,000
呃这个 PCIe 的 gen4 呢

1110
00:11:07,500 --> 00:11:11,600
就最上面的有 16 个 C2C 的接口

1111
00:11:32,300 --> 00:11:34,200
那这里面呢有非常多的 Superlane

1112
00:11:50,400 --> 00:11:51,700
传到 superlane 里面

1113
00:12:08,900 --> 00:12:11,066
是非常的有规律的

1114
00:12:12,100 --> 00:12:14,566
整个芯片呢是左右对称的

1115
00:12:25,500 --> 00:12:27,566
下一个 memory 来给到 MXM

1116
00:12:34,666 --> 00:12:36,766
或者下一个 Superlane 里面去执行

1117
00:13:25,100 --> 00:13:27,766
然后呢有 320B 的一个 SIMD

1118
00:13:36,000 --> 00:13:38,200
实际上呢这只是一个简单的 Unit

1119
00:13:47,666 --> 00:13:49,966
一个 VXM 专门算向量的

1120
00:13:53,900 --> 00:13:57,200
最后呢还有一个 MEM 专门做存储的

1121
00:14:06,600 --> 00:14:08,900
进行一个左右对称的排列

1122
00:14:32,400 --> 00:14:34,700
我们这里面呢有个 instruct dispatch

1123
00:14:42,000 --> 00:14:43,800
因为刚才有 320B 中向

1124
00:14:54,800 --> 00:14:55,666
看完指令之后

1125
00:14:57,000 --> 00:14:57,800
数据的流动呢

1126
00:15:26,600 --> 00:15:29,400
就是一条数据处理的流水线

1127
00:16:11,800 --> 00:16:13,500
这里面它需要的指令数呢

1128
00:16:17,100 --> 00:16:19,066
就变成了一个 144 位宽的

1129
00:16:21,200 --> 00:16:22,166
每个周期呢

1130
00:16:36,100 --> 00:16:38,100
没关系反正就是左右了

1131
00:16:52,500 --> 00:16:55,500
是在不同的 SIMD 里面去做一个流动的

1132
00:17:13,300 --> 00:17:15,500
有没有一种很明确的感觉

1133
00:17:23,400 --> 00:17:23,700
所以呢

1134
00:17:35,600 --> 00:17:38,866
实际上是交给不同的单元去执行的

1135
00:17:42,300 --> 00:17:43,100
TSP 里面呢

1136
00:17:51,400 --> 00:17:53,200
就是一切交给软件嘛

1137
00:17:56,900 --> 00:17:58,800
用单元把我们所有的每个单元的执行

1138
00:18:27,800 --> 00:18:29,966
各种各样的 SIMD 的处理单元

1139
00:18:37,600 --> 00:18:38,600
有两个 MEM 呢

1140
00:18:43,200 --> 00:18:44,800
我们现在呢逐个来打开一下

1141
00:18:49,800 --> 00:18:53,700
整个矩阵的单元呢包括 320 个 Mac

1142
00:19:09,200 --> 00:19:11,900
就组成一个 supercell

1143
00:19:14,600 --> 00:19:16,300
有两个 8B 的权重寄存器

1144
00:19:24,900 --> 00:19:27,700
同一时间呢能够处理 16 个行跟 16 个列

1145
00:19:41,200 --> 00:19:43,800
会通过右边的这个呢直接会传到里面

1146
00:19:58,200 --> 00:19:59,266
16 个计算单元

1147
00:20:02,900 --> 00:20:04,700
每个 ALU 呢可以使用数据流呢

1148
00:20:29,100 --> 00:20:30,866
一个呢就是 MXM 啊

1149
00:20:32,400 --> 00:20:35,100
那接着我们看一下中间的这一个模块

1150
00:20:39,700 --> 00:20:41,366
实际上有 20 个 Superlane

1151
00:20:44,000 --> 00:20:45,866
只有 5.5B 啊

1152
00:21:05,700 --> 00:21:07,600
所以一共呢单一个面呢

1153
00:21:18,100 --> 00:21:19,800
大家都说它不够用

1154
00:21:26,800 --> 00:21:30,166
stream registers 流式的寄存器

1155
00:21:41,300 --> 00:21:43,900
就是对我们的张量的数据呢进行 reshape

1156
00:21:57,100 --> 00:21:58,700
一个具体计算逻辑的

1157
00:22:08,500 --> 00:22:10,000
它的一个存储结构的

1158
00:22:20,500 --> 00:22:22,900
跟 cuda 的那种呢其实是一样的

1159
00:22:38,500 --> 00:22:39,100
然后 Bank

1160
00:22:48,900 --> 00:22:50,000
我们再看看细节的

1161
00:23:07,800 --> 00:23:10,466
用于片上的数据的一个存储

1162
00:23:37,100 --> 00:23:39,200
看看整体的系统的进行组网

1163
00:23:52,100 --> 00:23:54,300
所谓的 LP 就是 language process unit

1164
00:24:13,300 --> 00:24:15,000
我们要执行确定性的计算呢

1165
00:24:40,300 --> 00:24:41,666
就是 ZOMI 提出来的

1166
00:24:49,800 --> 00:24:52,466
你怎么通过软件去解决这些问题呢

1167
00:25:07,200 --> 00:25:09,800
这怎么去解决软件上

1168
00:25:14,800 --> 00:25:16,466
芯片实际上是 10 年立下了

1169
00:25:20,700 --> 00:25:23,500
来确保多个 TSP 之间的一个通讯呢

1170
00:25:33,600 --> 00:25:35,266
都会有明确的时间戳

1171
00:25:38,800 --> 00:25:39,900
要么呢整体的 runtime 呢

1172
00:26:33,500 --> 00:26:36,200
那一共呢就向右边的这个形态所设

1173
00:26:44,000 --> 00:26:46,966
那现在呢变成一个 GroqRack

1174
00:27:12,100 --> 00:27:15,500
能够提供非常高的一个节点的带宽

1175
00:27:22,500 --> 00:27:24,100
整个集群里面的一个带宽呢

1176
00:27:29,500 --> 00:27:31,200
我更加疑惑了

1177
00:29:17,800 --> 00:29:19,966
慢慢的进入到我们的计算的 bound

1178
00:29:20,600 --> 00:29:22,466
就必须要增加峰值的算力

1179
00:29:34,000 --> 00:29:36,266
在这一个节点增大了 Batch Size

1180
00:29:38,900 --> 00:29:41,000
那增大 Batch Size 吞吐增加的前提呢

1181
00:29:54,500 --> 00:29:56,500
采用的是一个自回归的方式嘛

1182
00:30:01,800 --> 00:30:03,266
然后呢再把两个生成 token

1183
00:30:16,800 --> 00:30:19,166
对我们的带宽的要求就非常的高了

1184
00:30:26,000 --> 00:30:27,900
受限于整个边 Bandwidth

1185
00:30:33,600 --> 00:30:36,466
刚才讲到的我们固定了一个 batch size

1186
00:31:11,000 --> 00:31:13,466
所以可以看到它的一个访存的带宽

1187
00:31:20,500 --> 00:31:22,200
但是这只是时延

1188
00:31:38,700 --> 00:31:41,266
反倒呢是把吞吐呢做的非常的大

1189
00:31:44,800 --> 00:31:48,466
就很明确的就知道两个产品的定位了

1190
00:32:13,400 --> 00:32:15,700
因为我们现在很多芯片的工艺

1191
00:32:18,400 --> 00:32:19,900
就是因为软件呢

1192
00:32:44,000 --> 00:32:44,500
而设计的

1193
00:33:37,100 --> 00:33:39,200
也就是我们刚才讲到的 KV 的占用的

1194
00:33:59,800 --> 00:34:02,666
就思考推理 3 年市场的具体的预测啊

1195
00:34:04,900 --> 00:34:05,266
首先呢

1196
00:35:06,600 --> 00:35:09,166
而且两台 H100 买不了五台 Groq

1197
00:35:56,600 --> 00:35:57,800
并不使用低时延的

1198
00:36:08,600 --> 00:36:11,366
而不是一开始一上来就考虑性价比的

1199
00:36:24,000 --> 00:36:25,066
Groq 的一个优势呢

1200
00:36:35,500 --> 00:36:38,066
大模型真正 C 端的应用还没有起来

1201
00:36:45,100 --> 00:36:47,300
真正的有用很多啊

1202
00:36:51,800 --> 00:36:53,066
直接进行对比

1203
00:37:22,500 --> 00:37:24,300
你不能用它来打

1204
00:37:59,200 --> 00:38:01,966
但但是有可能跟矿机是相似的

1205
00:38:04,600 --> 00:38:07,066
电力的成本呢会超过芯片的成本

1206
00:38:34,766 --> 00:38:36,566
所以呢我并不是很认同

1207
00:38:47,200 --> 00:38:49,100
未来 ZOMI 呢预测 Groq 这种呢

1208
00:38:53,200 --> 00:38:55,700
反正呢现在的 LM 大云模型呢

1209
00:39:24,900 --> 00:39:26,300
24 年去看待

1210
00:39:35,800 --> 00:39:36,566
那这个呢

1211
00:39:38,400 --> 00:39:39,466
腾讯+燧原

1212
00:00:00,466 --> 00:00:05,066
字幕生成：mkwei  字幕校准：mkwei

1213
00:00:08,666 --> 00:00:10,833
周五更到人憔悴

1214
00:00:12,766 --> 00:00:14,933
今天我们来洞察和分析一下

1215
00:00:24,400 --> 00:00:25,400
在这个视频里面呢

1216
00:00:38,800 --> 00:00:41,566
最关键的一个性能指标

1217
00:01:23,300 --> 00:01:25,300
对比 GPT4 或者其他

1218
00:01:31,966 --> 00:01:33,633
就做了一个整体的测评

1219
00:02:00,466 --> 00:02:02,933
它的推理的价格也是非常的惊人的

1220
00:02:54,466 --> 00:02:56,866
所以一般呢会设定 LLM 的

1221
00:05:29,266 --> 00:05:30,866
我输出一个单词

1222
00:09:56,966 --> 00:09:58,633
我们叫做 GroqNode

1223
00:10:23,266 --> 00:10:24,566
往右边看看啊

1224
00:12:19,200 --> 00:12:21,500
我们从第一个 west memory 呢

1225
00:12:41,600 --> 00:12:43,400
整体的硬件呢比较简单

1226
00:14:18,900 --> 00:14:21,800
哎哎现在是很明确了

1227
00:15:43,700 --> 00:15:44,966
左右对称

1228
00:37:37,766 --> 00:37:38,133
第一个呢

1229
00:39:13,366 --> 00:39:14,833
跟 DDR 一起做封测

