1
00:00:07,000 --> 00:00:08,600
曾经小小少年

2
00:00:17,566 --> 00:00:19,233
我们现在整体的回顾一下

3
00:00:42,300 --> 00:00:44,800
接着呢现在我们来到了第三个内容

4
00:00:51,000 --> 00:00:53,366
英伟达的 GPU 的整体的架构

5
00:01:10,466 --> 00:01:12,033
华为昇腾的这些

6
00:01:26,500 --> 00:01:29,800
一个是深入的打开英伟达的 GPU

7
00:01:29,800 --> 00:01:32,966
第二个呢深入的打开特斯拉的 DOJO

8
00:01:36,100 --> 00:01:39,600
TPU 里面呢咱们又分了四五个视频

9
00:01:39,600 --> 00:01:42,500
首先是整体回顾一下 TPU 的历史发展

10
00:01:55,300 --> 00:01:57,000
我们现在的大模型啊

11
00:01:57,300 --> 00:01:59,300
都是超级互联的方式

12
00:03:51,600 --> 00:03:52,200
实际上呢

13
00:03:59,000 --> 00:04:00,866
对应的就是我们的 V100 A100

14
00:04:11,866 --> 00:04:13,366
在这四年当中

15
00:04:44,700 --> 00:04:47,266
其实是真正的去面对我们的大模型

16
00:04:55,300 --> 00:04:55,666
第一篇呢

17
00:05:23,000 --> 00:05:26,000
但是不碍于我们去进行一个学习了解

18
00:05:30,000 --> 00:05:31,500
左边一个右边一个

19
00:05:35,100 --> 00:05:37,200
其中四个是相同重复的

20
00:05:37,200 --> 00:05:40,200
MXUMXU 就是我们在谷歌 TPUV1 里面

21
00:05:55,300 --> 00:05:57,100
为了降低我们电脑传输的时延

22
00:06:02,566 --> 00:06:05,966
来了解我们整个 TPU V4 的架构图之后呢

23
00:06:07,800 --> 00:06:10,466
TPUV4 的产品的具体的形态

24
00:06:37,400 --> 00:06:39,800
在谷歌 TPUV4 里面的具体的硬件呢

25
00:06:48,566 --> 00:06:49,766
一个互联的方式

26
00:06:49,766 --> 00:06:52,766
使得我们使得谷歌 TPUV4 Pod 里面

27
00:07:14,500 --> 00:07:17,700
这里面有两个单词叫做离散两个字

28
00:07:33,100 --> 00:07:34,400
这些稀数的特征呢

29
00:07:34,400 --> 00:07:35,000
实际上

30
00:07:40,766 --> 00:07:41,833
就它有个 index

31
00:07:45,466 --> 00:07:47,666
不是所有的数据都会有值

32
00:07:52,700 --> 00:07:53,666
我们的硬件呢

33
00:08:08,666 --> 00:08:10,466
把稀疏的分类的特征

34
00:08:19,066 --> 00:08:20,366
进行计算

35
00:08:32,066 --> 00:08:34,766
这是谷歌针对搜索场景里面最经典

36
00:08:42,766 --> 00:08:44,866
就是下面这层灰灰的 Embedding 层

37
00:08:46,900 --> 00:08:47,866
极度稀疏的数据

38
00:08:58,300 --> 00:09:01,166
主要就是专门针对我们的 Embedding 层

39
00:09:01,166 --> 00:09:02,933
进行一个加速

40
00:09:10,400 --> 00:09:12,900
就是支持 Embedding 的并行化

41
00:09:18,300 --> 00:09:19,366
或者用户的特征呢

42
00:09:21,766 --> 00:09:24,933
我们就需要在 Embedding 层进行一个切分

43
00:09:31,066 --> 00:09:32,966
能够在大部分的机器里面呢

44
00:09:35,066 --> 00:09:35,566
我们看一下

45
00:09:38,000 --> 00:09:40,466
一个具体的核心的架构

46
00:09:42,766 --> 00:09:44,666
跟大家一起去了解一下

47
00:09:47,966 --> 00:09:49,366
一共呢有 16 个

48
00:10:00,600 --> 00:10:01,800
一个是 Flush

49
00:10:01,966 --> 00:10:03,566
右边有个蓝色的比较大的框框

50
00:10:23,266 --> 00:10:24,866
计算 16tile 的结果之后呢

51
00:10:26,300 --> 00:10:27,266
要 concat 到一起

52
00:10:32,666 --> 00:10:34,633
但是具体呢确实没有办法打开

53
00:10:39,400 --> 00:10:40,666
或者第二个重要的特性呢

54
00:10:42,766 --> 00:10:46,233
Torus 呢我们叫做环面的一个互联的方式

55
00:11:25,900 --> 00:11:27,766
可以看到有绿色蓝色

56
00:11:44,600 --> 00:11:46,800
那下面我们简单的去看看

57
00:11:51,166 --> 00:11:52,733
使用的是 2D Torus

58
00:11:54,600 --> 00:11:55,966
在 TPUV4 里面呢

59
00:12:03,400 --> 00:12:05,266
可以切成最小的一个子单元

60
00:12:07,666 --> 00:12:10,333
我们刚才讲到的谷歌 TPUV4 里面的

61
00:12:22,700 --> 00:12:24,800
那立方体呢我们叫做 Cube

62
00:12:30,100 --> 00:12:33,266
注意了它是一个计算的核心数啊

63
00:12:36,766 --> 00:12:39,333
而不是指我们的芯片的数量

64
00:12:44,800 --> 00:12:45,866
因为今天的视频呢

65
00:12:48,500 --> 00:12:49,766
整理给下个视频

66
00:12:50,533 --> 00:12:52,200
我们将会在下个视频里面呢

67
00:12:56,500 --> 00:13:00,600
然后呢再看看一个光路由的交换机 OCS

68
00:13:00,600 --> 00:13:02,900
非常重要的一个内容

69
00:13:03,400 --> 00:13:03,600
最后呢

70
00:13:03,600 --> 00:13:06,600
再做一个简单的思考和优缺点的总结

71
00:00:01,266 --> 00:00:03,666
字幕生成：mkwei  字幕校准：mkwei

72
00:00:15,700 --> 00:00:17,566
里面的第四个内容

73
00:00:28,000 --> 00:00:31,000
看一下 a i 到底是怎么计算的

74
00:00:33,100 --> 00:00:34,466
跟大家一起去看了一下

75
00:00:34,466 --> 00:00:36,333
A i 的芯片基础

76
00:00:36,600 --> 00:00:39,266
所谓基础我们肯定要从 CPU

77
00:00:39,266 --> 00:00:42,333
GPU 到 NPU 开始看起

78
00:00:47,366 --> 00:00:49,933
去看看现在计算产业里面

79
00:00:53,366 --> 00:00:55,433
和它里面关于 AI 的计算

80
00:00:56,800 --> 00:01:01,066
Tensor Core 和 NV link 的一些具体的技术点

81
00:01:01,066 --> 00:01:02,266
把它打开

82
00:01:02,266 --> 00:01:03,566
在第四个内容里面呢

83
00:01:03,566 --> 00:01:05,533
我们来到了国外的 AI 芯片

84
00:01:09,400 --> 00:01:10,466
寒武纪的壁仞的

85
00:01:12,000 --> 00:01:13,600
国外呢又有特斯拉了

86
00:01:16,666 --> 00:01:18,466
今天呢我们深入的去看一下

87
00:01:18,466 --> 00:01:22,633
国外 AI 芯片里面的一个主要的内容

88
00:01:24,166 --> 00:01:26,533
我们分开了三个内容去介绍

89
00:01:44,300 --> 00:01:46,366
TPU2 啦 TPU3 啦

90
00:01:46,900 --> 00:01:48,266
现在我们来到了

91
00:01:49,666 --> 00:01:52,333
TPU4 超级互联

92
00:02:03,166 --> 00:02:04,966
而不是单张卡的算力

93
00:02:04,966 --> 00:02:06,433
非常牛逼

94
00:02:06,466 --> 00:02:10,066
整体的看看我们 TPU 历代的芯片啊

95
00:02:10,066 --> 00:02:11,233
TPU 不是我们的

96
00:02:13,200 --> 00:02:16,900
那整体其实它不仅仅只有四款芯片

97
00:02:16,900 --> 00:02:19,366
在 TPU 出现的时候

98
00:02:19,366 --> 00:02:22,033
中间呢还夹杂着其他的芯片

99
00:02:25,200 --> 00:02:26,200
Pixel 系列呢

100
00:02:26,200 --> 00:02:29,966
里面呢就就嵌入了很多 TPU 的模组

101
00:02:30,100 --> 00:02:31,900
一些 AI 的应用的史能

102
00:02:34,200 --> 00:02:35,866
还有 Pixel Neural Core

103
00:02:35,866 --> 00:02:38,866
还有谷歌的 Tensor 这三个系列

104
00:02:38,866 --> 00:02:42,566
那其他系列呢就包括 TPUV4I

105
00:02:42,566 --> 00:02:45,866
但是呢现在已经出了 TPUV5I

106
00:02:46,466 --> 00:02:47,433
在整个 TPU 历代

107
00:02:50,500 --> 00:02:53,866
ZOMI 最关心的指标其实有三个

108
00:02:53,866 --> 00:02:54,533
那第一个呢

109
00:02:57,100 --> 00:02:57,600
算力呢

110
00:02:57,600 --> 00:03:01,600
决定我们单张卡能够执行多快的速率

111
00:03:01,600 --> 00:03:03,466
接着呢我们还有另外两个指标

112
00:03:03,466 --> 00:03:04,833
一个是内存

113
00:03:06,600 --> 00:03:08,366
还有显存的带宽

114
00:03:08,366 --> 00:03:12,133
这两个呢是决定我们的峰值的上限

115
00:03:12,700 --> 00:03:15,200
现在我们来横向呢再对比一下

116
00:03:15,200 --> 00:03:16,466
实际上啊

117
00:03:16,466 --> 00:03:19,733
TPU 的回归经历了四年

118
00:03:24,466 --> 00:03:28,633
直到 2022 年 TPUV4 第四个版本才出来

119
00:03:30,266 --> 00:03:31,033
这四年呢

120
00:03:32,366 --> 00:03:33,433
一点都没落下

121
00:03:35,800 --> 00:03:37,066
也是非常大的

122
00:03:37,066 --> 00:03:39,333
促进了整个 AI 生态的发展

123
00:03:39,600 --> 00:03:41,200
那为什么谷歌的 TPU

124
00:03:41,200 --> 00:03:43,966
它的硬件历时了四年才发布

125
00:03:43,966 --> 00:03:44,666
呢我们现在呢

126
00:03:44,666 --> 00:03:47,033
等一下跟大家一起深入的探讨一下

127
00:03:49,966 --> 00:03:51,633
接着呢我们看一下竞品啊

128
00:03:52,200 --> 00:03:55,700
这四年英伟达已经发布了三代架构

129
00:03:55,700 --> 00:03:57,866
一台是 Volta Amber

130
00:03:57,866 --> 00:03:59,033
还有 Hopper 架构

131
00:04:00,866 --> 00:04:01,833
现在用的最多的

132
00:04:05,300 --> 00:04:06,300
这么一个架构

133
00:04:06,300 --> 00:04:09,866
现在的英伟达雅越来越 AI 了

134
00:04:09,866 --> 00:04:11,333
那看框架呢

135
00:04:13,366 --> 00:04:16,433
谷歌的 Tensorflow 基本上已经没有人用了

136
00:04:19,366 --> 00:04:21,466
现在提到训练大模型

137
00:04:21,466 --> 00:04:23,433
基本上大家都会用 PyTorch

138
00:04:26,566 --> 00:04:28,666
当然呢还有一些像 oneflow 啊

139
00:04:28,666 --> 00:04:29,366
像 Paddle 啊

140
00:04:29,366 --> 00:04:30,533
还有 Mindspore

141
00:04:32,866 --> 00:04:37,133
但是整个业界的趋势确实势不可挡

142
00:04:37,300 --> 00:04:38,366
之后呢我们看看技术

143
00:04:38,366 --> 00:04:40,166
就是大模型的涌现

144
00:04:40,166 --> 00:04:42,633
对大规模的算力需求的增长

145
00:04:47,266 --> 00:04:51,133
怎么用起我们澎湃的集群的算力

146
00:04:52,766 --> 00:04:53,933
今天跟大家分享的

147
00:04:55,666 --> 00:04:58,066
就是谷歌 TPUV4 的自己发表的论文

148
00:04:58,066 --> 00:04:58,466
第二篇呢

149
00:04:58,466 --> 00:05:01,333
就是谷歌 TPUV4 对应的光模块

150
00:05:03,000 --> 00:05:03,600
那现在呢

151
00:05:03,600 --> 00:05:04,400
我们事不宜迟

152
00:05:04,400 --> 00:05:07,066
马上呢进入到第一个内容

153
00:05:07,066 --> 00:05:08,133
虽然前奏多了点

154
00:05:09,000 --> 00:05:09,566
就是整体

155
00:05:09,566 --> 00:05:13,233
看看谷歌 TPUV4 的一些芯片的概况

156
00:05:14,400 --> 00:05:15,100
毫无悬念的

157
00:05:15,100 --> 00:05:17,566
我们来到了谷歌 TPUV4 里面的第一个内容

158
00:05:17,566 --> 00:05:18,466
肯定是去看看

159
00:05:18,466 --> 00:05:21,166
谷歌 TPUV4 里面的整体的架构

160
00:05:21,166 --> 00:05:21,766
那下面这

161
00:05:21,766 --> 00:05:23,033
个图呢虽然模糊了一点

162
00:05:26,366 --> 00:05:28,433
每个谷歌 TPV4 里面的架构呢

163
00:05:31,500 --> 00:05:35,100
而每个 Tensor Core 里面呢有分别六个单元

164
00:05:42,566 --> 00:05:44,033
或者叫做脉冲阵列

165
00:05:46,200 --> 00:05:47,300
另外呢还有两个单元

166
00:05:47,300 --> 00:05:48,900
一个呢叫做 scalar unit

167
00:05:48,900 --> 00:05:50,400
专门算我们的标量

168
00:05:50,400 --> 00:05:52,300
第二个呢就是 Vector Unit

169
00:05:52,300 --> 00:05:54,700
专门算具体的向量

170
00:05:57,100 --> 00:05:59,300
因此呢 HBM 我们的内存带宽呢

171
00:05:59,300 --> 00:06:02,566
分别位于我们的 Tensor Core 的左右两边

172
00:06:05,966 --> 00:06:07,833
我们现在来看看整个谷歌

173
00:06:11,566 --> 00:06:14,533
对比起谷歌 TPUV2V 3 的整体产品形态

174
00:06:16,666 --> 00:06:18,933
下面我们来枚举几个具体的内容

175
00:06:21,000 --> 00:06:22,500
从 16 纳米到 7 纳米

176
00:06:22,500 --> 00:06:23,700
这是第一个

177
00:06:24,266 --> 00:06:26,733
第二个呢就是内部的缓存增加了 9 倍

178
00:06:31,366 --> 00:06:33,466
而 HBM 呢依然是没有变的

179
00:06:33,466 --> 00:06:35,233
保持 32 GB

180
00:06:36,900 --> 00:06:37,400
另外的话

181
00:06:39,800 --> 00:06:41,266
增加了 Sparse core

182
00:06:41,266 --> 00:06:44,033
就是我们的稀疏的核心的计算

183
00:06:44,566 --> 00:06:46,633
最后在谷歌 TPU V4 的机器里面呢

184
00:06:55,366 --> 00:06:58,433
那下面我们简单的去看看几个比较新

185
00:06:59,400 --> 00:07:02,500
我们谷歌 TPUV4 里面的核心的一个模块

186
00:07:02,500 --> 00:07:04,000
就是我们的 sparse core

187
00:07:04,000 --> 00:07:06,466
专门做 Embedding 层的理解

188
00:07:06,466 --> 00:07:07,033
那下面呢

189
00:07:10,966 --> 00:07:11,366
Embedding

190
00:07:11,366 --> 00:07:11,733
一开始

191
00:07:13,900 --> 00:07:14,500
注意呢

192
00:07:17,700 --> 00:07:19,900
专门是用来做稀疏化的

193
00:07:19,900 --> 00:07:21,900
一个典型的方式

194
00:07:22,000 --> 00:07:23,066
在实际 NLP

195
00:07:23,066 --> 00:07:25,466
就自然语言处理和搜索的算法

196
00:07:25,466 --> 00:07:25,933
实际上呢

197
00:07:26,966 --> 00:07:28,733
或者我们字符的形式的输入

198
00:07:30,266 --> 00:07:32,833
它就是非常稀疏的一些数据

199
00:07:35,000 --> 00:07:37,100
对我们应试到硬件的矩阵方法里面

200
00:07:37,100 --> 00:07:38,966
是非常的不友好

201
00:07:38,966 --> 00:07:40,766
因为它更像一个哈西表

202
00:07:43,366 --> 00:07:44,066
里面的数据呢

203
00:07:44,066 --> 00:07:45,466
大量的稀数的意思就是

204
00:07:47,666 --> 00:07:48,933
可能有大量的 0

205
00:07:51,200 --> 00:07:52,700
在真正深度学习里面呢

206
00:07:53,666 --> 00:07:54,233
一般呢

207
00:07:57,100 --> 00:07:59,666
计算的就是每个矩阵呢都会有值

208
00:07:59,700 --> 00:08:03,600
在真正输到我们的神经网络模型之前

209
00:08:03,600 --> 00:08:05,966
呢就会加一个 Embedding 层

210
00:08:05,966 --> 00:08:08,533
就是 Embedding 层左边的这个模块

211
00:08:10,466 --> 00:08:12,733
转换成为空间更小

212
00:08:14,900 --> 00:08:15,800
然后呢

213
00:08:15,800 --> 00:08:19,066
再给到我们深度学习的算法网络模型

214
00:08:21,000 --> 00:08:22,800
虽然呢刚才讲的有点抽象

215
00:08:22,800 --> 00:08:25,700
我们下面呢一个简单的推荐的例子

216
00:08:25,700 --> 00:08:28,200
我们的搜网推的一个应用场景

217
00:08:28,200 --> 00:08:29,600
刚才就有个更感性的

218
00:08:29,600 --> 00:08:30,566
了解到这个网络了

219
00:08:30,566 --> 00:08:32,066
就是 Wide and Deep model

220
00:08:34,766 --> 00:08:36,733
最经典的一篇论文

221
00:08:37,166 --> 00:08:39,433
在真正点进去神经网络模型

222
00:08:40,700 --> 00:08:42,766
之前呢会有一层 Embedding

223
00:08:44,966 --> 00:08:46,933
去把我们的稀疏的数据

224
00:08:47,866 --> 00:08:50,833
变成一些比较稠密的数据或者矩阵

225
00:08:53,100 --> 00:08:55,500
这个呢就是 Embedding 层的作用

226
00:08:55,500 --> 00:08:58,300
而谷歌 TPUV4 里面的 sparse core 呢

227
00:09:04,600 --> 00:09:06,466
支持 Embedding 层的加速以外呢

228
00:09:06,466 --> 00:09:07,633
谷歌 TPUV4 里面呢

229
00:09:12,900 --> 00:09:15,300
也就是在我们的神经网络里面啊

230
00:09:15,566 --> 00:09:18,333
NLP 或者我们的推荐的一些推荐的语料

231
00:09:19,366 --> 00:09:21,333
非常非常的多的时候

232
00:09:24,900 --> 00:09:26,600
于是谷歌 TPU V4

233
00:09:26,600 --> 00:09:28,766
直接硬件上面支持这个特性

234
00:09:28,766 --> 00:09:31,066
做到 Embedding 层的并行

235
00:09:32,966 --> 00:09:34,633
做一个非常好的处理

236
00:09:35,566 --> 00:09:38,033
谷歌 TPU V4 里面的 sparse core 的

237
00:09:40,466 --> 00:09:42,766
下面这个图我们深入了打开

238
00:09:45,166 --> 00:09:46,466
在整个 Sparse core 里面呢

239
00:09:46,466 --> 00:09:47,966
最核心的就是这个模块

240
00:09:49,366 --> 00:09:52,366
每个 SC 就是我们的 Sparse core 呢有 16 个 tile

241
00:09:52,366 --> 00:09:55,533
我们称为这个一个 tile 里面一共有 16 个

242
00:09:57,466 --> 00:09:58,533
其实有三个

243
00:09:59,366 --> 00:10:00,633
一个是 seVPU

244
00:10:03,566 --> 00:10:05,433
叫做 SPMEM

245
00:10:05,700 --> 00:10:07,600
啊实际上就是 SPAS

246
00:10:07,600 --> 00:10:11,200
memory 里面呢有 2.5 MB 的缓存

247
00:10:11,266 --> 00:10:15,233
这里面最核心最核心的属于 SCVPU 了

248
00:10:17,600 --> 00:10:20,500
快速对我们的稀疏的数据呢进行计算

249
00:10:20,500 --> 00:10:22,900
一切的计算呢都依赖于这个模块

250
00:10:24,866 --> 00:10:26,333
就会把结果获取出来

251
00:10:27,266 --> 00:10:29,133
然后对外进行一个输出

252
00:10:30,766 --> 00:10:32,666
基本上还是相对比较简单

253
00:10:38,566 --> 00:10:39,433
第二个重点呢

254
00:10:40,666 --> 00:10:42,766
就是 3D Torus 的一个互联的方式

255
00:10:46,200 --> 00:10:48,166
通过 3D Torus 环面的方式呢

256
00:10:48,166 --> 00:10:52,366
可以紧密的把 4096 个 TPUV4 引擎

257
00:10:52,366 --> 00:10:53,133
一共合起来

258
00:10:57,866 --> 00:10:59,066
注意了敲重点呢

259
00:10:59,066 --> 00:11:01,766
这里面提供的是超过 e 级的

260
00:11:01,766 --> 00:11:03,433
一个 BF16 的算力

261
00:11:06,700 --> 00:11:09,266
端口呢我们可以看到一款的 TPU 呢

262
00:11:09,266 --> 00:11:11,533
因为它有 4 块 TPU 嘛

263
00:11:15,500 --> 00:11:18,200
每块 TPU 里呢又有一个单独的网口

264
00:11:18,200 --> 00:11:19,000
每块网口呢

265
00:11:19,000 --> 00:11:21,900
是会连接到一个具体的光路由

266
00:11:21,900 --> 00:11:23,966
或者光模块的交换机

267
00:11:23,966 --> 00:11:25,933
上面右边的这个产品图呢

268
00:11:27,766 --> 00:11:30,266
红色黄色不同的线

269
00:11:30,700 --> 00:11:32,900
其实连接的是一个液冷

270
00:11:32,900 --> 00:11:35,366
所以可以看到这边有些液冷的管

271
00:11:35,366 --> 00:11:37,333
实际上我们更关心的是

272
00:11:39,766 --> 00:11:42,766
还有这个网口的模块

273
00:11:42,766 --> 00:11:44,633
连接的是什么交换机

274
00:11:46,800 --> 00:11:48,466
3D Torus 环面的一个

275
00:11:48,466 --> 00:11:49,933
互联的方式

276
00:11:52,700 --> 00:11:54,600
也就是 2D 环面的互联方式

277
00:11:55,966 --> 00:11:57,566
提供的是一个 3D

278
00:11:57,566 --> 00:11:58,833
在谷歌 TPUV4 里面呢

279
00:12:05,266 --> 00:12:06,533
2 乘以 2 乘以一

280
00:12:10,300 --> 00:12:11,566
具体的一块芯片

281
00:12:11,566 --> 00:12:13,966
里面呢就有两个 Tensor Core

282
00:12:14,000 --> 00:12:15,766
那右边的这个图我们可以看到

283
00:12:15,766 --> 00:12:19,033
虽然它的一个切分方式非常的多

284
00:12:20,000 --> 00:12:22,700
会组成一个不同的一个立方体

285
00:12:24,800 --> 00:12:26,266
那整个立方体里面呢

286
00:12:26,266 --> 00:12:28,733
有非常多的这种小圆圈节点

287
00:12:33,266 --> 00:12:35,366
也就是我们的 MXU 的核心数

288
00:12:35,366 --> 00:12:36,766
脉动阵列的核心数

289
00:12:39,300 --> 00:12:41,100
所以我们的 3D 的库源呢

290
00:12:41,100 --> 00:12:42,000
首先第一个呢

291
00:12:42,000 --> 00:12:44,800
就是指我们的计算的核心数

292
00:12:45,833 --> 00:12:47,033
废话讲了多了一点

293
00:12:52,200 --> 00:12:54,166
去展开谷歌 TPU V4 里面

294
00:13:06,600 --> 00:13:08,266
那今天的内容呢就到这里为止

295
00:13:10,700 --> 00:13:12,500
谷歌 TPUv4 里面的

296
00:00:11,700 --> 00:00:15,700
今天呢我们来到了谷歌 TPU 的系列

297
00:00:23,600 --> 00:00:24,266
那首先呢

298
00:00:24,266 --> 00:00:25,033
我们第一个呢

299
00:00:44,800 --> 00:00:46,966
就是 GPU 的详解

300
00:00:49,900 --> 00:00:51,000
市值最高的

301
00:00:55,400 --> 00:00:56,800
非常核心的两个内容

302
00:01:05,500 --> 00:01:09,400
因为国内呢有很多 AI 芯片的厂商

303
00:01:22,600 --> 00:01:24,166
那国外 AI 芯片之前呢

304
00:01:32,966 --> 00:01:36,033
接着呢我们深入的打开谷歌 TPU

305
00:01:42,600 --> 00:01:44,300
接着呢就 TPU1 啦

306
00:01:48,266 --> 00:01:49,666
TPU 最后一个内容

307
00:01:52,300 --> 00:01:55,300
谈到超级互联就非常有意思了

308
00:01:59,300 --> 00:02:03,166
去提供非常澎湃的集群的算力

309
00:02:11,200 --> 00:02:13,200
是美国谷歌的

310
00:02:22,000 --> 00:02:25,200
特别是谷歌自研的安卓手机

311
00:02:47,400 --> 00:02:50,500
芯片的一个横向的对比图里面呢

312
00:02:54,500 --> 00:02:57,100
最重要就是提供多少的算力

313
00:03:04,800 --> 00:03:06,600
我们的显存能有多大

314
00:03:19,700 --> 00:03:24,466
TPU V3 呢是在 2018 年的时候开始发布的

315
00:03:28,600 --> 00:03:30,266
中间呢隔了四年

316
00:03:31,000 --> 00:03:32,366
谷歌在 AI 论文当中呢

317
00:03:33,400 --> 00:03:35,800
发了非常非常多的经典的论文

318
00:03:47,000 --> 00:03:49,966
这四年谷歌到底发布了哪些东西

319
00:04:01,800 --> 00:04:05,300
还有大家可能现在买都买不到的 H100

320
00:04:11,300 --> 00:04:11,866
实际上啊

321
00:04:16,400 --> 00:04:19,366
PyTorch 成为了整个 AI 框架的王者

322
00:04:23,400 --> 00:04:26,566
我好像找不到第二个框架了

323
00:04:30,500 --> 00:04:32,866
仍然也可以用来训练大模型

324
00:04:42,600 --> 00:04:44,700
所以呢谷歌 TPUV4 的出现

325
00:04:53,900 --> 00:04:55,300
主要来自于两篇论文

326
00:05:01,300 --> 00:05:03,000
OCS 所发表的论文

327
00:05:08,100 --> 00:05:09,000
我们第一个内容呢

328
00:05:28,400 --> 00:05:30,000
一共有两个 Tensor Core

329
00:05:44,000 --> 00:05:46,200
专门用来做张量的计算的

330
00:06:10,466 --> 00:06:11,566
就是右边的这个图

331
00:06:14,500 --> 00:06:16,500
变化差异非常的大

332
00:06:18,900 --> 00:06:21,000
首先第一个内容呢就是制造的工艺呢

333
00:06:26,700 --> 00:06:31,366
到 244 MB 内部的缓存不是指我们的 HBM

334
00:06:35,200 --> 00:06:36,900
这是三个主要的改变点

335
00:06:46,600 --> 00:06:48,566
增加了一个叫做 3D torus 的

336
00:06:52,766 --> 00:06:55,366
澎湃的峰值的算力

337
00:06:58,400 --> 00:06:59,400
的内容特别是指

338
00:07:07,000 --> 00:07:10,400
我们来了解一下什么是 Embedding 层

339
00:07:11,700 --> 00:07:13,900
主要是处理一些离散的分类的特征

340
00:07:25,900 --> 00:07:26,966
我们字符串呢

341
00:07:28,700 --> 00:07:30,266
是极度的离散的

342
00:07:41,800 --> 00:07:43,000
有个索引

343
00:07:48,900 --> 00:07:50,800
在我们整个矩阵单元里面

344
00:07:54,200 --> 00:07:57,100
是比较适合处理一些稠密的矩阵的

345
00:08:12,700 --> 00:08:14,900
密度更稠密的 Tensor

346
00:08:39,400 --> 00:08:40,700
就是我们的 hidden layer

347
00:08:50,800 --> 00:08:53,100
然后再给到我们的神经网络进行计算

348
00:09:07,600 --> 00:09:10,400
还提供了一个比较有意思的点

349
00:09:21,300 --> 00:09:21,766
于是呢

350
00:09:34,600 --> 00:09:35,066
那下面呢

351
00:09:55,500 --> 00:09:57,466
现在我们打开最核心的模块叫核心呢

352
00:09:58,500 --> 00:09:59,366
一个呢是 Fecth

353
00:10:15,200 --> 00:10:17,600
它是一个 8 比特的 SIMD 的一个单元

354
00:10:29,100 --> 00:10:30,766
整体的核心的架构呢

355
00:10:53,100 --> 00:10:57,866
提供 1.126e BF16 的一个计算的算力

356
00:11:03,400 --> 00:11:06,700
注意这里面是 BF16 而不是 BF32

357
00:11:11,500 --> 00:11:15,300
这里面一块两块 3 块 4 块

358
00:11:37,300 --> 00:11:39,766
中间这个最核心的模块

359
00:11:49,900 --> 00:11:51,166
在谷歌 TPUV2 里面呢

360
00:11:58,800 --> 00:12:03,300
可以把整个 4096 的节点切分得更细

361
00:12:06,500 --> 00:12:07,666
也就是一款

362
00:12:19,000 --> 00:12:20,000
不同的切分方式呢

363
00:12:28,700 --> 00:12:30,100
每个小圆圈节点呢

364
00:12:54,133 --> 00:12:55,900
整体的 pod 的形态

365
00:13:08,233 --> 00:13:10,700
我们将会在下一个内容里面去打开

366
00:00:04,966 --> 00:00:07,033
hello 大家好

367
00:00:08,600 --> 00:00:11,400
如今我们都又肥又圆的 ZOMI

368
00:00:19,200 --> 00:00:20,200
在 a i 芯片

369
00:00:20,200 --> 00:00:23,600
我们给大家分享了哪些知识点

370
00:00:25,000 --> 00:00:28,000
分享了 a i 的整个计算的体系

371
00:00:31,000 --> 00:00:33,100
接着呢我们在第二个内容的时候呢

372
00:01:13,600 --> 00:01:16,300
谷歌的 TPU 了非常的多

373
00:02:31,900 --> 00:02:34,200
那中间呢经历过 Edge V1

374
00:05:40,200 --> 00:05:42,566
跟大家介绍的脉动阵列

375
00:09:03,266 --> 00:09:04,633
除了单点的每块芯片呢

376
00:10:34,600 --> 00:10:38,566
因为具体的核心属于在谷歌 TPUV4 里面

377
00:12:47,033 --> 00:12:48,500
我们的时间又拖长了

