1
00:00:25,666 --> 00:00:26,866
那实际上我们回顾一下

2
00:00:50,766 --> 00:00:53,066
是谷歌 TPU 的整个系列的架构

3
00:00:54,666 --> 00:00:55,866
我们详细的分享了

4
00:00:55,866 --> 00:00:57,133
英伟达的 GPU 的架构

5
00:01:29,400 --> 00:01:31,800
接着我们看一下 TPU3 整体的性能

6
00:01:38,166 --> 00:01:40,733
比较惊人的互联的技术点

7
00:01:54,200 --> 00:01:55,900
现在我们的安卓手机市场

8
00:01:56,000 --> 00:01:57,266
用户越来越多了

9
00:02:34,466 --> 00:02:37,166
应该是在 2013 年就开始立项了

10
00:02:41,400 --> 00:02:41,866
所以说

11
00:02:41,866 --> 00:02:45,366
谷歌在这一方面确实非常具有前瞻性

12
00:02:47,100 --> 00:02:48,366
快速的来到第二个内容

13
00:03:08,300 --> 00:03:10,600
就还有谷歌最近自己推出的一个

14
00:03:20,200 --> 00:03:21,200
所以这里面可以看到

15
00:03:22,800 --> 00:03:24,900
芯片的情况和规格呢

16
00:03:30,566 --> 00:03:31,666
我们将会在后面呢

17
00:03:41,166 --> 00:03:42,166
它封装成 v1v2v3

18
00:04:22,766 --> 00:04:26,033
TPUV4I 呢其实跟二三差不多

19
00:04:32,700 --> 00:04:33,400
这里面可以看到

20
00:04:33,500 --> 00:04:35,800
确实整体的产品形态很奇怪哦

21
00:04:55,166 --> 00:04:55,733
实际上呢

22
00:05:10,766 --> 00:05:12,466
那这些 AI 的加速芯片呢

23
00:05:19,900 --> 00:05:22,200
这整一块 Soc 呢是谷歌自己设计的

24
00:05:36,266 --> 00:05:38,266
我们来到了第三个内容

25
00:05:40,366 --> 00:05:42,466
整体去看看 TPU 有什么不一样

26
00:05:45,566 --> 00:05:51,066
它是一个 deterministic deterministic deterministic

27
00:05:54,200 --> 00:05:55,466
Execution model

28
00:05:56,800 --> 00:05:57,466
实际上呢

29
00:05:57,466 --> 00:05:59,766
TPU 是一个确定性的执行模型

30
00:05:59,766 --> 00:06:00,333
第一代呢

31
00:06:05,100 --> 00:06:06,500
还是在 28 纳米

32
00:06:10,566 --> 00:06:12,733
现在当时候的 TPUV1 的主频呢

33
00:06:16,266 --> 00:06:18,433
当时候的 CPU 的主频呢

34
00:06:20,966 --> 00:06:23,066
不过呢整体功耗呢也是比较感人的

35
00:06:27,800 --> 00:06:29,800
谷歌的一个服务器集群里面呢

36
00:06:38,166 --> 00:06:39,366
这种呢就是 Pcie

37
00:06:42,966 --> 00:06:45,666
跟 CPU host 主机相连

38
00:07:06,700 --> 00:07:08,666
大量的去处理我们的自然语言处理

39
00:07:17,666 --> 00:07:19,266
不过呢 RNN 跟 LSTM 呢

40
00:07:33,566 --> 00:07:35,666
因为它的逻辑非常复杂

41
00:07:35,900 --> 00:07:36,800
在这个视频里面

42
00:07:36,866 --> 00:07:38,833
我们后面会介绍一下 TPU 啊

43
00:07:41,666 --> 00:07:44,066
V4 这整体的系列里面呢

44
00:07:59,200 --> 00:08:02,000
去减少我们没有用的额外的开销

45
00:08:06,666 --> 00:08:08,133
就是低精度啊

46
00:08:08,166 --> 00:08:09,466
它的 TPUV1 呢

47
00:08:14,566 --> 00:08:16,333
预测的最小值和最大值之间的

48
00:08:17,366 --> 00:08:19,333
所谓的量化的技术

49
00:08:20,866 --> 00:08:22,966
原来的神经网络的数据的表示呢

50
00:09:01,800 --> 00:09:03,400
都要从计算器里面

51
00:09:17,900 --> 00:09:19,866
右边的这个图所示呢

52
00:09:33,566 --> 00:09:35,666
我少了很多取指的问题

53
00:09:37,766 --> 00:09:39,466
直接的去把数据执行完之后

54
00:09:40,300 --> 00:09:40,900
非常的

55
00:09:47,866 --> 00:09:48,466
脉动阵列了

56
00:09:49,266 --> 00:09:51,133
假设呢我们下面的灰色的

57
00:09:52,666 --> 00:09:53,233
那一般呢

58
00:10:08,800 --> 00:10:09,300
第一次的

59
00:10:14,666 --> 00:10:17,433
变成我们的 Y12 跟 Y21 的数据的计算

60
00:10:19,100 --> 00:10:21,466
去计算我们的 Y22 跟 Y31

61
00:10:23,666 --> 00:10:24,666
都丢到我们的

62
00:10:24,700 --> 00:10:26,766
那个整体的逻辑计算当中

63
00:10:32,566 --> 00:10:34,733
就是我们的脉动阵列

64
00:10:49,600 --> 00:10:52,366
我们的数据到底是怎么串流的

65
00:11:05,966 --> 00:11:09,966
这个硬件呢就是 MXU Matrix multiply unit

66
00:11:13,100 --> 00:11:14,466
就是一个大的脉动阵列

67
00:11:16,566 --> 00:11:17,766
这里面呢 control 呢

68
00:11:24,600 --> 00:11:28,100
就是输入我们刚才讲到的输入的数据

69
00:11:37,300 --> 00:11:40,400
先固定在我们整个 systolic array

70
00:11:47,000 --> 00:11:49,066
最后得到完整的输出

71
00:11:49,100 --> 00:11:52,200
然后再输出给我们的计算的结果

72
00:11:53,300 --> 00:11:54,100
就类似于这样

73
00:12:14,300 --> 00:12:15,100
是在两年后

74
00:12:15,166 --> 00:12:16,633
2017 年 5 月份发布的

75
00:12:16,700 --> 00:12:19,600
使用了 16GB 的高带宽内存 HBM

76
00:12:31,266 --> 00:12:33,966
然后要用上 BF16 的一个原因

77
00:12:38,866 --> 00:12:39,966
实际上的整体架构呢

78
00:12:40,066 --> 00:12:42,233
跟谷歌 TPU V1 差不多

79
00:12:54,066 --> 00:12:55,466
谈到 BF 16 呢

80
00:13:05,466 --> 00:13:07,766
但是呢 FP16 的它

81
00:13:07,800 --> 00:13:09,900
的指数位呢只有五位

82
00:13:18,566 --> 00:13:21,133
从 10 个比特变成了 7 个比特

83
00:13:39,566 --> 00:13:40,866
我们的内存的消耗

84
00:13:44,900 --> 00:13:45,700
第三个优点呢

85
00:13:45,766 --> 00:13:47,933
就是结合了第一个优点和第二个优点

86
00:13:52,466 --> 00:13:54,533
这个就引入了 BF16 的好处

87
00:13:54,566 --> 00:13:55,833
而 TPUV2 之后呢

88
00:14:06,700 --> 00:14:07,966
使得 2017 年的时候

89
00:14:07,966 --> 00:14:09,166
谷歌在 ResNet50 呢

90
00:14:15,066 --> 00:14:16,766
这也是非常夸张的

91
00:14:35,366 --> 00:14:37,233
那接着我们会介绍最后一个内容

92
00:14:54,200 --> 00:14:56,200
从 TPU 的 V1 V2 V3 呢

93
00:15:00,766 --> 00:15:02,866
总体 MXU 的数量呢翻了一番

94
00:15:04,300 --> 00:15:07,900
而且首次亮相了 3D torus 的互联的方式

95
00:15:14,666 --> 00:15:17,066
它的一个互联的具体的形态

96
00:15:19,266 --> 00:15:20,533
还有它的加构图

97
00:15:25,400 --> 00:15:27,700
去回顾了一下 TPU 的历史的发展

98
00:15:40,600 --> 00:15:44,666
GPU 和 TPU 英伟达的 GPU 跟谷歌的 GPU

99
00:15:44,700 --> 00:15:47,266
最大的区别在哪些方面呢

100
00:15:47,766 --> 00:15:48,233
这里面呢

101
00:15:58,666 --> 00:15:59,266
第二个呢

102
00:16:01,800 --> 00:16:02,900
包括我们的机间互联

103
00:16:05,300 --> 00:16:05,800
第三个呢

104
00:16:20,800 --> 00:16:22,766
带来一点不一样的思考

105
00:16:26,300 --> 00:16:28,100
一对一的全部都展开

106
00:16:30,566 --> 00:16:31,833
欢迎打开和吐槽

107
00:00:08,866 --> 00:00:10,533
周末自己在家吃土

108
00:00:15,900 --> 00:00:17,866
我们来到了一个不一样的内容啊

109
00:00:17,866 --> 00:00:19,566
就是谷歌的 TPU

110
00:00:19,766 --> 00:00:22,333
看一下整个谷歌 TPU 的历史发展

111
00:00:22,400 --> 00:00:25,366
那讲到历史发展呢是今天的主要内容

112
00:00:28,266 --> 00:00:30,133
我们讲到的 AI 芯片系列

113
00:00:30,300 --> 00:00:33,966
从 AI 的计算体系再到 AI 的芯片基础

114
00:00:33,966 --> 00:00:37,066
从 CPU GPU 到现在的 NPU

115
00:00:37,300 --> 00:00:39,300
在 NPU 里面呢我们分开两个内容

116
00:00:39,366 --> 00:00:40,933
一个是跟国内相关的

117
00:00:42,766 --> 00:00:45,566
现在我们来到了国外相关的谷歌 TPU

118
00:00:45,666 --> 00:00:48,466
谷歌 TPU 里面呢有两个核心的内容

119
00:00:48,500 --> 00:00:50,366
一个呢是它的脉动阵列

120
00:00:50,400 --> 00:00:50,766
第二个呢

121
00:00:59,800 --> 00:01:00,700
接着我们今天呢

122
00:01:00,700 --> 00:01:02,466
来到谷歌 TPU 芯片的架构了

123
00:01:02,466 --> 00:01:04,033
详细的展开

124
00:01:04,066 --> 00:01:04,866
那展开之后呢

125
00:01:04,866 --> 00:01:06,466
我们会分开 4 个内容

126
00:01:06,466 --> 00:01:08,533
或者 5 个内容给大家去汇报的

127
00:01:08,566 --> 00:01:08,933
第一个呢

128
00:01:12,400 --> 00:01:12,866
第二个呢

129
00:01:12,866 --> 00:01:15,766
就是 TPU1234 里面都用到的

130
00:01:15,866 --> 00:01:18,166
一个脉动阵列的细节的展开

131
00:01:18,266 --> 00:01:20,366
接着我们看一下 TPU2

132
00:01:20,500 --> 00:01:23,700
第一款训练卡或者训练芯片的提出

133
00:01:23,766 --> 00:01:26,566
因为 TPU1 呢它主要是指推理吧

134
00:01:26,566 --> 00:01:29,333
TPU234 都是有训练功能了

135
00:01:33,666 --> 00:01:35,966
pod 呢就是一台呃超算了

136
00:01:36,466 --> 00:01:38,166
最后呢我们看一下 TPU 是有哪些

137
00:01:42,666 --> 00:01:44,366
好了我们现在来到第一个内容

138
00:01:44,366 --> 00:01:45,833
去看看 TPU 的诞生

139
00:01:45,900 --> 00:01:48,066
为什么会出现 TPU 这个概念

140
00:01:48,266 --> 00:01:50,033
其实呢是在 2013 年的时候啊

141
00:01:59,966 --> 00:02:02,533
都是基于谷歌的计算中心来去计算的

142
00:02:02,800 --> 00:02:05,600
而 AI 算力的消耗非常感人

143
00:02:05,666 --> 00:02:06,633
于是这个时候呢

144
00:02:06,700 --> 00:02:07,400
谷歌就觉得

145
00:02:07,400 --> 00:02:09,866
有必要自己去根据 AI 的范式

146
00:02:09,866 --> 00:02:12,933
去构建属于自己专属的芯片

147
00:02:13,766 --> 00:02:16,566
TPU 就顺势的推出出来了

148
00:02:17,100 --> 00:02:17,900
我们可以看到呢

149
00:02:17,900 --> 00:02:19,300
从谷歌的搜索当中呢

150
00:02:19,300 --> 00:02:20,700
AI 的算力的增长

151
00:02:20,800 --> 00:02:22,466
或者对 AI 的需求的增长呢

152
00:02:22,466 --> 00:02:24,333
是不断的去加大的

153
00:02:24,700 --> 00:02:26,566
2012 年开始到 2016 年

154
00:02:26,566 --> 00:02:28,533
是达到了前所未有的

155
00:02:30,400 --> 00:02:31,266
而 TPU 呢

156
00:02:31,266 --> 00:02:32,966
是诞生在 2015 年

157
00:02:32,966 --> 00:02:34,366
真正立项的时候呢

158
00:02:39,166 --> 00:02:41,333
学 AI 的时间还要早

159
00:02:50,800 --> 00:02:52,766
和它的产品形态啊

160
00:02:52,766 --> 00:02:53,366
这里面呢

161
00:02:53,366 --> 00:02:54,966
ZOMI 就列了一个表

162
00:02:55,000 --> 00:02:59,966
从 TPU 的 V1 V2 V3 到 TPU 的 V4

163
00:03:00,466 --> 00:03:02,566
我们中间还有夹杂着其他的产品

164
00:03:02,566 --> 00:03:03,666
例如 edge V1

165
00:03:03,766 --> 00:03:05,333
还有 Pixel neural core

166
00:03:05,366 --> 00:03:06,466
还有 TPU V4

167
00:03:06,466 --> 00:03:07,933
之前的 V4i

168
00:03:12,500 --> 00:03:14,366
但是呢我们后面的所有的系列呢

169
00:03:14,366 --> 00:03:17,233
都会主要围绕着谷歌的 v1v2v3

170
00:03:17,266 --> 00:03:20,133
跟 V4 这一套系列去介绍的

171
00:03:24,966 --> 00:03:26,533
都有详细的列出

172
00:03:28,566 --> 00:03:30,033
是没有详细的列出的

173
00:03:31,766 --> 00:03:35,266
详细的去给大家展开 TPU V2 V3 V4

174
00:03:35,600 --> 00:03:38,300
现在呢整体看看 TPU 历代的产品啊

175
00:03:38,300 --> 00:03:41,100
刚才讲到的只是芯片产品呢

176
00:03:43,766 --> 00:03:45,766
之后呢后面就有了 pod

177
00:03:45,866 --> 00:03:49,333
这这个 pod 呢就非常的有意思呢

178
00:03:49,366 --> 00:03:50,066
因为 pod 呢

179
00:03:50,066 --> 00:03:53,033
是一个超级计算机服务集群

180
00:03:53,466 --> 00:03:55,066
超级计算机服务节点

181
00:03:55,066 --> 00:03:57,933
提供非常庞大的 TPU 的集群

182
00:03:57,966 --> 00:04:00,933
把 TPU V2 V3 V4 全都封装起来

183
00:04:00,966 --> 00:04:02,633
变成一个大规模的集群

184
00:04:02,666 --> 00:04:04,833
这个呢我们也会在后面 TPU V3 的时候

185
00:04:04,866 --> 00:04:06,833
给大家详细的去汇报的

186
00:04:06,866 --> 00:04:08,133
那现在我们整体看看

187
00:04:09,900 --> 00:04:11,500
来一个总体的概念

188
00:04:11,800 --> 00:04:14,366
左上角的这一款呢就是 TPUV1

189
00:04:14,366 --> 00:04:16,133
这款呢就是 TPUV2

190
00:04:16,800 --> 00:04:19,566
和 TPUV3 呢跟 V1 没有太多的区别啊

191
00:04:19,566 --> 00:04:21,266
整体呢底板变成蓝色

192
00:04:21,266 --> 00:04:22,666
散热器呢也不一样

193
00:04:26,000 --> 00:04:29,100
但是 TPUV4 呢长得就非常的独立

194
00:04:29,166 --> 00:04:29,966
不一样

195
00:04:30,166 --> 00:04:32,733
后面就会介绍它到底有什么不一样啊

196
00:04:35,800 --> 00:04:38,300
跟前面几个产品形态都不太一样了

197
00:04:39,200 --> 00:04:41,500
接着呢有了各种各样的芯片呢

198
00:04:42,300 --> 00:04:43,600
之后呢谷歌的 TPU 呢

199
00:04:46,400 --> 00:04:48,000
从 TPU 的 V1

200
00:04:48,066 --> 00:04:50,366
第二的集群到后面的 extra pod

201
00:04:50,466 --> 00:04:51,866
这边有非常多呃

202
00:04:51,900 --> 00:04:52,866
绿绿绿的

203
00:04:52,866 --> 00:04:55,166
蓝蓝的不同的颜色的光缆

204
00:04:55,766 --> 00:04:58,833
是谷歌跟 IMB 一起去合作的

205
00:05:01,666 --> 00:05:04,466
我们后面也会特殊的去讲解一下

206
00:05:05,166 --> 00:05:07,333
回到谷歌 TPU 另外的一些产品形态呢

207
00:05:08,966 --> 00:05:10,733
不样的 AI 的加速芯片

208
00:05:14,666 --> 00:05:17,133
还有 Pixel 里面的 Tensor

209
00:05:22,266 --> 00:05:23,333
据这一系列呢

210
00:05:23,400 --> 00:05:25,800
谷歌就推出了自己的 Pixel 系列的手机

211
00:05:26,766 --> 00:05:29,733
也把它的 TPU 呢用在这些手机上面

212
00:05:30,366 --> 00:05:34,966
我们端侧实时运行的一些 AI 的场景

213
00:05:42,966 --> 00:05:45,533
首先呢我们看看 TPU V1 的概览

214
00:05:55,500 --> 00:05:56,566
好了我不练了啊

215
00:06:03,000 --> 00:06:05,100
呃我们现在国家还在努力的

216
00:06:06,566 --> 00:06:08,433
希望它能够量产成熟

217
00:06:12,766 --> 00:06:14,233
只有 700 兆赫兹

218
00:06:14,300 --> 00:06:16,266
对比起 2014 年 2015 年

219
00:06:18,766 --> 00:06:20,866
其实 TPU 的主频算低了

220
00:06:23,100 --> 00:06:24,700
能够去到 40 瓦

221
00:06:24,700 --> 00:06:25,766
还是比较低的

222
00:06:25,766 --> 00:06:27,133
而为了尽快把 TPU 呢

223
00:06:31,366 --> 00:06:33,933
做成一个外部的扩展加速器啊

224
00:06:34,000 --> 00:06:35,366
通过 Pcie 的插槽

225
00:06:36,700 --> 00:06:38,100
一排小针口

226
00:06:42,366 --> 00:06:42,933
的方式呢

227
00:06:45,666 --> 00:06:49,733
提供 12.5GB 每秒的有效的算力带宽

228
00:06:50,200 --> 00:06:52,400
现在我们回到 2015 年呢

229
00:06:52,400 --> 00:06:55,200
当时候最火的神经网络模型结构

230
00:06:55,266 --> 00:06:56,066
主要有三种

231
00:06:56,100 --> 00:06:57,500
一种是 MLP

232
00:06:57,600 --> 00:06:59,000
就是多层神经感知机

233
00:06:59,366 --> 00:07:01,466
然后就是多层 FFN 堆叠起来

234
00:07:01,466 --> 00:07:04,266
第二种呢就是 CNN 卷积神经网络

235
00:07:04,300 --> 00:07:06,700
第三种呢就是 RNN 跟 LSTM

236
00:07:08,666 --> 00:07:10,366
还有一些音频的

237
00:07:10,400 --> 00:07:12,166
虽然呢 RNN 跟 LSTM 呢

238
00:07:12,166 --> 00:07:14,033
随着 transformer 的推出

239
00:07:14,066 --> 00:07:17,433
他们现在已经很少被人们所利用了

240
00:07:19,266 --> 00:07:21,433
其实曾经占据过人工神经网络

241
00:07:21,500 --> 00:07:23,600
非常长的一段时间

242
00:07:23,766 --> 00:07:25,166
而当时候呢

243
00:07:25,366 --> 00:07:30,333
TPUV1 大部分只能处理 MLP 跟 CNN 两种情况

244
00:07:30,400 --> 00:07:32,266
至于 RNN 跟 LSTM 呢

245
00:07:32,266 --> 00:07:33,566
它是很难去处理的

246
00:07:40,266 --> 00:07:41,633
是因为在 TPU V2 V3

247
00:07:44,066 --> 00:07:46,566
它引入了低精度的数据的格式

248
00:07:46,566 --> 00:07:49,333
从 INT8 到 BF16 的首创

249
00:07:49,400 --> 00:07:49,766
接着呢

250
00:07:49,766 --> 00:07:52,866
又引入了矩阵的专用的加速处理器

251
00:07:52,900 --> 00:07:54,966
就我们刚才讲到的 MXU

252
00:07:55,066 --> 00:07:56,533
用了脉动阵列

253
00:07:56,800 --> 00:07:59,166
最后呢还提出了专用的硬件

254
00:08:05,166 --> 00:08:06,566
现在我们打开第一个特性

255
00:08:12,966 --> 00:08:13,666
使用 INT8 呢

256
00:08:13,666 --> 00:08:14,533
来去近似

257
00:08:16,300 --> 00:08:17,266
任意的数值

258
00:08:23,066 --> 00:08:24,766
可能由左边的这条线

259
00:08:24,800 --> 00:08:25,600
每一个点呢

260
00:08:25,600 --> 00:08:27,200
都非常的精确

261
00:08:27,500 --> 00:08:28,700
但是呢经过量化之后呢

262
00:08:28,700 --> 00:08:31,600
我的曲线的整体的幅度和频率

263
00:08:31,666 --> 00:08:32,866
是相同的

264
00:08:32,866 --> 00:08:33,733
区别就在于

265
00:08:33,800 --> 00:08:36,900
我们可能每一个点的精度没有那么高

266
00:08:37,300 --> 00:08:39,900
对于我们的神经网络模型来说呢

267
00:08:39,966 --> 00:08:41,833
它有足够的泛化性

268
00:08:41,900 --> 00:08:43,500
即使我们量化到 INT8 呢

269
00:08:43,566 --> 00:08:45,233
对我们整体的推理的性能

270
00:08:50,266 --> 00:08:52,266
有了刚才讲到的低精度呢

271
00:08:52,266 --> 00:08:53,733
其实第二个重要的特性啊

272
00:08:53,766 --> 00:08:56,233
第二个是我们的脉动阵列啊

273
00:08:56,566 --> 00:08:59,033
首先我们去回顾一下 CPU 跟 GPU 呢

274
00:09:03,666 --> 00:09:05,366
不断地去获取数据

275
00:09:05,366 --> 00:09:06,933
然后给到我们的 ALU

276
00:09:08,066 --> 00:09:08,766
就是 CUDA Core

277
00:09:08,800 --> 00:09:09,600
在 CPU 里面呢

278
00:09:09,600 --> 00:09:12,500
就是 CPU 的 ALU 进行取指译码

279
00:09:12,500 --> 00:09:13,900
执行过程当中呢

280
00:09:13,900 --> 00:09:15,500
就不断的去反复

281
00:09:15,766 --> 00:09:17,866
而整个 TPU 的脉动阵列啊

282
00:09:22,366 --> 00:09:25,133
就多个 ALU 把它串行起来

283
00:09:25,400 --> 00:09:27,266
通过一个计算器一次读取的数据

284
00:09:27,266 --> 00:09:28,533
给下一个 ALU

285
00:09:30,666 --> 00:09:32,533
计算完的结果再给 ALU

286
00:09:32,600 --> 00:09:33,566
这里面可以看到呢

287
00:09:39,566 --> 00:09:40,266
给下一个

288
00:09:40,866 --> 00:09:44,133
符合我们人工神经网络的矩阵的运算

289
00:09:44,500 --> 00:09:47,766
就出现了脉动阵列这么一个概念啊

290
00:09:48,500 --> 00:09:49,200
我们可以看到了

291
00:09:51,100 --> 00:09:52,566
就是模型权重

292
00:09:53,166 --> 00:09:54,066
模型权重

293
00:09:54,166 --> 00:09:56,366
是在推理的过场景过程当中呢

294
00:09:57,600 --> 00:09:58,966
在训练的过程当中呢

295
00:09:58,966 --> 00:10:01,766
我们的数据呢是不断的去演变的

296
00:10:01,766 --> 00:10:04,466
那现在呢是一个 3 乘以 3 乘以 2

297
00:10:04,566 --> 00:10:07,666
3 乘以 2 的这么一个矩阵的运算

298
00:10:07,700 --> 00:10:08,700
现在我们简单来看看

299
00:10:12,166 --> 00:10:14,633
接着呢我们把第二个数据呢累加起来

300
00:10:17,400 --> 00:10:19,066
接着我们在第三层的时候呢

301
00:10:21,766 --> 00:10:22,466
最后一次呢

302
00:10:22,466 --> 00:10:23,566
我们把所有的数据呢

303
00:10:28,300 --> 00:10:31,200
然后呢完成我们整体的运算

304
00:10:31,266 --> 00:10:32,533
这整一个过程呢

305
00:10:34,966 --> 00:10:35,933
Systolic Array

306
00:10:36,000 --> 00:10:38,100
Systolic 就是我们的脉动啊

307
00:10:38,100 --> 00:10:40,466
有点类似于那个脉搏的跳动

308
00:10:40,466 --> 00:10:41,366
所以叫做脉动

309
00:10:41,366 --> 00:10:42,833
不是一个饮料哦

310
00:10:42,966 --> 00:10:45,366
后面我们将会详细的

311
00:10:45,366 --> 00:10:47,433
特别是在 TPUV1 这个系列里面

312
00:10:47,500 --> 00:10:49,566
详细的展开我们的硬件应该怎么做

313
00:10:52,366 --> 00:10:54,966
这里面呢只是简单的一个数学的概念

314
00:10:55,000 --> 00:10:56,766
如果没有搞清楚的同学呢

315
00:10:56,766 --> 00:10:58,233
也可以重复的翻看

316
00:10:58,266 --> 00:11:00,433
刚才的那个动画的过程

317
00:11:01,266 --> 00:11:04,133
谷歌呢基于刚才上面的数学原理呢

318
00:11:04,166 --> 00:11:05,966
造造了一个硬件呢

319
00:11:09,966 --> 00:11:13,133
一个大的 systolic array

320
00:11:14,500 --> 00:11:16,500
看右边的这个图呢

321
00:11:19,800 --> 00:11:20,700
有两个箭头

322
00:11:20,800 --> 00:11:21,500
一个箭头呢

323
00:11:21,500 --> 00:11:23,700
是指向下面的这个一个队列

324
00:11:23,800 --> 00:11:24,666
下面这个队列呢

325
00:11:28,166 --> 00:11:29,566
或者一个 feature map

326
00:11:29,600 --> 00:11:32,166
我们中间产生的一些结果

327
00:11:32,466 --> 00:11:33,666
上面的这个箭头呢

328
00:11:33,666 --> 00:11:36,233
就是我们权重数据的输入

329
00:11:36,266 --> 00:11:37,333
把权重数据呢

330
00:11:40,466 --> 00:11:41,866
整个脉动阵列里面

331
00:11:41,966 --> 00:11:44,533
接着呢不断的去把我们的数据输进来

332
00:11:44,566 --> 00:11:46,733
然后进行一个累加的计算

333
00:11:56,866 --> 00:11:59,266
去单独的展开它

334
00:11:59,266 --> 00:12:02,333
宣称在谷歌的大规模的应用当中呢

335
00:12:02,366 --> 00:12:04,533
TPU 解决了它非常非常多的

336
00:12:07,000 --> 00:12:10,366
接着呢我们来到了谷歌 TPU V2 V3 V4

337
00:12:10,400 --> 00:12:12,066
简单的一个概述了

338
00:12:19,666 --> 00:12:21,266
而且很独特的一点就是

339
00:12:21,300 --> 00:12:24,000
谷歌 TPUV1 呢是专注于推理的

340
00:12:24,000 --> 00:12:26,300
而谷歌 TPU V2 呢是专注于训练

341
00:12:27,966 --> 00:12:31,233
我们现在训练大模型经常说 FP16 跑飞

342
00:12:35,466 --> 00:12:36,933
它的整体的拓扑形态呢

343
00:12:36,966 --> 00:12:38,833
由右边的这个图所示

344
00:12:42,300 --> 00:12:44,466
只是基于 v1 的版本呢进行改进

345
00:12:44,466 --> 00:12:45,533
然后就得到了 v2

346
00:12:45,700 --> 00:12:47,366
而这些改进的修改点呢

347
00:12:47,366 --> 00:12:49,766
就是训练和推理的差别了

348
00:12:49,766 --> 00:12:51,766
我们将会在第二个视频

349
00:12:51,800 --> 00:12:53,366
给大家去汇报的

350
00:12:53,366 --> 00:12:54,066
而这里面呢

351
00:12:55,500 --> 00:12:58,766
我们后面会详细的给大家去展开

352
00:12:59,800 --> 00:13:03,866
我们要看一下 FP32 的指数位 exponent

353
00:13:03,966 --> 00:13:05,433
它有八个比特

354
00:13:09,900 --> 00:13:11,466
而在 BF16 呢

355
00:13:11,566 --> 00:13:12,433
它的指数位呢

356
00:13:16,100 --> 00:13:18,566
然后减少了后面的小数位

357
00:13:21,100 --> 00:13:22,566
通过这么一种改变

358
00:13:22,666 --> 00:13:25,633
使得我们的神经网络里面的 BF16 呢

359
00:13:25,666 --> 00:13:29,266
能够表示更宽的数值范围

360
00:13:29,666 --> 00:13:30,233
那这里面呢

361
00:13:32,066 --> 00:13:33,533
BF 16 的几个好处啊

362
00:13:33,566 --> 00:13:35,033
第一个好处呢就是硬件上面呢

363
00:13:35,066 --> 00:13:37,133
节省我们的计算的内存

364
00:13:37,366 --> 00:13:37,633
第二个呢

365
00:13:37,666 --> 00:13:39,533
节省内存意味着我们加载的时间

366
00:13:40,866 --> 00:13:42,733
我们的搬运数据的时间呢

367
00:13:42,766 --> 00:13:44,866
就会有所的降低

368
00:13:47,966 --> 00:13:51,433
使得我们整体的吞吐和计算的速率

369
00:13:51,600 --> 00:13:52,466
有所提升

370
00:13:55,800 --> 00:13:58,666
谷歌就基于我们的 V2 呢建了一个 pod

371
00:13:58,666 --> 00:14:00,266
一个计算的基群

372
00:14:00,300 --> 00:14:02,766
左边 a 呢跟 d 呢是 CPU

373
00:14:02,900 --> 00:14:06,600
中间的 b 和 c 呢是我们的 TPU 陈列

374
00:14:12,966 --> 00:14:15,066
精度呢能到达 93%

375
00:14:16,866 --> 00:14:18,966
接着我们看看后面几个内容

376
00:14:19,000 --> 00:14:21,966
快速的过一过就是谷歌 TPU 的 V3

377
00:14:22,100 --> 00:14:23,966
那 V3 呢其实没有太多的改变了

378
00:14:24,000 --> 00:14:25,866
除了工艺的一些增加

379
00:14:25,900 --> 00:14:27,566
然后核数呢翻了一翻

380
00:14:27,766 --> 00:14:29,933
然后其他改进点并不大

381
00:14:29,966 --> 00:14:30,633
我们后面呢

382
00:14:32,266 --> 00:14:34,866
去介绍它的 pod 的形态

383
00:14:37,200 --> 00:14:39,000
就是谷歌 TPU V4

384
00:14:39,100 --> 00:14:41,166
V4 呢我们后面会详细的去展开的

385
00:14:41,200 --> 00:14:43,200
V4 更多的内容呢来自于这一

386
00:14:43,200 --> 00:14:44,766
篇论文实际上呢

387
00:14:44,766 --> 00:14:45,466
我们现在看到

388
00:14:45,500 --> 00:14:48,066
官网公布的信息并不多

389
00:14:48,066 --> 00:14:49,433
就这么一篇论文

390
00:14:49,466 --> 00:14:50,866
那谷歌 TPU V4 呢

391
00:14:50,966 --> 00:14:53,333
实际上是一款划时代的产品啊

392
00:14:58,100 --> 00:15:00,600
到 V4 呢真正用了 7 纳米的

393
00:15:02,900 --> 00:15:04,266
缓存呢也增加了很多

394
00:15:12,566 --> 00:15:14,666
这个就是我们后面会详细展开

395
00:15:17,100 --> 00:15:19,200
那后面可能还有一些 TPU 的规格呀

396
00:15:20,500 --> 00:15:22,200
还有 TPU V4 的 extra pod

397
00:15:23,800 --> 00:15:25,400
我们刚才跨了一段时间

398
00:15:29,866 --> 00:15:31,733
第一款训练卡到 TPU V3

399
00:15:31,766 --> 00:15:35,666
它整体的性能的 pod 超算提出到 TPUV4

400
00:15:35,700 --> 00:15:37,100
超级的互联

401
00:15:37,766 --> 00:15:39,866
那我们现在提出一个问题

402
00:15:49,566 --> 00:15:51,866
跟大家一起去汇报

403
00:15:51,900 --> 00:15:53,566
下面谷歌 TPU 整个系列

404
00:15:53,600 --> 00:15:54,300
首先第一个呢

405
00:15:54,366 --> 00:15:56,766
就是软件栈的区别

406
00:15:56,866 --> 00:15:57,733
包括 AI 框架

407
00:15:57,766 --> 00:15:58,566
AI 编译器

408
00:15:59,266 --> 00:16:01,733
就是互联方式的区别

409
00:16:02,966 --> 00:16:05,066
和芯片卡间的互联

410
00:16:07,500 --> 00:16:09,566
我们架构的演进同训练推理

411
00:16:09,600 --> 00:16:12,166
我们的推理的性能进一步的发挥到

412
00:16:12,166 --> 00:16:13,966
我们现在遇到的大模型

413
00:16:14,166 --> 00:16:15,766
到底有哪些区别

414
00:16:15,800 --> 00:16:17,866
还有哪些架构的演进

415
00:16:17,900 --> 00:16:19,900
和整体芯片的演进

416
00:16:19,900 --> 00:16:20,766
希望给大家

417
00:16:24,700 --> 00:16:26,300
现在 ZOMI 会把所有的视频呢

418
00:16:28,166 --> 00:16:30,566
在这里面每个视频都有详细的介绍哦

419
00:00:05,500 --> 00:00:06,366
大家好呀

420
00:00:10,700 --> 00:00:13,966
猛然回首发现自己已经发福的 ZOMI

421
00:00:40,900 --> 00:00:42,766
一个是跟国外相关的

422
00:00:53,366 --> 00:00:54,633
关于国外的 AI 芯片呢

423
00:01:08,900 --> 00:01:11,600
就是 TPU 的出现和它的历史的发展

424
00:01:50,000 --> 00:01:53,500
谷歌 AI 的技术负责人 Jeff Dean 就经过分析

425
00:02:12,900 --> 00:02:13,766
那这个时候呢

426
00:02:28,500 --> 00:02:30,300
一个快速的增长过程当中

427
00:03:26,500 --> 00:03:28,566
而 edge 呢还有 Pixel 系列呢

428
00:04:16,100 --> 00:04:16,766
有四个

429
00:04:58,800 --> 00:05:01,266
一种特殊的光缆接口

430
00:05:07,300 --> 00:05:08,900
因为他发明了很多不同

431
00:05:17,100 --> 00:05:19,866
谷歌 Tensor 里面的这块 TPU 啊

432
00:05:29,700 --> 00:05:30,300
去解决

433
00:06:08,400 --> 00:06:10,266
然后攻克 16-14 纳米

434
00:06:27,100 --> 00:06:27,700
部署到

435
00:08:59,000 --> 00:09:00,600
每次在计算的过程当中啊

436
00:09:06,900 --> 00:09:08,000
或者在 GPU 里面呢

437
00:09:28,500 --> 00:09:30,600
计算完的结果再给下一个 ALU

438
00:09:44,100 --> 00:09:44,566
于是呢

439
00:12:04,500 --> 00:12:06,566
AI 的算力的消耗

440
00:12:12,066 --> 00:12:14,266
那谷歌 TPUV2 呢

441
00:13:12,400 --> 00:13:15,100
实际上是跟 FP32 保持一致

442
00:13:30,200 --> 00:13:31,966
我们简单的总结了一下

443
00:15:07,900 --> 00:15:12,566
使得我们 TPU V4 所有的互联更加特别

444
00:00:01,266 --> 00:00:04,266
字幕生成：mkwei  字幕校准：mkwei

445
00:00:06,400 --> 00:00:08,600
周一周六在公司吃苦

446
00:00:15,466 --> 00:00:15,933
今天呢

447
00:00:26,800 --> 00:00:28,200
在整个系列里面

448
00:00:57,066 --> 00:00:59,633
然后看了一下特斯拉 DOJO 的整个系列

449
00:01:31,766 --> 00:01:33,533
还有它的 pod 的形态

450
00:01:53,466 --> 00:01:54,166
发现哎

451
00:01:57,200 --> 00:01:59,900
现在安卓背后的所有的 AI 功能啊

452
00:02:37,100 --> 00:02:39,100
比 ZOMI 去学深度学习

453
00:02:48,300 --> 00:02:50,700
我们看看历代的 TPU 的整体的参数

454
00:02:59,900 --> 00:03:00,400
但是呢

455
00:03:10,566 --> 00:03:12,233
Tensor Soc 啊

456
00:03:21,166 --> 00:03:22,766
里面的非常详细的

457
00:03:42,100 --> 00:03:43,700
还有 edge

458
00:04:08,066 --> 00:04:09,833
它的一个芯片的产品形态

459
00:04:41,466 --> 00:04:42,266
从 V1 到 V4

460
00:04:43,566 --> 00:04:46,333
就推出了历代的芯片的服务器

461
00:05:01,200 --> 00:05:01,666
这里面呢

462
00:05:12,400 --> 00:05:14,566
最后就变成了谷歌 Pixel

463
00:05:25,766 --> 00:05:26,566
的产品

464
00:05:38,200 --> 00:05:40,300
就是 TPU 的演进啊

465
00:06:00,266 --> 00:06:02,966
采用的是 28 纳米的制成工艺啊

466
00:06:29,766 --> 00:06:31,366
所以谷歌把 TPUV1 呢

467
00:06:35,300 --> 00:06:36,700
我们可以看到这面有

468
00:06:39,300 --> 00:06:42,266
Gen3 第三代里面的一个插槽和总线

469
00:07:38,766 --> 00:07:39,966
为什么会这么牛逼

470
00:08:01,966 --> 00:08:04,733
专注于 AI 的计算性能

471
00:08:09,400 --> 00:08:12,366
其实首创的提出了 INT8

472
00:08:19,266 --> 00:08:20,866
那我们看看中间的这条线呢

473
00:08:36,866 --> 00:08:37,333
但是呢

474
00:08:45,166 --> 00:08:48,866
特别是分类的场景影响并不大

475
00:09:00,566 --> 00:09:01,666
就是这个图呢

476
00:09:19,800 --> 00:09:22,366
就会将多个的运算的逻辑单元

477
00:09:35,600 --> 00:09:37,466
我少了很多译码的问题

478
00:10:09,266 --> 00:10:12,066
就是我们的 Y11 的第一个位置的计算

479
00:10:26,700 --> 00:10:28,266
变成我们的 Y33

480
00:11:17,700 --> 00:11:19,466
就是我们的逻辑控制器呢

481
00:11:52,166 --> 00:11:53,266
整体的过程当中呢

482
00:11:54,066 --> 00:11:56,866
我们后面在会 TPU V1 的详细加工里面

483
00:12:26,266 --> 00:12:27,933
并且引入了 BF16

484
00:12:33,900 --> 00:12:35,466
我们后面会详细的展开

485
00:12:58,700 --> 00:12:59,800
首先呢 B f 16 之

486
00:13:15,066 --> 00:13:16,133
有 8 个比特

487
00:14:09,100 --> 00:14:12,800
用了 30 分钟就完成了整个模型的训练

488
00:14:30,566 --> 00:14:32,233
更多的是在 TPUV3 的时候

489
00:14:53,266 --> 00:14:54,133
对于谷歌来说

490
00:14:56,166 --> 00:14:58,133
都是从 28 纳米到 16 纳米

491
00:15:27,666 --> 00:15:29,833
还有 TPU V1 的脉动阵列和 TPU V2

492
00:15:48,166 --> 00:15:49,566
ZOMI 希望带着这个问题呢

493
00:16:05,766 --> 00:16:07,533
整个芯片的架构

