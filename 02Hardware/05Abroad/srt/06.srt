1
00:00:08,000 --> 00:00:09,500
不顾一切向前冲

2
00:00:15,333 --> 00:00:17,100
但实际上呢比谁都卷

3
00:00:20,166 --> 00:00:23,166
我们来到了谷歌 TPU 第二代的芯片呢

4
00:00:37,133 --> 00:00:38,900
在整个 AI 芯片系列里面呢

5
00:00:46,466 --> 00:00:49,266
谷歌 TPU 芯片的架构剖析

6
00:00:49,666 --> 00:00:51,666
我们现在呢已经到了第三个小节

7
00:00:59,866 --> 00:01:00,833
我想问一下呢

8
00:01:00,866 --> 00:01:03,433
为什么你要分享这么多节

9
00:01:03,466 --> 00:01:05,733
谷歌的 TPU 的课程呢

10
00:01:07,333 --> 00:01:09,866
不管谷歌的训练卡和推理卡也好

11
00:01:14,200 --> 00:01:15,066
一个意义

12
00:01:15,700 --> 00:01:16,133
首先呢

13
00:01:21,400 --> 00:01:22,966
来做 AI 的推理

14
00:01:23,166 --> 00:01:23,566
这个呢

15
00:01:23,633 --> 00:01:26,300
是在 13 年的时候已经做好规划了

16
00:01:27,700 --> 00:01:29,733
而在谷歌的 TPU2 里面呢

17
00:01:29,766 --> 00:01:31,333
用了 BF16

18
00:01:31,633 --> 00:01:33,666
现在我们经常用来训练大模型

19
00:01:33,766 --> 00:01:35,266
解决数据不收敛

20
00:01:37,666 --> 00:01:41,466
是谷歌在 2016 年 17 年就引入的一个架构

21
00:01:41,600 --> 00:01:43,466
但是对于国内的其他厂商呢

22
00:01:45,066 --> 00:01:47,900
到 2023 年才开始慢慢引入的

23
00:01:54,633 --> 00:01:56,833
虽然不一定谷歌的 TPU 它自己很成功

24
00:02:03,733 --> 00:02:06,533
谷歌 TPU2 的整个产品形态

25
00:02:06,566 --> 00:02:07,266
那这一块呢

26
00:02:25,200 --> 00:02:27,500
现在我们看一下具体的谷歌 TPU

27
00:02:35,700 --> 00:02:37,266
而国内的 AI 芯片厂商呢

28
00:02:37,300 --> 00:02:38,500
大部分都聚焦于推理

29
00:02:40,866 --> 00:02:42,433
还是相对来说容易很多

30
00:02:53,233 --> 00:02:54,900
那第一点就是训练的并行化

31
00:02:55,000 --> 00:02:56,566
非常的难

32
00:02:56,600 --> 00:02:58,600
因为每个推理的任务都是独立的

33
00:02:58,666 --> 00:03:00,666
我们在整个 AI 芯片的集群呢

34
00:03:07,800 --> 00:03:08,400
那这个时候呢

35
00:03:08,500 --> 00:03:11,766
我们要调动跨集群的并行的资源

36
00:03:11,800 --> 00:03:14,066
所以说训练的并行难度呢是很复杂的

37
00:03:16,700 --> 00:03:19,233
因为呢我们不仅仅去算正向传播

38
00:03:19,266 --> 00:03:21,533
或者一个正向的推理 AI 框架呢

39
00:03:25,766 --> 00:03:26,900
包括我们的数据格式呢

40
00:03:29,300 --> 00:03:29,733
第三个呢

41
00:03:29,833 --> 00:03:32,366
就是需要更大的内存

42
00:03:36,400 --> 00:03:37,533
去复用我们的内存

43
00:03:38,266 --> 00:03:39,933
但是在训练的过程当中呢

44
00:03:41,733 --> 00:03:42,933
引起的临时变量

45
00:03:44,066 --> 00:03:46,066
一些临时的变量和状态

46
00:03:46,200 --> 00:03:47,433
在整体的过程当中呢

47
00:03:49,333 --> 00:03:50,833
在整体对内存的需求呢

48
00:03:55,733 --> 00:03:58,533
就是更加具有可编程性

49
00:04:01,833 --> 00:04:04,666
会比推理呢更加复杂更加多变

50
00:04:04,800 --> 00:04:06,000
所以这一块呢

51
00:04:16,866 --> 00:04:18,066
但是在训练过程当中呢

52
00:04:20,233 --> 00:04:21,733
我们要捕捉非常微小的

53
00:04:21,733 --> 00:04:22,866
一些数据的变化

54
00:04:27,700 --> 00:04:31,833
所以说训练会比推理难很多很多

55
00:04:34,200 --> 00:04:36,400
而谷歌 TPU 的第二代呢

56
00:04:48,933 --> 00:04:50,966
就是我们训练跟推理的差别

57
00:04:51,866 --> 00:04:55,233
就会重点呢去看看训练跟推理的差别

58
00:04:55,266 --> 00:04:57,633
做了哪些修改点

59
00:05:00,233 --> 00:05:02,800
我们现在正式交给命运的齿轮看看谷

60
00:05:05,400 --> 00:05:07,133
有什么具体的差异

61
00:05:07,366 --> 00:05:08,566
首先呢最大的差异

62
00:05:09,233 --> 00:05:10,800
是引用了 BF16

63
00:05:10,900 --> 00:05:12,666
因为现在训练大模型

64
00:05:19,600 --> 00:05:20,633
我们真正的来到了

65
00:05:23,700 --> 00:05:25,100
第一个呢就是谷歌第一代呢

66
00:05:26,800 --> 00:05:28,300
一个叫做 accumulator

67
00:05:46,100 --> 00:05:47,300
在训练过程当中呢

68
00:05:49,766 --> 00:05:52,333
于是呢就把刚才的 activation Pipeline 呢

69
00:05:52,400 --> 00:05:55,400
跟 accumulator 两个单元先换换位置

70
00:06:12,233 --> 00:06:13,366
有了这个内容之后呢

71
00:06:16,433 --> 00:06:18,100
进行一个改变

72
00:06:19,066 --> 00:06:21,266
一开始在谷歌 TPU 第一代呢

73
00:06:26,833 --> 00:06:27,966
然后再去激活

74
00:06:28,000 --> 00:06:30,200
这个激活特殊的 ALU 的计算

75
00:06:32,933 --> 00:06:34,133
我们的训练的场景了

76
00:06:34,133 --> 00:06:36,133
于是呢把它变成一个 Vector 的

77
00:06:36,166 --> 00:06:38,466
unit 可以专门针对类似于激活

78
00:06:38,566 --> 00:06:41,100
还有 softmax 这一类 Vector 的计算呢

79
00:06:43,966 --> 00:06:45,733
接到了我们来到了第三个模块

80
00:06:45,766 --> 00:06:47,433
也就是 MXU

81
00:06:57,400 --> 00:07:00,866
把 MXU 跟 Vector Unit 呢进行一个连接

82
00:07:00,933 --> 00:07:02,400
所有的数据的出口

83
00:07:02,400 --> 00:07:03,433
所有的计算呢

84
00:07:09,500 --> 00:07:11,600
特别是针对矩阵的计算呢

85
00:07:23,100 --> 00:07:24,300
然后呢通过 Vector Unit

86
00:07:28,800 --> 00:07:29,933
到了第四个改变点呢

87
00:07:34,000 --> 00:07:36,633
适用把所有的推理的场景当中呢

88
00:07:38,833 --> 00:07:40,500
直接加载进来去计算

89
00:07:47,366 --> 00:07:50,166
DDR3 的回写速度是非常慢的

90
00:08:09,133 --> 00:08:09,900
我们现在呢

91
00:08:15,533 --> 00:08:16,666
有 Scalar Unit 呢

92
00:08:17,733 --> 00:08:18,866
有 MXU Unit 呢

93
00:08:20,933 --> 00:08:22,933
一共有四个 Unit 的单元

94
00:08:23,966 --> 00:08:25,466
看到有这么多个单元呢

95
00:08:31,600 --> 00:08:33,633
有没有一些似曾相识的感觉

96
00:08:33,733 --> 00:08:35,500
现在是不是很多训练卡

97
00:08:37,733 --> 00:08:40,100
这也是 ZOMI 觉得谷歌的 TPU 的

98
00:08:40,133 --> 00:08:41,533
很多的计算的方式

99
00:08:52,166 --> 00:08:52,566
那现在呢

100
00:08:55,700 --> 00:08:56,200
首先呢

101
00:08:56,233 --> 00:08:59,033
指令不再是通过我们的 CPU 去获取的

102
00:08:59,700 --> 00:09:01,533
而是通过核发射单元

103
00:09:01,566 --> 00:09:03,333
取出超长的指令级

104
00:09:08,166 --> 00:09:09,733
然后再给其他协处理器呢

105
00:09:09,866 --> 00:09:11,700
进行控制执行的

106
00:09:12,533 --> 00:09:14,100
这里面所有最重要的应用呢

107
00:09:16,500 --> 00:09:17,900
具体 Vector Unit 展开呢

108
00:09:29,333 --> 00:09:30,633
而 Vector Unit 自己呢

109
00:09:30,633 --> 00:09:33,466
也可以做一些简单的 ALU 的运算

110
00:09:34,466 --> 00:09:36,733
Vector Unit 呢自己是一个 SIM 的结构

111
00:09:39,800 --> 00:09:40,566
而 ALU 里面呢

112
00:09:40,700 --> 00:09:42,900
又分为非常多个 ALU 的核

113
00:09:44,666 --> 00:09:47,200
大家也可以慢慢的去细品细品

114
00:09:47,266 --> 00:09:49,466
接着我们来到了第二个内容

115
00:09:49,566 --> 00:09:52,566
第二个计算单元就是 scalar unit

116
00:09:52,700 --> 00:09:54,200
scalar unit 呢用的比较

117
00:09:54,200 --> 00:09:54,433
少

118
00:09:58,033 --> 00:09:58,800
那标量的运算呢

119
00:10:12,400 --> 00:10:15,133
每个脉动阵列呢就变成 128*128

120
00:10:15,333 --> 00:10:19,733
对比起 v1 版本的 256*256 呢少了一半哦

121
00:10:20,266 --> 00:10:21,400
数据的计算格式呢

122
00:10:28,800 --> 00:10:32,333
就是对我们的矩阵的特殊的计算

123
00:10:32,433 --> 00:10:33,400
进行提供的

124
00:10:37,466 --> 00:10:40,533
这些呢都是在我们训练的过程当中

125
00:10:58,100 --> 00:11:00,300
而是用 128 乘以 128

126
00:11:00,433 --> 00:11:01,833
小了一半呢

127
00:11:03,433 --> 00:11:04,200
呀嘿

128
00:11:14,266 --> 00:11:16,033
它整体的计算的峰值

129
00:11:16,066 --> 00:11:18,100
可以做到非常非常的高

130
00:11:18,200 --> 00:11:21,366
但是呢利用率可能会变得非常的低

131
00:11:21,400 --> 00:11:22,633
因为不是所有的卷积

132
00:11:32,666 --> 00:11:33,066
于是呢

133
00:11:36,933 --> 00:11:39,200
还有计算的峰值和算力的利用率

134
00:11:39,333 --> 00:11:42,000
专门的采用了这个核心的位宽

135
00:11:44,133 --> 00:11:44,900
啊

136
00:11:54,733 --> 00:11:56,233
还是像 ZOMI 老师这样呢

137
00:12:00,533 --> 00:12:01,766
如果大家有更好的意见呢

138
00:12:01,866 --> 00:12:03,100
欢迎弹幕

139
00:12:03,600 --> 00:12:04,033
那这里面呢

140
00:12:09,500 --> 00:12:11,866
增加了一个 interconnect

141
00:12:13,333 --> 00:12:15,900
用来方便 TPU 去组成一个 pod 的

142
00:12:18,566 --> 00:12:19,066
聚拢模块呢

143
00:12:23,966 --> 00:12:25,166
在训练场景当中呢

144
00:12:25,200 --> 00:12:26,500
因为需要有大量的数据

145
00:12:35,200 --> 00:12:37,066
我们需要用到集群的能力

146
00:12:42,533 --> 00:12:45,000
也不是机跟机之间的一个互联带宽

147
00:12:49,233 --> 00:12:50,200
最核心的模块呢

148
00:12:54,400 --> 00:12:57,133
这个模块能够实现的 2D 的环面连接

149
00:12:58,566 --> 00:13:00,500
我们将会在下节内容里面呢

150
00:13:00,500 --> 00:13:02,000
在谷歌 TPU V3 里面

151
00:13:02,033 --> 00:13:04,700
详细的去展开这个 torus 的架构

152
00:13:04,766 --> 00:13:07,566
还有它的互联的整个集群的内容

153
00:13:09,233 --> 00:13:11,100
刚才我们看到的所有的内容呢

154
00:13:28,266 --> 00:13:29,833
最后呢变成简单的架构图呢

155
00:13:29,866 --> 00:13:31,433
就像这样所示

156
00:13:31,466 --> 00:13:32,500
那这个图里面呢

157
00:13:35,733 --> 00:13:36,500
所以说

158
00:13:36,500 --> 00:13:39,500
TPU core 跟 HBM 之间的代宽是最高的

159
00:13:42,633 --> 00:13:44,666
而 TPU 整一个新历

160
00:13:44,766 --> 00:13:46,866
跟另外新历之间的带宽呢

161
00:13:46,900 --> 00:13:49,366
会低层这也是英伟达的 GPU 呢

162
00:13:53,233 --> 00:13:55,433
从而提升 GPU 的计算的吞吐

163
00:13:57,600 --> 00:13:58,466
很不容易很不容易

164
00:13:58,533 --> 00:14:01,700
我们终于来到了 TPU V2

165
00:14:03,333 --> 00:14:05,900
看到这里面的人呢可能就并不多了

166
00:14:12,700 --> 00:14:14,000
把它们连接起来

167
00:14:16,666 --> 00:14:19,400
两个 MXU 还有里面的 memory

168
00:14:24,600 --> 00:14:25,566
两个核心

169
00:14:25,600 --> 00:14:27,166
具体呢将激活函数

170
00:14:28,500 --> 00:14:32,100
改成可编程性更高的向量单元 Vector Unit

171
00:14:34,733 --> 00:14:37,833
使用了 Vector Unit 呢代替了之前的双缓存

172
00:14:37,833 --> 00:14:40,400
而 MXU 呢作为相关单元的协处理器

173
00:14:40,600 --> 00:14:43,066
提升我们的可编程性

174
00:14:44,500 --> 00:14:46,066
做一个整体的调度和控制

175
00:14:46,133 --> 00:14:47,700
最后还增加了 scalar Unit 呢

176
00:14:47,800 --> 00:14:48,933
还有其他 Unit

177
00:14:49,400 --> 00:14:50,900
那在内存和互联方面呢

178
00:14:50,900 --> 00:14:52,200
就使用了 HBM 代替了

179
00:14:55,800 --> 00:14:58,433
增加了一个互联的模块

180
00:14:58,633 --> 00:15:00,133
使得 TPU 跟 TPU 之间呢

181
00:15:00,133 --> 00:15:02,333
提升整个互联的带宽

182
00:15:03,533 --> 00:15:04,466
那整体的区别呢

183
00:15:04,500 --> 00:15:06,066
我们看看架构图案

184
00:15:06,666 --> 00:15:08,066
原来 control 的四级流水呢

185
00:15:08,166 --> 00:15:09,133
现在变成右边的

186
00:15:09,133 --> 00:15:11,233
这个图更多的是通过 Vector Unit 呢

187
00:15:13,500 --> 00:15:13,733
这个

188
00:15:13,766 --> 00:15:17,733
整个就是我们 TPUV1 跟 V2 最大的区别

189
00:15:17,766 --> 00:15:21,133
也是从推理到训练最大的改变

190
00:15:21,466 --> 00:15:23,033
今天的内容呢就这么多

191
00:15:24,400 --> 00:15:25,366
拜了个拜

192
00:00:05,600 --> 00:00:06,566
哈喽大家好

193
00:00:12,766 --> 00:00:15,233
虽然天天负能量喊着冲不动啊

194
00:00:19,800 --> 00:00:20,200
今天呢

195
00:00:26,633 --> 00:00:29,266
去看看训练卡跟推理卡有什么不一样

196
00:00:32,400 --> 00:00:34,500
对整个 AI 芯片领域呢

197
00:00:34,533 --> 00:00:36,733
能有哪些划时代的变化

198
00:00:38,833 --> 00:00:41,033
我们现在来到了国外 AI 芯片里面

199
00:00:41,066 --> 00:00:43,433
特别是在 AI 谷歌 TPU 系列

200
00:00:51,700 --> 00:00:53,466
TPU 第一款训练卡

201
00:01:09,833 --> 00:01:11,033
它的每一代呢

202
00:01:16,166 --> 00:01:19,166
我们回顾一下谷歌的 TPU1 里面呢

203
00:01:26,266 --> 00:01:27,666
确实非常划时代

204
00:01:35,333 --> 00:01:37,100
数据跑飞的问题

205
00:01:37,233 --> 00:01:37,633
这个呢

206
00:01:50,000 --> 00:01:50,666
入的概念

207
00:01:52,500 --> 00:01:54,433
非常具有划时代的意义

208
00:01:56,933 --> 00:01:59,666
但是它对整个 AI 芯片的业界呢

209
00:01:59,700 --> 00:02:01,900
带来很多不一样的创新点

210
00:02:02,633 --> 00:02:03,766
现在我们整体看看

211
00:02:10,666 --> 00:02:11,266
这里面呢

212
00:02:11,266 --> 00:02:15,600
就镶嵌了四块 TPU2 的核心训练的芯片

213
00:02:17,733 --> 00:02:21,066
都有一款简单的小小的 TPU2 的芯片

214
00:02:21,466 --> 00:02:22,666
四块连接在一起呢

215
00:02:27,533 --> 00:02:28,500
V1 到 V2 呢

216
00:02:31,900 --> 00:02:32,566
大家都知道呢

217
00:02:33,833 --> 00:02:35,600
会比推理的过程复杂很多

218
00:02:38,566 --> 00:02:40,833
因为推理确实芯片做起来呢

219
00:02:42,533 --> 00:02:44,300
整个软件栈也会薄很多

220
00:02:44,333 --> 00:02:45,466
而训练呢

221
00:02:45,466 --> 00:02:47,833
它确实会复杂很多很多

222
00:02:48,666 --> 00:02:50,066
我太难了

223
00:02:51,266 --> 00:02:53,266
这里面 ZOMI 呢总结了 5 点

224
00:03:00,700 --> 00:03:03,000
它可以在推理任务里面横向的去扩展

225
00:03:03,266 --> 00:03:03,966
但是训练呢

226
00:03:06,133 --> 00:03:07,700
就需要迭代百万次

227
00:03:14,033 --> 00:03:16,600
第二个呢就是更复杂的计算

228
00:03:32,266 --> 00:03:33,666
或者更大的显存呢

229
00:03:37,466 --> 00:03:38,233
或者显存

230
00:03:39,900 --> 00:03:41,633
我们会产生反向传播

231
00:03:50,800 --> 00:03:53,066
可能会膨胀 8 到 9 倍

232
00:03:53,066 --> 00:03:54,933
所以说它需要更大的内存

233
00:03:55,266 --> 00:03:55,666
第四个呢

234
00:04:00,166 --> 00:04:01,733
在快速的演进

235
00:04:06,033 --> 00:04:08,333
大家 AI 芯片要演进的非常的快

236
00:04:08,700 --> 00:04:09,300
那最后一个呢

237
00:04:09,333 --> 00:04:11,900
就是高精度的数据格式

238
00:04:11,900 --> 00:04:12,833
在推理的过程当中

239
00:04:12,866 --> 00:04:13,633
我们之前都说了

240
00:04:18,033 --> 00:04:20,233
我们的梯度性是是非常非常的小的

241
00:04:24,666 --> 00:04:27,666
PF 16 啊 FP 12 等混合进度的计算

242
00:04:36,433 --> 00:04:38,200
就做得非常的巧妙

243
00:04:38,266 --> 00:04:40,033
他将 TPU 第一代的架构呢

244
00:04:42,266 --> 00:04:46,066
变成了谷歌 TPU 第二代用来做训练的

245
00:04:46,100 --> 00:04:48,833
而针对 TPU 第二代的架构的修改呢

246
00:05:02,833 --> 00:05:05,200
歌 TPU 第二代跟谷歌 TPU 第一代

247
00:05:08,533 --> 00:05:09,233
ZOMI 觉得是呢

248
00:05:12,666 --> 00:05:14,666
真的是 BF16 非常的重要

249
00:05:14,666 --> 00:05:15,800
没有了 BF16

250
00:05:18,466 --> 00:05:19,666
不过呢这是题外话

251
00:05:20,700 --> 00:05:23,700
谷歌第二代的一些真正的改变

252
00:05:30,200 --> 00:05:33,566
也就是右边这个图的这两个黄色方框

253
00:05:33,600 --> 00:05:35,066
在推理的场景当中呢

254
00:05:37,400 --> 00:05:38,900
实际上对我们的计算呢

255
00:05:38,933 --> 00:05:41,733
或者对我们的存储呢是非常有帮助的

256
00:05:41,766 --> 00:05:42,800
因为它比较 domain

257
00:05:42,833 --> 00:05:43,800
specific 嘛

258
00:05:43,866 --> 00:05:45,733
专门针对我们的领域来的

259
00:05:55,433 --> 00:05:56,300
换完位置之后呢

260
00:05:56,333 --> 00:05:58,100
再把 accumulator 这个模块呢

261
00:05:58,133 --> 00:06:00,400
跟 activation storage 这两个模块呢

262
00:06:00,766 --> 00:06:01,700
合并起来

263
00:06:09,033 --> 00:06:11,233
我们的可编程的缓存

264
00:06:21,200 --> 00:06:24,366
是专门针对激活的函数进行处理的

265
00:06:30,300 --> 00:06:31,800
这个特殊的 ALU 计算呢

266
00:06:31,800 --> 00:06:32,933
已经没有办法去满足

267
00:06:41,066 --> 00:06:43,866
专门来进行计算和处理

268
00:06:47,866 --> 00:06:50,233
第三个重要的改变呢就是 MXU

269
00:06:53,733 --> 00:06:55,933
现在呢用了新的方式之后

270
00:07:03,433 --> 00:07:05,700
都是通过 Vector Unit 进行分发

271
00:07:06,733 --> 00:07:09,466
MXU 脉冲阵列单元的计算

272
00:07:17,600 --> 00:07:19,966
对编译器和编程人员来说呢更友好

273
00:07:20,133 --> 00:07:23,133
我们只关心 Vector Unit 的控制

274
00:07:32,166 --> 00:07:33,900
第一代的时候使用 DDR3 的内存

275
00:07:40,933 --> 00:07:42,800
但是呢在训练的过程当中呢

276
00:07:42,900 --> 00:07:43,833
我们会在中间

277
00:07:46,300 --> 00:07:47,333
那这个时候呢

278
00:07:51,966 --> 00:07:55,766
我们把 DDR3 跟 Vector memory 放在一起

279
00:07:55,866 --> 00:07:57,433
然后再做一个改变

280
00:07:57,500 --> 00:08:00,133
把 DDR3 呢换成 HBM

281
00:08:00,166 --> 00:08:04,300
使得我们整体的读写的速度快了 20 倍

282
00:08:06,266 --> 00:08:09,100
这期视频又不小心讲多了好多的废话

283
00:08:10,000 --> 00:08:12,733
才来到了 TPU2 的具体的核心

284
00:08:12,766 --> 00:08:14,266
那谷歌 TPU 第二代的核心呢

285
00:08:16,700 --> 00:08:17,666
有 Vector Unit 呢

286
00:08:18,833 --> 00:08:20,833
还有一个 Transpose/Permute Unit

287
00:08:25,466 --> 00:08:28,033
不知道国内的 AI 芯片厂商的设计者

288
00:08:28,133 --> 00:08:29,633
还有国内 AI 芯片的从业者

289
00:08:35,466 --> 00:08:37,666
特别是 DSA 都是采用这种方式呢

290
00:08:41,566 --> 00:08:42,966
或者他的思想呢

291
00:08:43,000 --> 00:08:44,933
都被大家借鉴过来了

292
00:08:52,533 --> 00:08:54,033
我们看一下它的指令呢

293
00:09:03,366 --> 00:09:06,533
并且将具体的指令呢发送到 VPU 啊

294
00:09:06,566 --> 00:09:08,133
也就是 Vector process Unit

295
00:09:14,200 --> 00:09:16,466
就是我们的 Vector Unit

296
00:09:17,833 --> 00:09:21,433
就是左边的红色的这个大框框

297
00:09:21,500 --> 00:09:23,700
整体的 Vector Unit 呢有一个 Control 了

298
00:09:23,733 --> 00:09:24,933
通过 Control 呢来去下

299
00:09:24,933 --> 00:09:26,333
把我们的具体的指令

300
00:09:26,400 --> 00:09:29,233
给到不同的协处理器上面

301
00:09:42,900 --> 00:09:44,633
具体的参数规格呢

302
00:09:58,900 --> 00:10:00,466
其实在 AI 的运算过程当中呢

303
00:10:00,533 --> 00:10:02,100
它不算说非常重要

304
00:10:02,133 --> 00:10:04,333
但是呢没有它肯定是不行的

305
00:10:05,033 --> 00:10:05,433
现在呢

306
00:10:05,466 --> 00:10:07,733
我们来到之前在谷歌 TPU 第一代里面

307
00:10:07,800 --> 00:10:11,766
很重要的一核心就是 MXU 脉动阵列

308
00:10:21,400 --> 00:10:22,533
支持混合精度

309
00:10:22,633 --> 00:10:26,766
支持 BF16 跟 FP32 两种格式

310
00:10:27,866 --> 00:10:28,833
最后还有一个另类呢

311
00:10:35,066 --> 00:10:37,333
矩阵的规约置换矩阵

312
00:10:40,600 --> 00:10:42,800
特别是反向的传播过程当中呢

313
00:10:46,400 --> 00:10:47,900
哎慢着慢着

314
00:10:48,066 --> 00:10:49,466
ZOMI 老师你好啊

315
00:10:50,900 --> 00:10:55,033
刚才你不是说到 V2 呢变成 128 乘以 28 吗

316
00:10:55,200 --> 00:10:58,033
为什么谷歌不再用 256 乘 256

317
00:11:04,533 --> 00:11:08,133
小新你问的这个问题呢非常的及时哦

318
00:11:08,433 --> 00:11:11,233
我们看看右边的这个图案

319
00:11:11,733 --> 00:11:14,266
如果买从这里的大小呢是 256 乘 256 呢

320
00:11:23,633 --> 00:11:25,633
这么大的一个矩阵的计算的

321
00:11:26,033 --> 00:11:28,233
而放到 64*64 呢

322
00:11:28,433 --> 00:11:29,900
虽然发现呢利用率多了

323
00:11:33,033 --> 00:11:36,833
谷歌就采用 128*128 权衡芯片的电路面积

324
00:11:47,333 --> 00:11:48,633
来到了最后一个内容

325
00:11:51,800 --> 00:11:54,633
不知道大家觉得 10 分钟比较合适

326
00:11:56,233 --> 00:11:58,600
把每一节课的内容都详细的展开

327
00:11:58,666 --> 00:12:00,533
或者尽可能的详细的展开呢

328
00:12:04,000 --> 00:12:04,400
我们看看

329
00:12:04,500 --> 00:12:05,466
谷歌 TPU Core 里面的

330
00:12:05,566 --> 00:12:07,433
一个具体的互联的方式

331
00:12:12,133 --> 00:12:13,333
这么这么一个模块

332
00:12:15,866 --> 00:12:17,600
超级计算机的形态的

333
00:12:26,466 --> 00:12:28,566
不断的去喂进我们的神经网络

334
00:12:31,966 --> 00:12:33,800
所以单个核心呢是不够用的

335
00:12:33,833 --> 00:12:35,133
我们需要用到并行的计算

336
00:12:37,300 --> 00:12:37,966
值得注意的

337
00:12:38,300 --> 00:12:41,133
TPU 核心跟 TPU 核心之间的互联带宽

338
00:12:41,200 --> 00:12:42,500
而不是卡跟卡之间

339
00:12:44,966 --> 00:12:47,766
现在呢我们看一下核之间的互联

340
00:12:50,266 --> 00:12:51,466
就是我们刚才讲到的

341
00:12:51,533 --> 00:12:54,366
右边的这个红色的 interconnect router

342
00:12:57,300 --> 00:12:58,533
也就是 2D 的 torus

343
00:13:13,200 --> 00:13:13,800
实际上呢

344
00:13:16,066 --> 00:13:17,200
有两个核心

345
00:13:17,200 --> 00:13:19,066
也就是有一个 Core1 和一个 Core2

346
00:13:19,266 --> 00:13:21,466
组成一块小小的芯片

347
00:13:21,466 --> 00:13:22,333
整个芯片里面呢

348
00:13:22,366 --> 00:13:24,566
是通过 interconnect router 呢

349
00:13:39,633 --> 00:13:42,633
TPU Core 跟 TPU Core 之间的带宽呢是其次的

350
00:13:49,400 --> 00:13:51,133
去建立多级的缓存

351
00:14:01,833 --> 00:14:03,333
最后的总结和思考了

352
00:14:06,400 --> 00:14:07,966
里面我们刚才呢有两个核心

353
00:14:08,166 --> 00:14:09,300
上面是一个核心

354
00:14:09,300 --> 00:14:10,500
下面是一个核心

355
00:14:14,033 --> 00:14:16,600
这里面呢最大电路呢还是 MXU

356
00:14:20,066 --> 00:14:21,633
我们尝试做一个总结

357
00:14:21,666 --> 00:14:24,500
TPU V2 呢主要是提供了两个 Tensor Core

358
00:14:27,266 --> 00:14:28,466
也就是我们的 Activation Pipeline

359
00:14:43,066 --> 00:14:44,466
方便我们的 AI 编译器呢

360
00:14:52,233 --> 00:14:55,766
DDR3 而且还使得 HBM 和 VM 之间呢

361
00:15:11,333 --> 00:15:12,533
进行一个控制

362
00:15:23,266 --> 00:15:24,400
谢谢各位喽

363
00:02:22,633 --> 00:02:24,400
形成我们整一块板卡

364
00:02:32,533 --> 00:02:33,733
在训练过程当中呢

365
00:02:50,000 --> 00:02:51,200
那训练有哪些难点呢

366
00:04:40,000 --> 00:04:42,000
做了一些稍微的改动

367
00:07:43,800 --> 00:07:46,166
产生非常大量的变量和权重

368
00:08:29,600 --> 00:08:31,466
或者国内 AI 系统的从业者呢

369
00:10:49,400 --> 00:10:50,600
我有个问题哦

370
00:11:48,600 --> 00:11:51,766
就是谷歌 TPU 的内存和互联的方式

371
00:13:13,766 --> 00:13:16,033
谷歌 TPU 的一个芯片里面呢

372
00:13:24,533 --> 00:13:27,366
去把两个核心连接在一起

373
00:00:01,500 --> 00:00:03,366
字幕生成：mkwei  字幕校准：mkwei

374
00:00:06,666 --> 00:00:07,900
想成功先发疯

375
00:00:17,566 --> 00:00:19,333
加油奥利给

376
00:00:23,200 --> 00:00:25,766
第二代呢是谷歌第一款训练卡

377
00:00:25,833 --> 00:00:26,066
是我们

378
00:00:26,133 --> 00:00:26,533
今天呢

379
00:00:43,633 --> 00:00:45,366
这一个系列里面

380
00:00:45,400 --> 00:00:46,533
真正的去看看

381
00:00:53,566 --> 00:00:57,700
去看看 TPU 第二代 TPU2 有什么不一样哦

382
00:00:58,633 --> 00:00:59,833
ZOMI 老师你好啊

383
00:01:06,800 --> 00:01:07,300
实际上呢

384
00:01:19,166 --> 00:01:21,433
就首创用了 INT8 的数据格式

385
00:01:43,533 --> 00:01:45,033
可能到了 2022 年

386
00:01:48,233 --> 00:01:50,000
这些谷歌在七八年前已经引

387
00:01:50,633 --> 00:01:52,400
所以我们说谷歌的 TPU

388
00:02:15,733 --> 00:02:17,666
每一块散热卡下面呢

389
00:02:28,600 --> 00:02:31,600
它最重要的改变呢就是从推理到训练

390
00:03:04,000 --> 00:03:06,200
我们简单的一个小模型为 Resnet50 呢

391
00:03:21,600 --> 00:03:24,133
还要做自动微分计算权重参数和

392
00:03:24,133 --> 00:03:25,633
输入数据的导数

393
00:03:26,900 --> 00:03:28,766
都要求更高的精度

394
00:03:33,766 --> 00:03:35,066
因为在推理过程当中呢

395
00:03:35,066 --> 00:03:36,266
我们可以非常快速的

396
00:03:42,900 --> 00:03:44,033
还有我们优化器的

397
00:03:47,366 --> 00:03:49,133
会比我们简单的训练

398
00:03:58,533 --> 00:04:00,100
因为训练的算法和模型呢

399
00:04:13,666 --> 00:04:16,500
可以他用 INT8 的整形做推理

400
00:04:22,900 --> 00:04:24,633
于是呢就引入了 BF 16 啊

401
00:04:51,000 --> 00:04:51,766
我们今天呢

402
00:05:15,733 --> 00:05:18,366
做大模型的预训练呢很容易跑飞

403
00:05:25,066 --> 00:05:26,833
其实有两个存储区啊

404
00:05:28,333 --> 00:05:30,000
一个是 activation storage

405
00:05:35,100 --> 00:05:37,366
设置了一些专用的存储模块呢

406
00:05:47,366 --> 00:05:49,733
为了提升可编程性呢

407
00:06:06,266 --> 00:06:09,066
于是呢就变成一个 Vector memory

408
00:06:13,333 --> 00:06:16,433
下个我们针对 Activation Pipeline

409
00:06:18,200 --> 00:06:19,066
Activation Pipeline

410
00:06:24,433 --> 00:06:26,800
也就是我们卷积之后有个 batch normalization

411
00:06:50,300 --> 00:06:53,666
原来 MXU 呢是跟 Vector memory 相连接的

412
00:06:55,900 --> 00:06:57,300
谷歌 TPUV2 呢

413
00:07:06,200 --> 00:07:06,700
这个时候呢

414
00:07:17,033 --> 00:07:17,633
这种结构呢

415
00:07:24,266 --> 00:07:27,733
来去下发到不同的计算单元上面

416
00:07:30,000 --> 00:07:31,566
就是 DDR3

417
00:07:31,600 --> 00:07:32,266
一开始呢

418
00:07:36,766 --> 00:07:38,866
每一次需要用到的一些权重呢

419
00:07:50,400 --> 00:07:51,900
于是呢在训练的过程当中呢

420
00:08:05,666 --> 00:08:06,266
哎呀

421
00:08:14,266 --> 00:08:15,500
其实有非常多啊

422
00:09:36,766 --> 00:09:39,566
里面呢有非常多个 ALU

423
00:09:54,533 --> 00:09:58,066
更多的是计算一些简单的标量的运算

424
00:10:33,466 --> 00:10:35,033
例如矩阵的转置呢

425
00:10:42,733 --> 00:10:44,500
会经常遇到的

426
00:11:22,733 --> 00:11:23,666
都能够吃得下

427
00:11:29,933 --> 00:11:32,133
但是呢性能却下降了

428
00:11:45,233 --> 00:11:47,233
终于一口气讲完这么多

429
00:12:07,433 --> 00:12:09,433
在 HBM 和 Vector memory 里面呢

430
00:12:19,200 --> 00:12:22,833
就是右边的这个框框 interconnect router

431
00:12:28,766 --> 00:12:31,333
不断轮续的迭代循环去训练

432
00:12:38,000 --> 00:12:38,333
就是

433
00:12:48,600 --> 00:12:49,300
核间的互联

434
00:13:11,100 --> 00:13:13,133
都是左边的这个模块

435
00:13:32,600 --> 00:13:35,600
线越宽代表它的带宽越高

436
00:13:51,066 --> 00:13:53,166
通过减少英伟达 GPU 的时延呢

437
00:14:10,466 --> 00:14:12,666
中间呢通过 interconnect router 呢

438
00:14:32,066 --> 00:14:34,366
那下面几个都是 Vector Unit 带来的好处

439
00:15:12,500 --> 00:15:13,433
调度分发

440
00:00:09,500 --> 00:00:12,500
我是人到中年冲不动的 ZOMI

441
00:00:29,933 --> 00:00:32,400
谷歌推出的第一款训练卡呢

442
00:01:11,066 --> 00:01:14,133
ZOMI 老师都觉得它是一个跨时代的

443
00:02:07,333 --> 00:02:10,700
就是它的 TPU2 里面的整一块板卡

444
00:06:01,833 --> 00:06:06,233
变成更加类似于传统 CPU 里面的 L1 cache

445
00:07:12,333 --> 00:07:16,766
就成为了向量单元 VU 的一个协处理器

446
00:08:54,100 --> 00:08:55,600
具体从哪里获取过来的

