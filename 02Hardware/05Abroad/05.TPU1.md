# 谷歌TPU v1 - 脉动阵列

## 本章简介

[TODO]




我们先来看一下谷歌的打印电路板


![TPU1 打印电路板](images/tpuv1/tpuv1_01.png)

## TPU v1 芯片架构

首先我们来解析一下TPU v1上几个关键的元素：

- **DDR3 DRAM / Weight FIFI**: 第一个讲的是谷歌的Weight FIFO，位于图像的右上角。在模型推理场景，通过DDR3-2133接口，模型权重会被存储在TPU v1上的DDR3 RAM内。这些权重通过PCIe从主机计算机的内存中“预加载”到这些芯片上，然后可以传输到芯片的Weight FIFO内存中，以便矩阵乘法单元（MXU）使用。

- **矩阵乘法单元 MXU（MMU）**: MXU以脉动阵列的形式工作，能够提供$ 256 \times 256 \times 8 $bit的乘加计算，在每个时钟周期输出256个16bit的部分和计算结果。矩阵单元里面包含一个64kb的Weight Tile以及一个双缓存单元用于缓存回调。MXU被谷歌的工程师们描述为“TPU的心脏”，在本章节后面的内容中，我们会更加细致的去剖析这一部分的设计。

- **累加器 Accumulators**: 这是一个能够存储4MiB的32-bit数据的累加单元，用来存储MXU计算后的结果。这4MiB代表着4096个，每个含256个元素的32位累加器。为什么是4096呢？原因是谷歌的工程师注意到每字节的运算次数需要达到峰值性能大约是1350，继而将其向上舍入到2048，然后再翻倍让便编译器在运行至峰值性能时能使用双缓冲，这也就是为什么这个累加器被设计为4MiB。

- **控制指令 Control**: 每个芯片都需要一个控制模块，而在TPU中，整个控制单元采用了四级流水线设计。控制单元的主要任务是接收通过PCIE总线或主机（CPU）传递的指令，并将这些指令执行于TPU。这些指令源自CPU，而芯片的指令集为CISC，共包含12条指令。TPU采用CISC而非更简单的WISC指令集的原因是，谷歌定义的每条指令的平均执行周期为10-20个时钟周期，这使得每条指令相对复杂。特别是，TPU中的各种单元，如MXU、UB和AU，都定义了一些专门为神经网络设计的高级复杂指令。
    - Read_Host_Memory: 从CPU host读取数据到Unified Buffer 
    - Read_Weight: 从Weight DRAM读取数据到Weight FIFO
    - Matrix Multiply/Convolve: 执行乘法或卷积运算
    - Activate: 执行ReLU，Sigmoid等激活计算
    - Write_Host_Memory: 把计算结果数据从Unified Buffer输出到CPU host

通过这些指令，TPU能够顺序地执行读取、写入、计算和激活操作，从而处理神经网络各层的具体计算需求。
![Alt text](images/tpuv1/tpuv1_02.png)

## TPU v1芯片布局图

我们可以看到整个TPU属于一个专用的电路，里面最大的两个就是Local Unified BUffeer和MXU，一个用于缓存，一个用于计算加乘计算。由于TPU是专门应用于矩阵计算的芯片，继而不需要极度复杂的控制单元，所以我们上文讲到的控制器只用了2%的面积，给核心功能留下了更多的空间。

![Alt text](images/tpuv1/tpuv1_03.png)

## 脉冲阵列 - Systolic Array

脉冲阵列就是TPU的核心，也是我们本章最重要的内容。脉冲阵列的英文名Systolic Array就是源于它处理数据的节奏就像我们的心跳一样，于是便有了这个名字。

在我们讲到具体的实现之前，我们需要先回顾一下Img2Col这个算法。我们知道，在推理场景，在2017年附近，卷积神经网络占据了当时场景的半壁江山。在卷积计算的时候，我们实际上不会对真正地对图片或者feature map进行卷积，而是会用Img2Col的方式把图片变成矩阵，把我们的卷积换成矩阵相乘的方式。在我们之前推理系统里面我们讲过算法是怎么把卷积操作变成在数学上和卷积相同的矩阵乘法操作，再通过Col2Img返回来把我们的计算结果变成feature map。而自然而然，其中计算压力最大的部分便是我刚才提到的“矩阵乘法”操作。

![Alt text](images/tpuv1/tpuv1_04.png)

如下图所示，两个矩阵相乘实际上就是用矩阵A的一行去点乘矩阵B的一列，最终获得输出矩阵的某一个元素。

![Alt text](images/tpuv1/tpuv1_05.png)

一般来说，这种计算