# AI 计算体系总结

## 主要知识点回顾

本章我们主要是想弄清楚AI计算体系到底是什么，围绕着这个目标我们依次从如下几个方面进行了梳理，每个内容都对应着AI计算体系的一些思考。下面我们将对着几个方面做一个简单的知识回顾。

- 深度学习的计算模式
- AI的计算体系
- 矩阵乘运算
- 比特位宽

**深度学习计算模式**

AI有三种范式：监督学习，无监督学习，强化学习。CNN，RNN， LSTM， Transformer都是经典的网络模型。随着AI模型精度的提升，网络结构也越来越大，参数越来越多。为了减轻模型内存大小和运算量，模型压缩和剪枝成为AI领域的一个热门研究方向。AI模型中轻量化的网络模型结构可以让AI模型部署在更多的业务场景中，进一步推广AI应用。随着大语言模型chatGPT3.5的优异表现，大模型应用进入一个爆发期。张量并行，Pipeline并行，Sequence并行等分布式并行技术都是做大模型训练和推理的重要一环。

**AI的计算体系**

深度学习的计算模式对AI芯片的设计具有很大的牵引力，这些模式特点决定了AI的关键指标。精度、吞吐量、时延、能耗、系统价格、易用性等都是AI的关键指标，AI芯片设计中带宽和PE执行引擎决定了AI模型在芯片上的峰值算力情况，所以在带宽和PE之间找到一个平衡点让算力利用率最大化非常重要。

**矩阵乘运算**

AI模型中，矩阵乘是一个非常核心的算子，一定程度决定了AI模型的整体性能走向。像conv, FC, Linear都可以转换为矩阵乘的运算。对矩阵乘的性能优化可以从软件和硬件两个方面进行考虑。软件上可以减少冗余的MACs运算，对kernel进行Loop优化，Memory优化等来提高PE利用率。硬件上可以通过增加PE单元执行能力，支持低比特PE计算，增加片内Cache，增加内存带宽等提高PE利用率。

**比特位宽**

AI的计算本质就是数据的运算，不同数据类型具有不同的比特位宽，而不同的比特位宽对AI芯片的能耗和面积也是不一样的。所以针对不同的应用场景，选择不同位宽的数据类型可以提高AI芯片的性价比。AI模型训练一般使用FP32和FP16数据类型，而推理一般是Int8和FP16数据类型。

## 总结

在这一章节中，我们主要介绍的是AI系统中硬件模块关于AI计算体系的一些背景知识点，通过本章的学习可以帮助我们更好的理解AI芯片的技术发展路线。最后再做一个简单的概况：

1. 通过整体看看 AI 或者深度学习计算模式：经典模型结构和轻量化模型结构、模型量化和剪枝到大模型分布式并行，从而理解“计算”需要什么。

2. 通过AI芯片关键指标，了解一块AI芯片要更好的支持“计算”，需要关注那些重点工作，从而引出峰值算力和带宽之间的关系。

3. 最后通过深度学习的计算核心“矩阵乘”来看对“计算”的实际需求和情况，为了提升计算性能、降低功耗和满足训练推理不同场景应用，对“计算”引入 TF32/BF16 等复杂多样的比特位宽。

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=866076530&bvid=BV1j54y1T7ii&cid=1056463544&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
