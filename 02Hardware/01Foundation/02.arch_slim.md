# AI计算模式(上)

# AI的发展与范式
## AI的发展
人工智能（Artificial Intelligence，AI）最早是在1956年的美国达特茅斯会议上被正式确立下来。早期人工智能依靠人类经验建立推理系统。到了上世纪70年代，研究者构建了更为复杂的专家系统，采用知识表示和知识推理等技术解决复杂问题。然而，人对本身的某些智能行为也很难解释其中的原理，因此也我们也很难通过知识和推理的方式实现这些行为的智能系统。机器学习（Machine Learning，ML）主要是设计一些学习算法，让计算机可以在数据中自行分析并找出其中的规律，用于后续的预测。

受人脑神经系统启发，早期学者提出了一种模仿神经系统的数学模型——人工神经网络。早期著名的神经网络模型是感知机，可以通过迭代试错完成学习过程，然而该模型无法处理异或问题。1974年，反向传播算法被提出，后来被LeCun引入卷积神经网络中，在手写体数字识别任务上取得巨大成功。近年来，学者不断探索训练神经网络的方法，通过“预训练+微调”的方式训练深度神经网络，对一个通用大模型采用预训练，处理下游任务时再对模型进行微调。随着大规模并行计算以及GPU的普及，训练模型可用的算力大幅提升，神经网络模型可变参数量越来越庞大，模型的学习能力大大增强。

## AI三大范式
一般训练AI有三大范式，监督学习、无监督学习以及强化学习。监督学习是指用已知标签的样本进行训练，常见的是回归问题、分类问题。无监督学习则是用无标签样本训练，常见的是聚类、密度估计、特征学习。监督学习就像语文老师教学生哪些是病句，然后让学生去判断新的句子是不是病句。无监督学习则是语文老师把很多句子和病句混合在一起，学生自行探索其中的规律将病句找出来。强化学习则是一种“试错”学习，通过尝试不同行为在当前环境下得到的环境反馈来调整智能体的行为策略，就像学生写作文让老师来打分，得分高说明此次写作好，下次碰到类似的题目可以继续这样写，如果得分低则说明此次写作不好，要换个写法试试。

# 网络模型结构设计和演进
## 什么是神经网络
神经网络是一个很形象的描述，回顾一下生物中的神经系统，最基本的单元是神经元，而人工神经网络中同样以神经元作为基本单元。神经元一般有多个输入，每个输入通道都有一个权重值，这个权重值是可变的，神经网络通过不断调整权重值最后可以输出想要的结果。对于输入$x_0,x_1,...,x_{n-1}$，每个输入通道都有一个权重，分别是$w_0,w_1,...,w_{n-1}$，附加一个偏置$b$，于是神经元的输入的加权和为$z=\sum_{d=0}^{n} w_dx_d + b$，将加权和输入到非线性激活函数$f$中，得到该神经元的激活值，$a=f(z)$。在现代神经网络中，激活函数都是连续可导的。常见的激活函数有Tanh，ReLU，Sigmoid，Linear等。
![激活函数](https://upload-images.jianshu.io/upload_images/26105747-f33515bb25c12b33.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


下面将神经网络的计算用更简化的符号表达：将输入的表示简化为向量$\mathbf{x}$，每个神经元的所有通道权重是$\mathbf{w}$，那么一个神经元的输出则是$\sigma(\mathbf{w}^T\mathbf{x}+b)$，当输入同时给多个神经元时，这些神经元输出组成的向量可以表示为$\sigma(\Big[{{\mathbf{w}^T_0 \atop \mathbf{w}^T_1} \atop {... \atop \mathbf{w}^T_{n-1}}}\Big]\mathbf{x}+\Big[{{b_0 \atop b_1} \atop {... \atop b_{n-1}}}\Big])$，其中$\Big[{{\mathbf{w}^T_0 \atop \mathbf{w}^T_1} \atop {... \atop \mathbf{w}^T_{n-1}}}\Big]$可以用矩阵$\mathbf{W}$表示。这些神经元组成了一层神经网络，神经网络理论上可以不断叠加，通过构建更大更深的神经网络以增强它的学习能力。

神经网络不同层学习到的知识是不同的，Google团队的DeepDream文章中对GoogLeNet不同网络层的激活值进行可视化，发现浅层（靠近原始输入的网络层）学习到的是线条纹理的特征，而深层（接近结果输出的网络层）学习到的是更复杂的图案，接近输入的图像。

## 主流网络结构
### 全连接层
全连接网络层是相邻层的神经元都两两相连，因此对于第$l$层网络，进行下面两种运算
$$\mathbf{z}^{(l)}=\mathbf{W}^{(l)} \mathbf{a}^{(l-1)}+\mathbf{b}^{(l)},\\
\mathbf{a}^{(l)}=\sigma( \mathbf{z}^{(l)} )$$
第一种是仿射变换，第二种是非线性变换。$\mathbf{W}^{(l)}$和$\mathbf{b}^{(l)}$是第$l$层的参数，第$l$层的激活$\mathbf{a}^{(l)}$是由第$l-1$层的激活值决定。

### 卷积层
卷积层是卷积神经网络中的核心，而卷积神经网络常用于图像识别。在全连接层中，如果相邻层神经元两两连接，那么参数量会非常大，而采用卷积操作，将会大大减少可学习的参数量。卷积层有两个主要特点：1）局部连接：卷积层的每一个神经元都只和下一层的局部窗口内的神经元相连，权重参数由原来的$M_l \times M_{l-1}$变成$M_l \times K$，$K$是卷积核大小；2）权重共享，所有神经元共享相同的卷积核参数。
![全连接层与卷积层对比](https://upload-images.jianshu.io/upload_images/26105747-b392cf6e03be355e.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


不同于全连接层神经元用一维展开的形式表达，为了更直观理解卷积层，常用二维或者三维形式组织卷积层的神经元。下面是两种常见的卷积方式：第一种的特点是填充零使得输入特征图和输出的特征图大小一致；第二种则是不填充零，因此输出特征图比输入特征图更小。
![两种基本的卷积方式](https://upload-images.jianshu.io/upload_images/26105747-c4cb5b2ef84614b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

将其扩展到三维，对于一张有RGB三通道的32X32的图像，那么输入数据的大小为32X32X3，那么对第一层卷积层中每个输出特征图都需要3个卷积核，用这3个独立的卷积核分别对RGB三通道的图像进行卷积计算（第一种卷积方式）并求和，之后附加偏置并输入到激活函数得到一个32X32大小的输出特征图，这个特征图上可能包含了原图像某一方面的特征。如果需要学习更多特征则需要输出多个相互独立的特征图，例如输出16个特征图，那么第一层卷积层的输出就是32X32X16，可以作为下一个网络层的输入。那么这个卷积层的权重参数量则是$3\times16\times K$。

### 循环神经网络
循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络。在RNN中，神经元不但可以接受其它神经元的信息，也可以接受自身的信息。给定一个输入序列$x_1,x_2,...,x_T$，RNN中的神经元包含两部分输入，一部分是$t$时刻下的输入$x_t$，一部分是上一个时刻的隐状态$h_{t-1}$，那么该神经元在$t$时刻的输出是
$$h_t=f(Uh_{t-1}+Wx_t+b)$$
其中$U,W,b$是神经元的可训练参数，$f$是激活函数，输出$h_t$也被称为$t$时刻的隐状态，可作为下一时刻的输入。

### 注意力机制
注意力机制是一种资源分配手段，将有限的计算资源用于处理更重要的信息。注意力机制的计算可分为两步，第一步是对所有输入的信息计算注意力分布，第二步则是根据注意力分布来计算输入信息的加权平均。为了从输入信息中选择出与特定任务相关的信息，引入一个与任务相关的查询向量$q$，给定一个查询向量$q$以及输入$x$，通过一个打分函数$s$计算$q,x$之间的相关性。$s$打分函数有几种常见的计算方式：
$$加法\quad\quad\quad s(x,q)=Wx+Uq,\\
点积\quad\quad\quad\quad\quad\,\,\, s(x,q)=x^Tq,\\
缩放点积\quad\quad\quad s(x,q)=\frac{x^T q}{\sqrt D},\\
双线性\quad\quad\quad\,\, s(x,q)=x^TWq,\\$$

注意力机制有多种变体，包括键值对注意力、多头注意力、结构化注意力等等。键值对注意力将输入分为两部分，分别是key和value，key参与打分函数的计算，value则作为注意力机制作用的对象，当key和value相同时就是普通的注意力机制。多头注意力是使用了多个查询向量，分别计算打分函数，每个注意力关注输入信息的不同部分。结构化注意力则是剖析输入信息的结构，比如文本信息可分为词、句、段、篇章，具有层次化结构，可以使用层次化的注意力进行信息选择。当前受人关注的transformer模型主要基于多头自注意力机制，其中注意力是采用键值对注意力，打分函数计算采用缩放点积的方式。

## 经典卷积神经网络模型
典型的卷积神经网络是由卷积层、池化层、全连接层交叉堆叠形成。下面是一些经典的卷积神经网络模型，例如LeNet-5，AlexNet，GoogLeNet等等。总体来说，模型的可学习参数量在逐步增大，网络层数也越来越深，同时为了减少计算开销和模型训练时间，学者在卷积核尺寸、模型结构等方面不断进行优化。
|     | LeNet-5 | AlexNet | VGG16 | GoogLeNet | ResNet50 | EfficientNet-B4 |
| -------- | -------- | -------- | -------- | -------- | -------- | -------- |
| 输入尺寸 | 28X28 |  227X227 |  224X224 |  224X224 |  224X224 |  380X380 |  
| 参数量 | 60k | 61M | 138M | 7M | 25.5M | 19M |
| MACs | 341k | 724M | 15.5G | 1.43G | 3.9G | 4.4G | 
| 时间 | 1998 | 2012 | 2015 | 2015 | 2016 | 2019 | 

### LeNet-5
LeNet-5是最早的卷积神经网络之一，其通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别。
第一层是卷积层，有6组特征映射，每组是28X28大小的特征图，紧接着是一个池化层，用于降采样，减少神经网络的参数，这里是最大池化，将上一层的紧邻四个值中选出最大的值作为池化层的激活值，这一层的计算只涉及比较（如果是平均池化则是求和以及除法），且不包含参数。后面叠加了一个卷积层和池化层，之后是三个全连接层，需要注意的是从池化层到全连接层，需要将池化层所有神经元展开成一维向量，长度是16X5X5。后续计算主要涉及矩阵乘法。LeNet-5的结构较为简单，可训练参数量约为60k。
|  网络层  | 输入大小 | 核大小 | 核个数 | padding | stride | 输出大小 |
| -------- | -------- | -------- | -------- | -------- | -------- | -------- |
| conv1 | (3,32,32) | 5x5 | 6 | 0 | 1 | (6,28,28) |
| pool1 | (6,28,28) | 2x2 | none | 0 | 2 | (6,14,14) |
| conv2 | (6,14,14) | 5x5 | 16 | 0 | 1 | (16,10,10) |
| pool2 | (16,10,10) | 2x2 | none | 0 | 2 | (16,5,5) |
| fc1 | 16x5x5 | 120 | none | none | none | 120 |
| fc2 | 120 | 84 | none | none | none | 84 |
| fc3 | 84 | 10 | none | none | none | 10 |

### AlexNet
AlexNet主要在以下几个方面提出优化。
1.ReLU激活函数：传统的神经网络普遍使用 Sigmoid 或者 tanh 等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。以 Sigmoid 函数为例，当输入的值非常大或者非常小的时候，这些神经元的梯度接近于 0（梯度饱和现象），如果输入的初始值很大的话，梯度在反向传播时因为需要乘上一个 Sigmoid 导数，会造成梯度越来越小，导致网络变的很难学习。
2.重叠池化：一般的池化的步长与池化窗口大小一致，但是该模型中池化步长小于窗口长度，实验结果表明可以缓解过拟合。
4.数据增强方法：模型的能力是由模型的参数量和训练的数据量决定的，训练AlexNet时，提出了多种图像数据增强的方法，包括翻转、裁剪、平移、PCA后加上高斯扰动。
4.局部归一化：在神经生物学有一个概念叫做 “侧抑制”（lateral inhibitio），指的是被激活的神经元抑制相邻神经元。归一化（normalization）的目的是 “抑制”，局部归一化就是借鉴了 “侧抑制” 的思想来实现局部抑制，尤其当使用 ReLU 时这种 “侧抑制” 很管用，因为 ReLU 的响应结果是无界的（可以非常大），所以需要归一化。
5.Dropout：引入 Dropout 主要是为了防止过拟合。在神经网络中 Dropout 通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为 0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为 0），直至训练结束。Dropout 也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout 只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。
6.多GPU训练：单GPU内存有限，不利于网络规模扩大，每个GPU负责部分网络，特定层需要GPU之间的通信。

### VGG16
相比于AlexNet，VGG探索了更大更深的网络模型的效果，将卷积核变小，全部使用3X3卷积核，3X3卷积核堆叠三层的感受野与7X7卷积核的感受野一样，用小卷积核堆叠代替大卷积核既可以增加网络深度，也可以减少网络参数，使用大卷积核的参数量为7X7XMXM，换成小卷积核后参数量为3X(3X3XMXM)。

### GoogLeNet
GoogLeNet中，引入了Inception模块，在一个Inception模块使用了多尺度的卷积核，不同尺度的卷积核具有不同大小的感受野，可以提取不同尺度的特征，如下图所示。
![Inception模块](https://upload-images.jianshu.io/upload_images/26105747-fcfd6bdfb3dfa4e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但是上面的结构会导致模型参数量过大，为了降低参数量，在5X5卷积之前用1X1卷积并减少输出的通道数，假设之前需要128X5X5X256，如果中间的通道数设为32，则新的模型中参数量为128X1X1X32+32X5X5X256。
![改进后的Inception模块](https://upload-images.jianshu.io/upload_images/26105747-ebb507a7e716a622.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

之后，为了进一步降低参数量，GoogLeNet将大卷积核变为小卷积核堆叠，比如将5X5的卷积核首先变成两个3X3的小卷积核堆叠，每个3X3的小卷积核再变成1Xn的卷积核和nX1的卷积核的堆叠。


### ResNet
在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现了梯度消失和梯度爆炸两种问题。为了解决这个问题，ResNet中采用了残差结构，让特征图隔层相加。另外采用BN层，代替Dropout，BN层使得整个训练样本集所对应feature map的数据满足一定分布规律（均值为0，方差为1）。



## 经典循环神经网络模型
### 循环神经网络RNN
RNN是基础的循环神经网络，其输入为$x_t$，输出为$y_t$，
$$h_t=f(Uh_{t-1}+Wx_t+b),\\
y_t=Vh_t,$$
其中$U,W,V,b$是可学习参数，$f$是Tanh激活函数。

### 长短期记忆网络LSTM
当输入序列较长时，RNN会存在梯度爆炸和梯度消失的问题，为了解决这个问题，一个有效的改进方法是引入门控机制，其中经典的模型是长短期记忆网络（Long Short-Term Memory Network，LSTM）。LSTM除了隐状态$h_t$，还引入了一个新的内部状态$c_t$。LSTM中有三个门控单元，分别是遗忘门、输入门、输出门，所有门控单元的激活函数均采用Sigmoid函数，输出的值在0到1之间。输出的向量再对各部分信息采用向量元素乘，对信息进行一定程度的遗忘。其中，遗忘门控制上一个隐状态$h_{t-1}$需要遗忘多少信息，输入门控制当前内部状态$\widetilde{c}_t$需要保存多少信息，输出门控制当前内部状态$c_t$有多少信息需要输出给隐状态$h_t$。三个门控的计算方式为
$$f_t=\sigma(W_f x_t+U_f h_{t-1}+b_f),\\
i_t=\sigma(W_i x_t+U_i h_{t-1}+b_i),\\
o_t=\sigma(W_o x_t+U_o h_{t-1}+b_o),\\$$
那么当前时刻$t$的输出为$c_t,h_t$
$$\widetilde{c}_t=tanh(W_{\widetilde{c}} x_t+U_{\widetilde{c}} h_{t-1}+b_{\widetilde{c}}),\\
c_t=f_t\odot c_{t-1}+i_t\odot \widetilde{c}_t,\\
h_t=o_t\odot tanh(c_t)$$
![LSTM门控机制](https://upload-images.jianshu.io/upload_images/26105747-dba48c288debf978.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

LSTM的一种变种是GRU，同样使用门控机制，但结构比LSTM更简单，相比于LSTM计算量更少。

## AI计算模式思考1
1.需要支持神经网络模型的计算逻辑：权重数据共享，以便对神经元的权重值进行求和
2.能够支持高维的张量运算和存储：需要支持大的通道和大的特征图像高效加载
3.支持常用的神经网络模型结构：神经网络中多存在卷积操作、矩阵乘、注意力机制，需要对这些常用操作实现高效矩阵乘

# 模型压缩
## 模型量化
模型量化是指通过减少权重表示或激活所需的比特数来压缩模型。即通过降低参数的表示精度来压缩模型大小，例如原先模型训练时所有参数采用32bit长度，但在用于部署时，为了减少模型所需的内存，将32bit压缩为8bit。通过降低比特位数的模型压缩方法可以实现压缩参数、提升运算速度、降低内存开销、降低功耗、提升芯片面积。目前在模型量化方向上有一些研究热点，包括感知量化训练、减少计算比特位、非线性量化、减少权重计算。

1、感知量化训练：通常量化是在模型训练完成后对参数压缩，但这个过程会使得模型有一定程度的精度损失。通过在模型训练阶段引入量化相关约束，让模型适应量化带来的误差，即量化感知训练（Quantization-aware training，QAT），能够更好地解决模型量化导致的精度下降问题。

2、减少计算比特位：二值化网络模型最早由Yoshua Bengio在2016年提出，论文中使用随机梯度下降方法训练带有二值化的权重和激活值的神经网络模型，网络中权重和激活值只包括1和-1。

3、非线性量化：根据量化数据表示的原始数据范围是否均匀，可以将量化分为线性量化和非线性量化。线性量化也称为均匀量化，在线性量化中相邻两个量化值之间的差距是固定的。非线性量化中量化值之间的间隔不固定。由于网络中值的分布往往是不均匀的，更多的是类似高斯分布的形式。非线性量化可以更好的捕获分布相关的信息，数据多的地方量化间隔小，量化精度高；数据少的地方量化间隔大，量化精度低。因此非线性量化的效果理论上比线性量化更好。然而非线性量化的通用硬件加速比较困难，而且实现更加复杂。

4、减少权重计算：传统卷积运算需要使用大量乘法，而大量乘法运算依赖于GPU等硬件设备，限制了在移动设备上的应用。为了避免乘法运算，一些模型中采用了新的卷积核，例如XNORnet采用了逻辑运算核，用逻辑计算代替乘法运算，Addernet采用了加法核，用加法运算代替乘法运算。

## 网络剪枝
标准的网络剪枝的过程一般是：
1）训练：训练模型得到最佳网络性能；
2）剪枝：根据剪枝算法对模型剪枝，调整网络结构中神经元、卷积核、通道或者网络层；
3）微调：在原数据集上进行微调，用于弥补模型因剪枝丢失的精度。

除了上述的标准剪枝过程之外，基于子模型采样的剪枝也表现出很好的剪枝效果。一次子模型采样过程为：
1）对训练好的原模型中可修剪的网络结构按照剪枝目标进行采样；
2）对采样出的网络结构进行剪枝，得到采样子模型并进行性能评估，选取最优的子模型进行微调得到最后的剪枝模型。

另外还有基于搜索的剪枝，它主要依靠强化学习或者神经网络结构搜索相关理论。给定剪枝目标之后，基于搜索的剪枝在网络结构中搜索较优的子结构。

按剪枝方法分类可分为结构化剪枝和非结构化剪枝，非结构化剪枝去除不重要的神经元，相应地，被剪除的神经元和其他神经元之间的连接在计算时会被忽略。由于剪枝后的模型通常很稀疏，并且破坏了原有模型的结构，所以这类方法被称为非结构化剪枝。非结构化剪枝能极大降低模型的参数量和理论计算量，但是现有硬件架构的计算方式无法对其进行加速，所以在实际运行速度上得不到提升，需要设计特定的硬件才可能加速。与非结构化剪枝相对应的是结构化剪枝，结构化剪枝通常以滤波器（卷积核）、通道或者整个网络层为基本单位进行剪枝。一个滤波器被剪枝，那么其前一个特征图和下一个特征图都会发生相应的变化，但是模型的结构却没有被破坏，仍然能够通过 GPU 或其他硬件来加速，因此这类方法被称之为结构化剪枝。

高精度的大模型在进行剪枝后，在内存占用相同情况下，仍比同体积的非稀疏模型有更高的精度。另外，在资源有限的情况下，剪枝是比较有效的模型压缩策略。然而，模型剪枝后可能会产生稀疏矩阵，在硬件稀疏矩阵存储方向可以继续优化。

## AI计算模式思考2
1、支持不同bit位数：对低比特量化相关的研究落地提供int8/int4甚至更低的精度；在M-bits和E-bits之间权衡Tradeoff。
2、利用硬件提供稀疏计算：硬件上减少零值的重复计算；减少网络模型对内存的需求，稀疏化网络模型结构。

## 参考文献
https://www.knime.com/blog/a-friendly-introduction-to-deep-neural-networks
https://machine-learning.paperspace.com/wiki/activation-function
https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/
https://zhuanlan.zhihu.com/p/619914824
https://blog.csdn.net/weixin_43424450/article/details/134352857
LeCun Y, Bottou L, Bengio Y, Haffner, P. [Gradient-based learning applied to document recognition.](https://doi.org/10.1109/5.726791) 
Krizhevsky A, Sutskever I, Hinton G E. [ImageNet classification with deep convolutional neural networks.](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
Szegedy C, Liu W, Jia Y, et al. [Going deeper with convolutions.](https://doi.org/10.1109/CVPR.2015.7298594)
Simonyan K, Zisserman A. [Very deep convolutional networks for large-scale image recognition.](http://arxiv.org/abs/1409.1556)
Kaiming H, Xiangyu Z, Shaoqing R, Jian S. [Deep Residual Learning for Image Recognition.](https://doi.org/10.1109/CVPR.2016.90)
Hanting C, Yunhe W, Chunjing X, Boxin S, Chao X, Qi T, Chang X. [AdderNet: Do We Really Need Multiplications in Deep Learning?](https://doi.org/10.1109/CVPR42600.2020.00154)

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=993183455&bvid=BV17x4y1T7Cn&cid=1046878675&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
