# AI计算模式(下)

## 一、轻量化网络模型设计
设计轻量化模型两个主要方法。第一种方法是根据实验与结果分析，人工调整模型结构，比如改变模型不同层的输出特征图尺寸和卷积方式。

第二种则是神经架构搜索（Neural Architecture Search，NAS），NAS是研究如何可以不借助于人工的人工调试的方式就可以达到自动化设计高性能深度神经网络架构。NAS本质上是一个优化问题，通过合适的优化算法，对NAS进行求解，最终自动化得到一个神经网络的结构。一个NAS算法由三个部分组成：搜索空间、搜索策略和评估策略。根据搜索空间的颗粒度不同，目前搜索空间可以大概分成以下四类：

1、Layer-based：关注网络层中的参数，比如卷积核。
2、Block-based：每个块中包含多个网络层，关注块里的参数，比如残差模块。
3、Cell-based：每个cell中堆叠多个块，关注块之间的连接。
4、Topology-based：关注的是基本单元之间的操作。

搜索策略可以采用强化学习、梯度下降、进化学习等方法。评估算法则需要进行大量的训练、评估、验证和性能比较，以确定最佳神经网络。然而，每个神经网络的全面训练都是资源密集型的，需要大量的时间和计算资源。

### 1、调整神经网络结构
#### 叠加小卷积核代替大卷积核
在VGG网络模型中，用小卷积核代替大卷积核来减少网络参数，假设大卷积核的大小为$7\times 7$，用$3\times 3$的小卷积核代替。使用大卷积核时，下一层网络中的神经元的感受野是$7\times 7$。为了在使用小卷积核时可以有等效的感受野，需要增加网络层，则最后一层对前一层的感受野是$3\times 3$，对再前面一层的感受野是$5\times 5$，对第一层的感受野是$7\times 7$。现在来对比两种方式所需要的参数量，使用大卷积核时，参数量为$7\times 7+1$，而使用小卷积核时参数量为$3\times (3\times 3+1)$。使用小卷积核代替大卷积核除了能减少参数量，还能增加网络深度。
![小卷积核代替大卷积核1](https://upload-images.jianshu.io/upload_images/26105747-52b63650f6475d22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在GoogLeNet模型的InceptionNetV3中，进一步减小卷积核的尺寸，将$3\times 3$的卷积核拆分成$1\times 3$卷积核和$3\times 1$卷积核的叠加。
![小卷积核代替大卷积核2](https://upload-images.jianshu.io/upload_images/26105747-10e42d27a9c7f9a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#### 减少通道数
在GoogLeNet的InceptionNetV2中，插入了使用$1\times 1$的卷积核的网络层用以减少模型参数量。假设输入特征图是有$M$个通道，输出特征图有$N$个通道，则需要的卷积核个数是$M\times N$，卷积核大小为$3\times 3$时，参数量为$(3\times 3+1)\times M\times N$。现在插入一个有$K$（$K<M$）个通道的中间层，这个中间层使用$1\times 1$的卷积核，参数量为$(1\times 1+1)\times M \times K+(3\times 3+1)\times K \times N$。
![使用1X1卷积核减少通道数](https://upload-images.jianshu.io/upload_images/26105747-195f0ab92dde7b72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#### 特征图重用
在ResNet中，引入了残差结构，让特征图隔层相加作为输入。ResNet的特征图之间的连接是稀疏连接，为了进一步重用特征图，DenseNet中采用密集连接，相对于ResNet减少了参数量和计算量。
![Dense模块](https://upload-images.jianshu.io/upload_images/26105747-9679f8655f73616e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


一张图片经过神经网络进行特征提取后，能够得到很多特征图，其中存在相似的特征图。因此在GhostNet中，作者提出对一张特征图进行变换得到相似的特征图，可以减少网络需要的参数量。作者设计了Ghost模块代替传统卷积计算，首先采用标准卷积得到少量特征图，然后对这部分特征图生成相似特征图，生成过程是对每个特征图分别进行线性操作，比如$3\times 3$和$5\times 5$线性核，这种线性核的计算量少于标准卷积。通过这样的方式让特征图成倍增加，然后将不同的特征图拼接到一起，组合成新的输出。由于计算量主要集中在标准卷积计算，Ghost模块中的计算量会大大降低。
![Ghost模块](https://upload-images.jianshu.io/upload_images/26105747-37151d33382a8a22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## AI计算模式思考3
1、卷积核设计方面：通过叠加小卷积核代替大卷积核降低参数量，使用$1\times 1$卷积核降低参数量；采用多尺寸卷积核而不是单一尺寸，在网络同一深度上提取不同特征；从使用固定形状卷积核转向使用可变形卷积核。
2、通道方面：使用DW卷积代替标准卷积；使用分组卷积；通道加权计算。
3、卷积层连接方面：使用残差结构构建更深层网络；使用密集连接，降低网络参数量，重用特征



## 二、大模型分布式并行
随着神经网络模型变得越来越大，存储参数所需的空间越来越大，在单机单卡上训练大模型是不可能的，如何分布式并行训练大模型，成为一个研究热点。

### 1、DP、DDP和FSDP
数据并行（Data Parallelism，DP）的基本过程是：多块 GPU分别加载一个完整的模型，更新模型参数时把一个batch拆分成多个micro-batch，分别发给这些GPU，每个 GPU 做完计算后，得到一份梯度，最后把所有梯度上传给一个服务器进行累加，用于更新模型参数。分布式数据并行（Distributed Data Parallel，DDP）与DP不同的地方在于每块GPU做完计算后，通过Ring-AllReduce 的方法通信完成参数更新。DP 中有一个最明显的问题就是通过单一 server 节点进行梯度聚合，此时 server 的带宽就会称为瓶颈 。于是 DDP 把通讯方式改成了 Ring-AllReduce，解决了通讯负载不均衡的问题。

**Ring-AllReduce**
如下图所示，all-reduce是一种将跨设备的同一行的值求和，并将结果值返回到相应行的操作。
![all-reduce](https://upload-images.jianshu.io/upload_images/26105747-915b965f53eb2b59.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
all-reduce的操作可以分为两步，第一步是reduce-scatter，每个设备完成一部分的求和，第二步是all-gather，将各部分的求和发送到所有其它设备。
![all-reduce operation](https://upload-images.jianshu.io/upload_images/26105747-d370ab0f4bb33af6.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在reduce-scatter阶段，我们假设有p个设备(图例中p=4)，矩阵的大小为V，将矩阵分为p块，那么经过reduce-scatter运算后，每个设备将接收到一个大小为V/p的数据块。如果设备之间的通信是双工的，其带宽为β，则在采用环通信时，每个设备的输入/输出带宽都可以同时达到β，同一时刻下所有设备的输入/输出带宽之和也等于p×β，即所有设备的带宽都得到充分利用。总共有p个设备，每个设备上的数据被分成p个部分，因此基于环通信的reduce-scatter操作必须执行p-1个步骤。经过p-1步后，每个设备在矩阵对应的位置上完成了求和。在整个过程中，每个设备发送和接收的数据量为(p-1)V/p，输出或输入带宽为β，因此该过程所需的时间为(p-1)V/pβ。如果p足够大，完成时间将接近V/β，完成时间与设备数量p无关！当然，所有设备之间传输的数据量是(p-1)V，它与设备数量p成正比。
![reduce-scatter](https://upload-images.jianshu.io/upload_images/26105747-958dfe1cbed1a797.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

all-gather阶段的通信过程与reduce-scatter阶段类似，只是这个阶段不需要进行求和操作，只需要完成通信并更新参数。所有设备之间传输的数据量是(p-1)V，该过程所需的时间为(p-1)V/pβ。
![all-gather](https://upload-images.jianshu.io/upload_images/26105747-374a9395b0d987e7.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对Ring-AllReduce算法进行分析：在V保持不变的情况下，增加p，设备的数量(我们称p为集体通信的并行宽度)，所有设备之间的通信流量将成比例地增加，所有设备的冗余内存（不同的设备存储了一些相同的数据）也将成比例地增加。当然，完成某种集体交流所需的时间与平行宽度p几乎无关。所以，增加平行宽度p是一把双刃剑。一方面，它使每个设备处理更少的数据，即V/p，从而缩短了计算时间。但另一方面，它占用了更多的通信带宽(p-1)V和更多的内存空间(p-1)V。

FSDP（Fully Sharded Data Parallel） 是在 DDP的基础上提出的。在 DDP 中，核心的能力还是训练数据并行（Data Parallel），并没有实现对模型参数的分片管理，即模型并行（Model Parallel）。在 FSDP 中实现了模型的分片管理能力，真正实现了模型并行。将模型分片后，在使用 FSDP 训练模型时，每个 GPU 只保存模型的一个分片，这样能够使 GPU 的内存占用比 DDP 方式小得多，从而使分片的大模型和数据能够适配 GPU 容量，更有希望实现超大模型的分布式训练。FSDP的过程大致如下：初始阶段，首先对模型参数分片，并且GPU只持有它自己的分片；随后运行all_gather算法，收集所有GPU上的模型参数分片，得到完整模型并完成forward计算，计算完成后丢掉所有被收集过的其它GPU上的模型参数分片；再次运行 all_gather，收集所有GPU上的模型参数分片，得到完整模型运行backward计算，接着运行reduce_scatter，所有GPU同步梯度更新，最后丢掉所有从其它GPU上收集过的模型参数分片。

### 2、Megatron-LM
NVIDIA Megatron-LM 是一个基于 PyTorch 的分布式训练框架，用来训练基于Transformer的大型语言模型。Megatron-LM 综合应用了数据并行（Data Parallelism），张量并行（Tensor Parallelism）和流水线并行（Pipeline Parallelism）。其中流水线并行是将模型的不同层放置在不同的设备上。例如，前几层放在一个设备上，中间几层放在另 一个设备上，最后几层放在第三个设备上。这种方式可以在不同设备上并行执行不同的模型阶段，从而提高效率。张量并行则是对模型的层内部进行分割，将某一层的计算分配到不同的设备上。这也可以被理解为将大矩阵运算拆分成多个小矩阵运算，然后分布到不同的设备上进行计算。

#### 张量并行
将神经网络中的计算抽象为矩阵乘法，比如$XA=Y$。行并行是把$A$按行分割，同时把$X$按列分割，则
$$XA=[X_1\ X_2]\Big[ {A_1 \atop A_2}\Big] = X_1A_1+X_2A_2=Y_1+Y_2=Y$$
将上式的$A_1,X_1$和$A_2,X_2$分别发送给两个GPU，同时完成乘法运算$X_1A_1$和$X_2A_2$，最后运算结果求和。
![行并行](https://upload-images.jianshu.io/upload_images/26105747-751eba0fa0d4ca87.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

列并行则是将$A$按列分割，$A=[A_1\ A_2]$，两个GPU分别得到$A_1$和$A_2$，同时它们都存储$X$，分别完成运算$XA_1$和$XA_2$。
![列并行](https://upload-images.jianshu.io/upload_images/26105747-e41717abddec1a72.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在Transformer中，包括两种主要的网络层，一种是前馈层，一种是多头自注意力层。

对前馈层的运算分割，假设输入矩阵是$X$，那么对权重矩阵$A$只能采用列并行，因为矩阵乘法结果需要进行非线性变换（GeLU激活），而$GeLU(X_1A_1+X_2A_2)\neq GeLU(X_1A_1)+GeLU(X_2A_2)$，所以这里的矩阵乘法运算不能采用行并行而是采用列并行。列并行的结果为$[Y_1\ Y_2]=[GeLU(XA_1)\ GeLU(XA_2)]$。GeLU操作后还有Dropout的操作，这里对权重矩阵$B$进行行分割，完成行并行运算。
![前馈层分割](https://upload-images.jianshu.io/upload_images/26105747-815e067206705618.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于多头自注意力层，首先，多头注意力机制本身具有并行性，以列并行方式对与键（K）、查询（Q）和值（V）相关联的矩阵进行分区，从而在一个GPU上本地完成与每个注意力头对应的矩阵乘法。其次，对于后续的全连接层，权重矩阵$B$就按行切分。
![多头自注意力层分割](https://upload-images.jianshu.io/upload_images/26105747-a53447d34e8f6ee8.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#### 流水线并行
目前主流的流水线并行方法包括了两种：Gpipe和PipeDream。与这两者相比，Megatron中的流水线并行实现略有不同，它采用了Virtual Pipeline的方法。简而言之，传统的流水线并行通常会在一个设备上放置几个模块，通过在计算强度和通信强度之间取得平衡来提高效率。然而，虚拟流水线则采取相反的策略。在设备数量不变的前提下，它将流水线阶段进一步细分，以承载更多的通信量，从而降低空闲时间的比率，以缩短每个步骤的执行时间。

G-pipe将Transformer按层切分放到多个device上，forward计算和backward计算采用流水线的方式进行，这样做的劣势是空泡率（下图流水线中设备的空闲时间）较高。下图中是G-pipe的方法演示，蓝色块1-8编号表示8个micro-batch的前馈计算，绿色是反向传播，device1处输入输入数据计算得到中间结果，把计算结果交给device2进行网络后续的前馈计算，device2完成后又交给device3，最后device4得到模型最终的输出结果，再进行反向传播。
![G-pipe](https://upload-images.jianshu.io/upload_images/26105747-cb1038e41fe95a12.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

PipeDream的方案如下图所示，其相对与G-pipe的改进在内存方面， 空泡时间和G-pipe一致，但通过合理安排前向和反向过程的顺序，减少了设备需要保存的激活值，每个device上最少只需要保存1份micro-batch的激活值，最多需要保存4份激活值。可以看到，激活值份数的上限从micro-batch数量8变成了pipeline stage阶段4，这样就可以通过增大micro-batch数量有效降低空泡占比。
![PipeDream](https://upload-images.jianshu.io/upload_images/26105747-64f4b7d54350d9eb.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Virtual Pipeline在 device 数量不变的情况下，分出更多的 pipeline stage，以更多的通信量，换取空泡比率降低。Virtual Pipeline是怎么做到的呢？对照示例，若模型被分为16层（编号0-15），4个device，G-pipe和PipeDream是把模型分成4段，按编号0-3层放device1，4-7层放device2，以此类推。Virtual Pipeline则是按照virtual_pipeline_stage的概念减小切分粒度，以virtaul_pipeline_stage=2为例，将0-1层放在device1, 2-3层放在device2，4-5层放在device3，6-7层放到 device4，8-9层继续放在device1，10-11层放在device2，12-13层放在device3，14-15层放在device4。按照这种方式，设备之间的点对点通信次数和通信量直接翻了virtual_pipeline_stage倍，但空泡比率降低了。下图是Virtual Pipeline的演示，蓝色表示模型的0-7层的forward运算，浅蓝色表示模型的8-15层的forward运算，浅绿色表示8-15层的backward运算，深绿色表示0-7层的backward运算，编号是micro-batch的编号。
![Virtual Pipeline](https://upload-images.jianshu.io/upload_images/26105747-3b076be1669aacba.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## AI计算模式思考
1、支持神经网络模型：支持高维张量的存储和计算；支持不同神经网络的计算逻辑
2、模型压缩：为量化模型提供不同bit位数；为剪枝模型利用硬件提供稀疏计算
3、轻量化网络模型：降低卷积计算的复杂度；复用卷积核内存信息
4、大模型分布式并行：增加内存容量、加速互联带宽；提供专用大模型DSA IP模块，提供低比特快速计算

## 参考文献
https://zhuanlan.zhihu.com/p/650383289
https://oneflow2020.medium.com/how-to-derive-ring-all-reduces-mathematical-property-step-by-step-9951500db96
https://zhuanlan.zhihu.com/p/432969288

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?aid=865754482&bvid=BV1754y1M78X&cid=1049051877&page=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
