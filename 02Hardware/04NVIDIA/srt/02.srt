1
00:00:00,000 --> 00:00:04,300
Subtitle：PlusV98

2
00:00:04,750 --> 00:00:07,000
大家好,我是 ZOMI

3
00:00:07,000 --> 00:00:09,312
现在来到了 Tensor Core 的第二弹

4
00:00:09,312 --> 00:00:12,025
去看看 Tensor Core 的架构演进

5
00:00:12,025 --> 00:00:15,584
其实呢,在整个英伟达的 GPU 架构里面呢

6
00:00:15,584 --> 00:00:18,212
讲了非常多简单的概念

7
00:00:18,212 --> 00:00:20,192
特别是从 Turing 到 Hopper 架构呢

8
00:00:20,192 --> 00:00:23,159
整个 Tensor Core 呢是发展了非常多代

9
00:00:23,159 --> 00:00:23,393
那现在呢

10
00:00:23,393 --> 00:00:26,043
来到了整个 Tensor Core 的架构的演进

11
00:00:26,043 --> 00:00:30,401
去看看每一代不同的英伟达的 GPU 的架构

12
00:00:30,401 --> 00:00:34,000
现在从 Volta 架构到 Turing,Ampere,Hopper 架构

13
00:00:34,000 --> 00:00:35,900
去看看每一代的 Tensor Core 都

14
00:00:35,900 --> 00:00:37,732
有哪些不一样的点

15
00:00:39,400 --> 00:00:42,379
回顾一下,在第一代 Volta 架构

16
00:00:42,379 --> 00:00:43,492
里面的 Tensor Core 呢

17
00:00:43,492 --> 00:00:45,739
主要是 4x4 的矩阵

18
00:00:45,739 --> 00:00:49,699
输入是 Fp16 的 A 和 B 两个矩阵

19
00:00:49,699 --> 00:00:52,534
然后加上 Fp32 和 Fp16 的矩阵之后呢

20
00:00:52,534 --> 00:00:55,243
得到 Fp16 和 Fp32 的矩阵

21
00:00:55,243 --> 00:00:58,296
通过这种方式呢进行混合精度的计算

22
00:00:58,296 --> 00:01:00,619
真正线程执行的时候呢

23
00:01:00,619 --> 00:01:03,542
是把 A 矩阵的一行乘上 B 矩阵的一列

24
00:01:03,542 --> 00:01:05,654
再加上 C 矩阵的一个元素

25
00:01:05,654 --> 00:01:08,214
得到 D 矩阵里面的其中一个元素

26
00:01:08,214 --> 00:01:11,947
这个呢就是 Tensor Core 里面的 FMA 指令

27
00:01:11,947 --> 00:01:13,675
执行的具体的内

28
00:01:15,025 --> 00:01:18,431
现在来看看今天的主要的概念

29
00:01:18,431 --> 00:01:20,765
Tensor Core 的历代的发展

30
00:01:20,765 --> 00:01:22,030
一共呢经历了 4 代

31
00:01:22,030 --> 00:01:24,099
看一下每一代有哪些不一样

32
00:01:24,974 --> 00:01:27,545
首先呢 Tensor Core 是从 Volta 架构开始的

33
00:01:27,545 --> 00:01:29,134
当初呢只是 Fp32

34
00:01:29,134 --> 00:01:31,449
直到 Turing 架构呢,它支持的更多

35
00:01:31,449 --> 00:01:33,166
到现在的 H100 呢

36
00:01:33,166 --> 00:01:35,481
它基本上的支持格式呢就更多了

37
00:01:35,481 --> 00:01:37,721
每一代的支持格式也会越来越多

38
00:01:37,721 --> 00:01:40,942
可以预见下一代英伟达的 GPU 的产品呢

39
00:01:40,942 --> 00:01:43,545
也可能支持更多精度的指令

40
00:01:46,500 --> 00:01:50,158
现在呢来到了第一代英伟达的架构

41
00:01:50,158 --> 00:01:51,936
Volta 架构里面的 SM

42
00:01:51,936 --> 00:01:54,992
左边的这个就是 SM 的原来的架构图

43
00:01:54,992 --> 00:01:58,614
SM 里面呢有 4 个子核,叫做 Sub Core

44
00:01:58,614 --> 00:02:01,102
那左边的这一块呢就是一个

45
00:02:01,102 --> 00:02:02,926
右边的这一块呢又是一个

46
00:02:02,926 --> 00:02:04,314
一共呢有 4 个子核

47
00:02:04,314 --> 00:02:07,611
现在后面都会以右边的这个形式去呈现

48
00:02:07,611 --> 00:02:12,913
首先呢一个 SM 里面有 L1 的 I $,L1 的指令缓存

49
00:02:12,913 --> 00:02:16,945
对应呢有 4 个子核 Sub Core,1234

50
00:02:17,027 --> 00:02:20,552
而 Sub Core 下面呢又有对应的 L1 的缓存

51
00:02:20,552 --> 00:02:23,277
还有 Share Memory,就是共享内存

52
00:02:23,277 --> 00:02:26,752
值得注意的就是看看下面的箭头

53
00:02:26,752 --> 00:02:28,337
首先呢 L1 Cache 里面呢

54
00:02:28,337 --> 00:02:30,692
会把具体的指令呢发到 Sub Core

55
00:02:30,692 --> 00:02:34,242
这里面的指令呢或者这里面的箭头呢是单向的

56
00:02:34,242 --> 00:02:37,008
Sub Core 计算完之后呢,它的箭头呢是双向的

57
00:02:37,008 --> 00:02:40,648
也就是可以获取计算数据或者对数据呢

58
00:02:40,648 --> 00:02:42,000
进行重复的计算

59
00:02:43,145 --> 00:02:47,291
现在呢重新的去解析一下第一代的 Tensor Core

60
00:02:47,291 --> 00:02:48,336
也就是 Volta 架构的 Tensor Core

61
00:02:48,336 --> 00:02:52,043
首先呢 SM 呢它是主要负责对寄存器里面的整体

62
00:02:52,043 --> 00:02:54,992
逻辑进行读和写和计算的

63
00:02:55,108 --> 00:02:56,939
而这里面的每一个 Sub Core 呢

64
00:02:56,939 --> 00:03:00,755
就包括了 Tensor Core,IP64,FP32,还有 INT8

65
00:03:00,755 --> 00:03:03,670
当然还有特殊的处理单元 MFU

66
00:03:03,670 --> 00:03:06,539
所以 Sub Core 呢不仅是指 Tensor Core

67
00:03:06,539 --> 00:03:09,021
也不仅仅是指 CUDA Core

68
00:03:09,021 --> 00:03:12,299
Cuda Core/Tensor Core/RT Core

69
00:03:12,299 --> 00:03:14,114
都包在 Sub Core 里面

70
00:03:14,414 --> 00:03:16,260
在每一个 Sub Core 的上面呢

71
00:03:16,260 --> 00:03:18,543
就是我鼠标的所在的这一块位置呢

72
00:03:18,600 --> 00:03:20,388
其实还有一个 Warp Schedule

73
00:03:20,388 --> 00:03:23,826
专门针对现成的 Warp 进行调度的

74
00:03:23,826 --> 00:03:27,308
数据呢就存储在 L1 或者 L0 的 Cache 里面

75
00:03:28,233 --> 00:03:31,170
视频里谈到了针对第一代架构的 Tensor Core 呢

76
00:03:31,170 --> 00:03:35,429
每一个 Sub Core 都有一个 4x4x4 的 Tensor Core

77
00:03:35,429 --> 00:03:39,480
而 Warp Schedule 呢向 Tensor Core 发送具体的矩阵乘法

78
00:03:39,480 --> 00:03:42,050
也就是 GML,WAM 的运算指令

79
00:03:42,050 --> 00:03:44,824
计算完之后呢,就会从寄存器

80
00:03:44,824 --> 00:03:46,648
也就是中间的这个位置

81
00:03:46,648 --> 00:03:50,261
去读取或者去接收 ABC 矩阵

82
00:03:50,261 --> 00:03:52,216
A 矩阵和 B 矩阵呢是 FP16

83
00:03:52,216 --> 00:03:54,520
C 矩阵呢就是 FP32 或者 FP16

84
00:03:54,520 --> 00:03:57,388
执行多次的 4x4 的矩阵乘法

85
00:03:57,388 --> 00:03:59,512
直到完成整个矩阵运算之后呢

86
00:03:59,512 --> 00:04:02,396
将所得的的矩阵呢写回去寄存器

87
00:04:02,396 --> 00:04:03,910
也就是 Register File

88
00:04:03,910 --> 00:04:06,446
或者 Share Memory 里面

89
00:04:07,350 --> 00:04:09,556
接下来对左边的这个 SM 图呢

90
00:04:09,556 --> 00:04:12,625
进行再打开看一下 SM 的微架构

91
00:04:12,625 --> 00:04:14,644
首先呢也是从上往下看

92
00:04:14,644 --> 00:04:18,000
上面就是一个共享的 L1 缓存

93
00:04:18,000 --> 00:04:19,636
每个时钟周期呢可以

94
00:04:19,636 --> 00:04:21,451
执行 4 个 WarpInstruction

95
00:04:21,451 --> 00:04:22,950
下属 4 个 Sub Core 是独立的

96
00:04:22,950 --> 00:04:25,195
里面的数据呢是不进行缓存的

97
00:04:25,195 --> 00:04:27,509
但 Sub Core 里面的有两个 Tensor Core 嘛

98
00:04:27,509 --> 00:04:29,899
两个 Tensor Core 的数据呢是

99
00:04:29,899 --> 00:04:32,366
可以共享的,再往下呢有一个共享内存

100
00:04:32,366 --> 00:04:34,411
共享内存每个时钟周期呢可以执行

101
00:04:34,411 --> 00:04:37,920
或者可以传输 128B 的数据

102
00:04:37,920 --> 00:04:40,391
当 SMEM 计算完这个全数矩阵之后呢

103
00:04:40,391 --> 00:04:43,723
就会把数据呢回传到 L2 的 Cache 里面

104
00:04:43,723 --> 00:04:46,041
最后呢就返回到 host CPU

105
00:04:47,239 --> 00:04:50,178
现在继续展开一下刚才的一个 Sub Core 里

106
00:04:50,178 --> 00:04:52,725
面的微架构,Sub Core 里面的微架构呢

107
00:04:52,725 --> 00:04:54,023
就很多的内容了,刚才

108
00:04:54,023 --> 00:04:56,231
看到的 L1 Cache 呢是在上面

109
00:04:56,231 --> 00:04:57,458
L1 Cache 里面呢

110
00:04:57,458 --> 00:05:02,042
具体的执行的就会到 L0 Cache 或者叫 Register File

111
00:05:02,042 --> 00:05:04,583
把一些数据呢传输到这

112
00:05:04,583 --> 00:05:07,762
具体的指令的分发呢是通过 Warp Schedule 的

113
00:05:07,762 --> 00:05:10,642
针对的计算呢通过 Math Dispatch Unit

114
00:05:10,642 --> 00:05:11,560
分发到具体的

115
00:05:11,560 --> 00:05:14,584
FP64、BP32、FP32 还有 MMU

116
00:05:14,584 --> 00:05:16,588
这几个具体的执行单元器计算

117
00:05:16,588 --> 00:05:19,192
但是呢如果调用的是 WMMA

118
00:05:19,192 --> 00:05:21,480
相关的 API 或者相关的指令呢

119
00:05:21,480 --> 00:05:23,688
Warp Schedule 呢就直接去触发或者去执行

120
00:05:23,688 --> 00:05:25,289
Tensor Core 里面的计算

121
00:05:25,312 --> 00:05:27,432
Tensor Core 里面呢就有两个 4x4x4

122
00:05:27,432 --> 00:05:30,537
的 Tensor 去每个时钟周期去执行

123
00:05:30,537 --> 00:05:35,000
最后呢就把数据呢回存到 Register File,也就是寄存器里面

124
00:05:35,800 --> 00:05:40,800
寄存器再通过 MIO 的 Data Pipe 呢跟 Shared Memory 进行通讯

125
00:05:42,425 --> 00:05:44,420
这里面有大量的数据传输

126
00:05:44,420 --> 00:05:48,516
数据呢存在哪里非常的关键,于是呢现在打开一下

127
00:05:48,516 --> 00:05:52,516
看一下 L1 的缓存还有共享的内存之间的一个关系

128
00:05:53,325 --> 00:05:57,312
在 Volta 架构里面呢对比起 P100 呢有一个很大的改进点

129
00:05:57,312 --> 00:06:02,022
就是把 L1 的缓存呢跟共享内存的合并成为同一块空间

130
00:06:02,022 --> 00:06:08,486
共享内存的 SMEM 呢可以为整个 SM 呢提供高达 96KB 的存储空间

131
00:06:08,486 --> 00:06:14,693
针对 L2 也就是对应的 Ever Cast 呢也进行了更新,已经有了一个 5%到 15%的提升

132
00:06:14,693 --> 00:06:21,338
Volta 架构里面的 Sub Core 也就 V 架构里面呢单独提供了一个 Tensor Core 的指令

133
00:06:21,338 --> 00:06:27,321
提供给 Warp Schedule,Warp Schedule 呢直接去执行,不需要通过 Mesh Dispatch Unit 去进行分发

134
00:06:27,321 --> 00:06:33,321
除了 Tensor Core 是专门针对 AI 框的矩阵进行计算之外呢,Volta 架构还减少了指令的延迟

135
00:06:34,871 --> 00:06:39,296
现在来到了 Tensor Core 的第二代,去看一下 Turing 架构里面的 Tensor Core

136
00:06:39,296 --> 00:06:42,875
那首先呢 SM 就不管了,直接打开 Sub Core

137
00:06:42,875 --> 00:06:45,232
就是 V 核 在 Turing 架构里面呢

138
00:06:45,232 --> 00:06:50,992
tensor core 除了原先的 FP16 呢,其实还增加了 INT8 和 INT4 多种类型

139
00:06:50,992 --> 00:06:56,400
另外的话还是 FP16 的 FastPath 每个时钟周期呢,可以执行 32 次

140
00:06:56,400 --> 00:07:01,400
而原来的 IP 到 8 个时钟周期内呢,可以执行单个多线程 GEM 的计算

141
00:07:01,400 --> 00:07:05,200
也就是计算频率或者计算计算吞吐就更高了

142
00:07:06,000 --> 00:07:11,800
第二代的 Turing 架构提出了,其实距离上一代的 Volta 架构呢只是距离了一年

143
00:07:11,800 --> 00:07:15,952
那现在看看第三代的 Tensor Core 有哪些巨大的改变

144
00:07:15,952 --> 00:07:22,373
首先呢,现在呢去澄清或者去给大家汇报的一个内容,就是多级的缓存或者多级数据的带宽

145
00:07:22,373 --> 00:07:30,373
首先呢,看到的就是 NVLink,NVLink 呢是针对单节点多卡之间进行数据互联的

146
00:07:30,373 --> 00:07:37,373
再往上,L2 和 DRAM 呢,就是针对每一款每一块 GPU 卡里面的系统内存

147
00:07:37,373 --> 00:07:41,550
再往上呢,就是每一个 SM 里面的内存,首先有共享内存呢,有 IL 了

148
00:07:41,550 --> 00:07:44,100
针对具体的计算的就是 Math 了

149
00:07:44,100 --> 00:07:47,908
包括 Tensor Core 或者 CUDA Core 都是取决于 Math

150
00:07:47,908 --> 00:07:53,725
而真正的 A100 呢,它最重要的改变就是 Movement Efficiently

151
00:07:53,725 --> 00:07:57,175
就是数据搬运更加的快,有了一个三倍的提升

152
00:07:57,175 --> 00:07:59,176
现在看看它具体是怎么做的

153
00:08:00,301 --> 00:08:03,875
首先呢,看到 NPA 架构之前呢,包括 Turing 架构,Volta 架构

154
00:08:03,875 --> 00:08:06,500
如果或者如果 Tensor Core 呢,想要使用共享内存

155
00:08:06,500 --> 00:08:12,592
就必须要把数据呢,从全局内存里面去加载到寄存器,也就是 Register File

156
00:08:12,592 --> 00:08:17,621
然后再写进去共享内存,整体来说是数据要搬来搬去

157
00:08:17,621 --> 00:08:21,573
除了增加数据的搬运呢,其实还影响了时延

158
00:08:21,573 --> 00:08:26,149
于是呢,NPA 架构呢,就提出了提供一个异步的内存拷贝机制

159
00:08:26,149 --> 00:08:31,141
通过一个具体的新的指令,叫做 LDGSTS

160
00:08:31,141 --> 00:08:34,289
这个呢,指令叫做 Load Global Storage Shared

161
00:08:34,289 --> 00:08:42,850
实现了全局内存不需要经过具体的寄存器,直接加载到共享内存里面

162
00:08:42,850 --> 00:08:44,420
那这个呢,看看具体怎么做

163
00:08:44,927 --> 00:08:49,875
首先,V100 原来的操作方式,如果想要使用共享内存呢

164
00:08:49,875 --> 00:08:53,248
就需要从 LU 把共享内存的搬到寄存器堆里边

165
00:08:53,248 --> 00:08:54,433
也就是 Register File 里面呢,然后再给内存呢

166
00:08:54,433 --> 00:08:58,144
就会搬到 F 里面,具体给 Tensor Core 去执行

167
00:08:58,144 --> 00:09:00,547
那上面就是 Tensor Core

168
00:09:00,547 --> 00:09:03,555
每一次数据搬运都会非常的占用时延

169
00:09:03,555 --> 00:09:08,188
在 A100 里面呢,A100 里面呢,就提出了一个软件的 Sync Copy

170
00:09:08,188 --> 00:09:13,350
异步的拷贝机制,通过新的指令,可以把 L2 Cache 里面的全局内存

171
00:09:13,350 --> 00:09:18,950
直接搬到 SMEM 共享内存里面,然后给 Register File 呢,直接的去执行

172
00:09:18,950 --> 00:09:21,850
每次数据搬运都会增加时延、功耗

173
00:09:22,525 --> 00:09:26,964
大家有没有注意到,A100 跟 V100,除了中间的这个传输之外呢

174
00:09:26,964 --> 00:09:33,108
其实有一点呢,漏掉了,就是在 V100 里面呢,需要 4 个读取的数据给到 Warp

175
00:09:33,108 --> 00:09:34,647
Warp 只需要读取两次

176
00:09:34,647 --> 00:09:38,790
这里面呢,又有另外一个技术,就是 Ampere 架构的 Tensor Core

177
00:09:38,790 --> 00:09:42,211
就是一个 Warp 呢,就提供了 32 个线程,32 个线程可以共享数据

178
00:09:42,211 --> 00:09:45,795
而 Volta 架构里面呢,整个 Tensor Core 只有 8 个线程

179
00:09:45,795 --> 00:09:50,300
这样的好处 可以在线程之间呢,减少矩阵的数据搬运

180
00:09:50,300 --> 00:09:51,329
因此呢,数据搬运的次数呢

181
00:09:51,329 --> 00:09:54,000
会比 V100 更少

182
00:09:54,000 --> 00:10:02,315
看一下具体的 FFMA,也就是 Fuse Fold MathMath and Add,就是矩阵乘加的操作

183
00:10:02,315 --> 00:10:05,045
解读一下上面的这个图啊

184
00:10:05,045 --> 00:10:09,150
绿色的这些小块呢,叫做 Support,就是刚才提到的 Support

185
00:10:09,150 --> 00:10:15,413
包括这里面的图呢,就是 Tensor Core,而连续的蓝色的框呢,就表示寄存器 Register

186
00:10:15,413 --> 00:10:18,155
当寄存器纯粹使用 CUDA Core 的时候呢

187
00:10:18,155 --> 00:10:24,278
会把所有的数据呢,都放在 Register 里面,每个 Register 呢,针对一个 CUDA Core 的数据呢,进行传输

188
00:10:24,278 --> 00:10:26,774
所以使用 CUDA Core 呢,是算得非常慢的

189
00:10:26,774 --> 00:10:29,552
在 V100 里面呢,每个 Tensor Core 呢,可以处理 8 个线程

190
00:10:29,552 --> 00:10:31,052
每个线程都有自己的寄存器

191
00:10:31,052 --> 00:10:35,235
所以在 8 个时钟周期内呢,可以执行 1024 个 MAC 的操作

192
00:10:35,235 --> 00:10:40,304
那下面呢,就是 A100 的 TC,就是 Tensor Core 的指令啊

193
00:10:40,304 --> 00:10:44,307
可以看到,每个 Tensor Core 呢,可以 32 条线程,现在 32 条线程

194
00:10:44,307 --> 00:10:51,000
因此呢,可以在 8 个时钟周期内呢,去寄存 2048 个 MAC,每个时钟周期处理其中一块的数据

195
00:10:51,675 --> 00:10:55,727
第三代的 Tensor CoreAmpere 架构里面呢,除了制造工艺提升之外呢

196
00:10:55,727 --> 00:11:01,809
还提供了更多的线程使得硬件执行更快,数据搬运的更少,每个时钟的吞吐呢,更大

197
00:11:03,450 --> 00:11:08,337
大家看完这个图之后呢,有没有一点感觉

198
00:11:08,337 --> 00:11:14,353
就是为什么出现了 Tensor Core 之后呢,会比单纯的使用 CUDA Core 执行的更快呢?

199
00:11:14,353 --> 00:11:16,319
针对矩阵计算

200
00:11:16,319 --> 00:11:18,401
具体呢,是因为线程啊,每一次针对 Tensor Core

201
00:11:18,401 --> 00:11:22,526
都可以处理的更快,处理的更多,吞吐是不一样的

202
00:11:24,900 --> 00:11:28,664
现在来到了 2022 年提出的 Hopper 架构

203
00:11:28,664 --> 00:11:33,750
里面呢,就提出了第 4 代 Tensor Core,第 4 代 Tensor Core 其实呢,有三个重要的改变的点

204
00:11:33,750 --> 00:11:38,459
现在来回顾一下,其实前三代的 Tensor Core 呢,都是基于 Warp-Level 进行编程的

205
00:11:38,459 --> 00:11:45,874
那英伟达架构 A100 里面呢,做了软件的异步加载之后呢,其实它还是基于 Web-Level 进行编程

206
00:11:45,874 --> 00:11:51,127
简单地来说呢,就是把数据呢,从 HBM,就是全局内存,加载到寄存器

207
00:11:51,127 --> 00:11:54,766
再通过 Warp Scheduleh 去调用,后来完成整个矩阵的运算

208
00:11:54,766 --> 00:11:58,862
最后再把数据回传到寄存器,再不断地去运算

209
00:11:58,862 --> 00:12:01,256
这整一个过程呢,它其实有两个问题

210
00:12:01,256 --> 00:12:06,862
那第一个问题呢,就是数据的搬运和计算是严重地去偶合的

211
00:12:06,862 --> 00:12:11,048
线程加载矩阵数据的时候呢,其实会独立地去获取矩阵的地址

212
00:12:11,048 --> 00:12:14,505
会非常消耗继承器的数量还有存储的带宽

213
00:12:14,505 --> 00:12:19,435
第二个就是可扩展性受到约束,因为单个 Warp 里面的线程是有限的

214
00:12:19,435 --> 00:12:24,235
而单个 Warp 里面的线程有限呢,就会导致矩阵的计算的规格受到约束

215
00:12:24,235 --> 00:12:26,180
于是呢,在第 4 代 Tensor Core 里面呢

216
00:12:26,180 --> 00:12:34,180
就提出了一个 TMA,叫做 Tensor Memory Accelerator,就是增量内存加速的功能

217
00:12:34,180 --> 00:12:41,700
左边这个图呢,就是 A100,就是上一代的安培架构里面的一个整体的 SM 的架构图

218
00:12:41,700 --> 00:12:44,356
右边呢,就是 H100 里面的 SM 的架构图

219
00:12:44,356 --> 00:12:48,610
可以看到基本上没有太多的差别,除了因为工艺制程的原因呢

220
00:12:48,610 --> 00:12:51,746
导致这里面的 CUDA Core 和 Tensor Core 的密度更高之外呢

221
00:12:51,746 --> 00:12:55,522
最重要的就是多了一个 Tensor Memory Accelerator

222
00:12:55,522 --> 00:12:59,198
简称叫做 TMA,也就是硬件的数据异步加载

223
00:12:59,198 --> 00:13:06,580
在 A100 里面呢,其实已经提出了一个软件的数据异步加载,这里面呢,TMA 直接把它硬件化了

224
00:13:06,580 --> 00:13:11,188
非常方便把全局内存的数据呢,异步加载到共享内存

225
00:13:11,188 --> 00:13:14,580
直接给 Register File 寄存器去读写

226
00:13:15,200 --> 00:13:20,185
H100 之前的架构里面呢,大部分都是从左边的图所示啊

227
00:13:20,185 --> 00:13:23,897
只有 Grid 和 Block 之分,线程是没有办法控制的

228
00:13:23,897 --> 00:13:28,692
所以呢,因此呢,针对硬件呢,分别对应的是一个 SM,SM 对应的是 Block

229
00:13:28,692 --> 00:13:33,044
而 Grid 呢,是对应整个 Devices,也就是单块 GPU

230
00:13:33,044 --> 00:13:38,420
局部的数据呢,只能通过 Shared Memory 在 SM 内进行共享

231
00:13:38,420 --> 00:13:41,172
跨 SM 之间呢,是不能够处理的

232
00:13:41,172 --> 00:13:48,556
而 Hopper 架构呢,直接在硬件上面呢,引入了一个交叉互联网络,也就是直接把数据拓展到 4 个 SM

233
00:13:48,556 --> 00:13:50,924
SM 之间呢,可以互相通讯

234
00:13:51,250 --> 00:13:58,250
于是在软件上或者 CUDA 上面呢,引用了一个新的概念,叫做 TBC,也就是把 4 个 SM 聚合起来

235
00:13:58,250 --> 00:14:06,250
SM 跟 SM 之间呢,可以高效访问他们互相之间的内存,所以这种呢,叫做分布式共享内存

236
00:14:06,250 --> 00:14:13,411
另外的话,你既然硬件有改变,所以软件有改变,软件呢,就提出了 Warp group 这种编程的模式

237
00:14:13,411 --> 00:14:17,411
对应的就引入了刚才说到的 Flatbox Cluster 这个概念

238
00:14:18,325 --> 00:14:22,014
更直观的从软件层面去看一下,有什么区别啊?

239
00:14:22,014 --> 00:14:22,325
左边这个呢,就是没有进行分布式共享内存的

240
00:14:22,325 --> 00:14:25,014
左边这个呢,就是没有进行分布式共享内存的

241
00:14:25,014 --> 00:14:30,014
每个 Thread Block 呢,就是对应的 SM,里面可以共享自己的内存

242
00:14:30,014 --> 00:14:36,014
但是呢,SM 跟 SM 之间呢,没有办法进行数据交互,于是呢,只能通过全局内存进行交互

243
00:14:36,014 --> 00:14:41,553
但是呢,在 H100 里面呢,引入了一个 SM 的 Cluster,或者现成 Block 的 Cluster

244
00:14:41,553 --> 00:14:44,773
通过硬件来实现分布式的共享内存

245
00:14:44,773 --> 00:14:47,773
SM 跟 SM 之间的数据呢,可以互联

246
00:14:47,773 --> 00:14:50,923
不需要再次把数据呢,放在 HBM 里面再进行交互

247
00:14:50,923 --> 00:14:54,098
这样的话,可以减少寄存器的数量的利用

248
00:14:54,098 --> 00:14:56,376
还可以减少数据传输的时延

249
00:14:58,975 --> 00:15:03,544
现在来到最后一个内容了,就是 Tensor Core 的应用

250
00:15:03,544 --> 00:15:08,544
其实呢,在做 Tensor Core,或者在 H100 里面呢,主要是针对大模型

251
00:15:08,544 --> 00:15:13,544
或者 transformer 的架构进行堆叠的这种像 GPT,ChatGPT 这种大模型啊

252
00:15:13,544 --> 00:15:16,620
但是呢,这些大模型输入的时候呢,有非常多的词汇

253
00:15:16,620 --> 00:15:20,012
会把词汇呢,embedded 成具体的一些向量

254
00:15:20,012 --> 00:15:23,012
然后呢,输出的时候呢,还是以一个向量为主

255
00:15:23,012 --> 00:15:28,012
经过 softmax 就会输出一个比词表更大的一个向量

256
00:15:28,012 --> 00:15:34,687
那这个时候呢,词向量的表就会变得非常非常的大,或者矩阵变得非常的大

257
00:15:34,687 --> 00:15:38,508
在整个 transformer 计算的时候呢,也就变得非常大

258
00:15:38,508 --> 00:15:41,644
刚才谈到,其实 Tensor Core 它的数量是有限的

259
00:15:41,644 --> 00:15:50,936
在 V100 里面,它是 4x4x4,但是呢,在软件上面呢,拓展到 16x16x16,不断地从局部进行搬运

260
00:15:50,936 --> 00:15:58,488
那这个时候呢,其实不是说随随便便的就能够从软件上面去处理所有的 embedded

261
00:15:58,488 --> 00:16:00,315
或者处理所有的大矩阵的

262
00:16:00,315 --> 00:16:02,900
更多呢,看一下刚才的那个例子

263
00:16:02,900 --> 00:16:08,000
针对大模型呢,input Size 等于 1024,然后 batch Size 是 5120

264
00:16:08,000 --> 00:16:11,330
在 v100,使用 FP16 进行训练

265
00:16:11,330 --> 00:16:16,194
然后呢,整个词汇表的大小是三万多

266
00:16:16,194 --> 00:16:20,111
transformer 里面的 Attention 架构呢,里面就会有很多 Mac 漫画矩阵层

267
00:16:20,111 --> 00:16:26,111
如果 pad 到 8 的倍数,整体的性能呢,会比没有 pad 到 8 的倍数里面高很多

268
00:16:26,111 --> 00:16:33,111
这个时候呢,就要求软件编程的时候,其实也需要注意到硬件怎么样才能实现的更加高效

269
00:16:33,111 --> 00:16:38,374
那这个呢,叫做 Padding Vocabulary Size,就对矩阵需要进行 Padding 的操作

270
00:16:40,000 --> 00:16:44,050
就到这里为止了,进行一个简单的总结

271
00:16:44,050 --> 00:16:48,050
在历代的 Tensor Core,主要有三个提升点

272
00:16:48,050 --> 00:16:52,050
第一个呢,就是提升内存,打破整体的内存墙

273
00:16:52,050 --> 00:16:55,050
第二个呢,SM 里面呢,提供更多的数据格式

274
00:16:55,050 --> 00:17:00,050
从帕斯卡的标准的 FP16 到 TF32、IP8、IP4

275
00:17:00,050 --> 00:17:07,050
最后一个呢,就是对应的硬件改了,然后对应的软件编程呢,也会去修改

276
00:17:07,050 --> 00:17:10,050
预设呢,又有了新的 CUDA 的编程的模式

