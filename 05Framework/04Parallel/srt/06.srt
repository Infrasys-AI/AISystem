1
00:00:00,140 --> 00:00:04,700
字幕生成: 粟君杰字幕校对: 粟君杰

2
00:00:04,820 --> 00:00:05,870
嗨，大家好

3
00:00:05,870 --> 00:00:06,980
我是 ZOMI

3
00:00:06,980 --> 00:00:09,560
今天迎来了一个新的内容

4
00:00:09,860 --> 00:00:11,720
混合并行

5
00:00:11,720 --> 00:00:13,000
可以看到

6
00:00:13,000 --> 00:00:15,670
其实深度学习迎来了大模型之后

7
00:00:15,670 --> 00:00:18,490
从 2016 年或者 17 年的时候

8
00:00:18,490 --> 00:00:20,270
出现 Transformer 之后呢

9
00:00:20,270 --> 00:00:24,810
整个网络模型的参数量形成了一个断代的局面

10
00:00:25,570 --> 00:00:27,540
想要大模型训练起来

11
00:00:27,540 --> 00:00:30,440
不仅仅需要大量的 ai 集群或者服务器

12
00:00:30,440 --> 00:00:32,960
还需要很多分布式训练的策略

13
00:00:32,960 --> 00:00:34,400
或者一些算法

14
00:00:34,400 --> 00:00:36,020
今天的内容呢

15
00:00:36,020 --> 00:00:39,300
主要是来分享一下大模型的混合并行

16
00:00:39,300 --> 00:00:41,560
那混合并行呢，主要是混合

17
00:00:41,560 --> 00:00:46,120
在上一节的时候去介绍的模型并行

18
00:00:46,120 --> 00:00:46,780
张量并行

19
00:00:46,780 --> 00:00:47,380
数据并行

20
00:00:47,380 --> 00:00:48,520
还有 Pipeline 并行

21
00:00:48,520 --> 00:00:51,300
把这几种并行的方式混合起来

22
00:00:51,300 --> 00:00:53,820
而在算法结构里面呢

23
00:00:53,820 --> 00:00:55,560
又有两种最著名的算法

24
00:00:55,560 --> 00:00:57,720
第一种呢，就是推荐大模型

25
00:00:57,720 --> 00:01:00,140
主要以 Embending 层为主的

26
00:01:00,140 --> 00:01:04,910
另外一种呢，就是 LLM large language model

27
00:01:04,910 --> 00:01:06,880
大规模语言模型

28
00:01:06,880 --> 00:01:10,390
今天来分别介绍两种模型

29
00:01:10,390 --> 00:01:11,680
推荐模型呢

30
00:01:11,680 --> 00:01:17,900
以 2019 年 Meta 发表的 deep learning recommendations model

31
00:01:17,900 --> 00:01:21,160
DLRM 作为一个经典的例子

32
00:01:21,160 --> 00:01:24,620
去讲讲混合并行在这里面是怎么去实现的

33
00:01:24,620 --> 00:01:28,700
在正式进入到推荐大模型的混合并行的策略

34
00:01:28,700 --> 00:01:29,350
之前呢

35
00:01:29,350 --> 00:01:32,320
回顾一下 CTR 网络模型

36
00:01:32,320 --> 00:01:34,820
CTR 呢，就是 Click-Through-Read

37
00:01:34,820 --> 00:01:37,670
中文呢，叫做点击率预估

38
00:01:37,670 --> 00:01:38,900
简单的来说呢

39
00:01:38,900 --> 00:01:41,150
就是我有一系列的用户的信息

40
00:01:41,150 --> 00:01:43,860
还有一些物品的信息

41
00:01:43,860 --> 00:01:47,820
然后去预测用户去点击这个物品

42
00:01:47,820 --> 00:01:49,620
购买的一个可能性

43
00:01:49,620 --> 00:01:52,500
所以叫做点击率预估的模型

44
00:01:52,500 --> 00:01:55,800
点击率预估模型呢，有两个很重要的 feature

45
00:01:55,800 --> 00:01:56,840
就是特征

46
00:01:56,840 --> 00:01:59,560
第一个特征叫做连续性的特征

47
00:01:59,560 --> 00:02:01,840
主要是讲一些用户本身的特征

48
00:02:01,840 --> 00:02:03,100
包括用户的年龄啊

49
00:02:03,100 --> 00:02:03,760
性别啊

50
00:02:03,760 --> 00:02:06,820
还有用户历史的购买的一些记录

51
00:02:06,820 --> 00:02:09,980
都另外一种特征呢，叫做 Category Feature

52
00:02:09,980 --> 00:02:12,380
就是在排序的物品的特征

53
00:02:12,380 --> 00:02:14,680
那物品的特征就会非常的丰富了

54
00:02:14,680 --> 00:02:16,180
不仅仅包括物品的 ID

55
00:02:16,180 --> 00:02:18,760
还有物品的非常丰富的图片的信息

56
00:02:18,760 --> 00:02:19,690
文本的信息

57
00:02:19,690 --> 00:02:22,390
还有物品历史被点击过的次数

58
00:02:22,390 --> 00:02:24,600
物品被购买的记录的信息

59
00:02:24,600 --> 00:02:28,020
哪些用户喜欢这些物品相关的所有的信息

60
00:02:28,020 --> 00:02:32,010
所以可以看到主要有两个信息 feature 去组成

61
00:02:32,010 --> 00:02:36,380
回到 Meta 的这篇 DLRM 的网络模型里面呢

62
00:02:36,380 --> 00:02:39,680
网络模型结构呢，就如图里面所示

63
00:02:39,680 --> 00:02:42,560
首先用户的一些连续性的特征呢

64
00:02:42,560 --> 00:02:43,860
叫做 Numerical Features

65
00:02:43,860 --> 00:02:47,520
用灰色来代表连续性的这些特征呢

66
00:02:47,520 --> 00:02:50,180
用 mp 网络模型作为一个表示的

67
00:02:50,180 --> 00:02:54,110
而一些物体的 Category 的特征呢可能会非常的多

68
00:02:54,110 --> 00:02:56,400
而且非常大量的习俗化

69
00:02:56,400 --> 00:02:59,460
所以文章当中呢，就使用 Embeding 层作为表征

70
00:02:59,460 --> 00:03:02,840
然后再通过点成把刚才的用户特征

71
00:03:02,840 --> 00:03:05,220
还有 Category 的特征合起来

72
00:03:05,220 --> 00:03:07,380
然后再给 MVP 网络模型

73
00:03:07,380 --> 00:03:09,180
做一个点击率的预估

74
00:03:09,180 --> 00:03:11,680
就是 Click Possibility

75
00:03:11,680 --> 00:03:14,560
回顾分布式训练里面的张量并行

76
00:03:14,560 --> 00:03:16,800
对于 Embeding 层呢，有两种

77
00:03:16,800 --> 00:03:18,840
一种是 Table-wise 的切换方式

78
00:03:18,840 --> 00:03:20,940
一种是 Colum-wise 的切换方式

79
00:03:20,940 --> 00:03:22,770
那 Table-wise 的切分方式呢

80
00:03:22,770 --> 00:03:25,260
主要是以目录特征作为一个切分

81
00:03:25,260 --> 00:03:28,000
我把不同的特征切分到不同的机器

82
00:03:28,000 --> 00:03:32,050
而 Colum-wise 呢就是我把所有的特征都合成一个大表

83
00:03:32,050 --> 00:03:34,860
然后每个特征呢，按列的进行切分

84
00:03:34,860 --> 00:03:38,400
例如特征 0 的会切分 0~63 列

85
00:03:38,400 --> 00:03:39,300
在 GPU 0

86
00:03:39,300 --> 00:03:42,720
然后把剩下的 64~127 列的切分

87
00:03:42,720 --> 00:03:44,720
在 GPU 1

88
00:03:44,720 --> 00:03:47,510
当我的 Category Future 的时候非常的大

89
00:03:47,510 --> 00:03:51,560
可能一个 Embeding 层不能够完全的放在 GPU 0 里面

90
00:03:51,560 --> 00:03:54,890
所以呢 Table-wise 的这种方式呢是不推荐采用的

91
00:03:54,890 --> 00:03:57,980
而更多的是采用 Colum-wise 的这种方式

92
00:03:57,980 --> 00:04:01,190
在 Meta DLRM 这篇文章里面呢

93
00:04:01,190 --> 00:04:05,600
就采用了刚才介绍的 Colum-wise 的切分方式

94
00:04:05,600 --> 00:04:08,240
首先第一个做的就是数据的并行

95
00:04:08,240 --> 00:04:11,780
把 GPU 0，GPU 1 都存放不同的 category

96
00:04:11,780 --> 00:04:13,660
或者用户的目录的数据

97
00:04:13,660 --> 00:04:16,420
然后呢，去输给网络模型

98
00:04:16,420 --> 00:04:17,339
第一层

99
00:04:17,339 --> 00:04:20,189
那 Embedding 呢，就会做一个模型的并行

100
00:04:20,189 --> 00:04:22,720
就是谈到的张量并行

101
00:04:22,720 --> 00:04:24,400
Colum-wise 的切换方式

102
00:04:24,400 --> 00:04:27,440
接着呢，实现一个 All-to-All 的通信

103
00:04:27,440 --> 00:04:29,780
把 Embedding 后的所有的数据

104
00:04:29,780 --> 00:04:32,020
同步到所有的机器上面

105
00:04:32,020 --> 00:04:34,820
接着呢，再进行一个数据的并行

106
00:04:34,820 --> 00:04:36,740
那这里面的数据的并行呢

107
00:04:36,740 --> 00:04:38,630
就不是训练的数据的并行

108
00:04:38,630 --> 00:04:43,700
而是网络模型参数里面的权重优化器啊

109
00:04:43,700 --> 00:04:44,780
激活输出啊

110
00:04:44,780 --> 00:04:49,660
这些网络模型当中衍生的数据的一个并行

111
00:04:49,660 --> 00:04:52,600
现在有 n 个 GPU，GPU 0 里面呢

112
00:04:52,600 --> 00:04:53,980
把 Category Feature 0

113
00:04:53,980 --> 00:04:58,520
还有 Category Feature 2 拿到一部分的 batch 数据之后呢

114
00:04:58,520 --> 00:05:00,500
去传给 Model Parallel 来

115
00:05:00,500 --> 00:05:03,440
GPU N 里面有会存放 Category N

116
00:05:03,440 --> 00:05:07,370
同时我还会存放 Category Future 2 的一些数据

117
00:05:07,370 --> 00:05:08,780
那这里面呢，有一个差异

118
00:05:08,780 --> 00:05:11,770
就是 Category 0 只存放在 GPU 0

119
00:05:11,770 --> 00:05:14,020
Category N 存放在 GPU N

120
00:05:14,020 --> 00:05:17,080
是因为有可能这台 GPU 里面的内存

121
00:05:17,080 --> 00:05:19,090
已经放不下太多的特征了

122
00:05:19,090 --> 00:05:21,760
所以这些特征呢，有部分是重叠

123
00:05:21,760 --> 00:05:24,160
只有这台服务器去拥有的

124
00:05:24,160 --> 00:05:27,040
这时候呢，把数据就传给 Embedding 层

125
00:05:27,040 --> 00:05:29,380
Embedding 层呢，就像刚才所说的

126
00:05:29,380 --> 00:05:32,230
采用的一个 Colum-Wise 的这种切分方式

127
00:05:32,230 --> 00:05:35,100
把不同的列切分到不同的机器

128
00:05:36,420 --> 00:05:39,140
最后的输出呢，去使用通信集合

129
00:05:39,140 --> 00:05:41,420
All-to-All 的方式进行通信

130
00:05:41,420 --> 00:05:44,560
可以看到 GPU 0 对于网络模型的输入

131
00:05:44,560 --> 00:05:46,900
有了所有目录的 Embedding 的特征

132
00:05:46,900 --> 00:05:49,780
包括刚才的第 0 个 Embedding 的特征

133
00:05:49,780 --> 00:05:52,900
第 n 个特征和第二个 Embedding 的特征

134
00:05:52,900 --> 00:05:58,140
同理 GPU N 有了所有目录的 Embedding 的特征

135
00:05:58,140 --> 00:06:02,280
然后呢，再作为真正的神经网络 MLP 层做一个输入

136
00:06:02,280 --> 00:06:05,320
然后去做一个点击率的预估

137
00:06:05,320 --> 00:06:06,880
那这里面呢，刚才说了

138
00:06:06,880 --> 00:06:11,170
会做一个 Data Parallelism 数据的并行

139
00:06:11,170 --> 00:06:13,870
把网络模型的权重参数

140
00:06:13,870 --> 00:06:15,920
网络模型的激活的输出

141
00:06:15,920 --> 00:06:20,040
还有网络模型的优化器的状态都进行一个并行

142
00:06:20,040 --> 00:06:22,770
这里面呢，ZOMI 就想提出一个问题

143
00:06:22,770 --> 00:06:24,750
为什么需要强调

144
00:06:24,750 --> 00:06:29,000
需要对 Embedding 这一层单独的去做模型并行

145
00:06:29,000 --> 00:06:30,350
或者张量并行呢

146
00:06:30,350 --> 00:06:34,500
而不是去强调我后面的网络做一个并行呢

147
00:06:35,260 --> 00:06:39,660
其实这个问题呢，是跟推荐网络模型是相关的

148
00:06:39,660 --> 00:06:44,070
由于 Embedding 层在深度学习推荐模型当中呢

149
00:06:44,070 --> 00:06:46,220
起着非常重要的地位

150
00:06:46,220 --> 00:06:49,520
一般呢，它们用于将输入的用户的数据

151
00:06:49,520 --> 00:06:53,100
或者离散的特征映射到一些高维的向量

152
00:06:53,100 --> 00:06:55,900
以便于后面神经网络去处理

153
00:06:55,900 --> 00:06:56,980
而 Embedding 层呢

154
00:06:56,980 --> 00:06:59,890
通常是构成深度学习推荐模型的

155
00:06:59,890 --> 00:07:01,340
大部分的参数

156
00:07:01,340 --> 00:07:05,030
可能一个 Embedding 长就可以达到了 TB 的级别

157
00:07:05,030 --> 00:07:06,920
所以在训练的期间

158
00:07:06,920 --> 00:07:10,070
很难把它们放在单个 GPU 的内存

159
00:07:10,070 --> 00:07:14,610
所以呢，现代的推荐系统或者深度学习推荐系统

160
00:07:14,610 --> 00:07:15,600
一般来说

161
00:07:15,600 --> 00:07:19,840
需要模型并行和数据并行两种方式进行组合

162
00:07:19,840 --> 00:07:23,110
才能够更好地去训练网络模型

163
00:07:23,110 --> 00:07:26,340
再往下呢，这个图就会更加复杂一点

164
00:07:26,340 --> 00:07:27,300
这些用户的特征

165
00:07:27,300 --> 00:07:30,780
连续的特征呢，使用的是 Data Parallelism 数据并行

166
00:07:30,780 --> 00:07:33,300
而用户的这些一般连续性的特征呢

167
00:07:33,300 --> 00:07:35,360
使用 MLP 层

168
00:07:35,360 --> 00:07:39,140
MLP 层对权重参数有反向的梯度进行并行

169
00:07:39,840 --> 00:07:42,560
最后就是 DLRM 的实验结果

170
00:07:42,560 --> 00:07:44,420
硬件呢，用了两个 AMD

171
00:07:44,420 --> 00:07:45,740
还有 A100-80G

172
00:07:45,740 --> 00:07:49,220
还有 DGXA100 三种硬件进行对比

173
00:07:49,220 --> 00:07:52,460
而在没有进行任何并行的加速的时候呢

174
00:07:52,460 --> 00:07:57,490
可能效果是每秒钟只能处理 17.7k 的数据

175
00:07:57,490 --> 00:08:01,030
而经过刚才的 hybrid parallelism

176
00:08:01,030 --> 00:08:04,240
把数据并行和模型并行混合在一起

177
00:08:04,240 --> 00:08:07,840
能够得到一个接近 700 倍的性能的提升

178
00:08:07,840 --> 00:08:10,450
所以混合并行还是非常有用的

179
00:08:10,450 --> 00:08:14,480
可接下来呢以 Megatron-LM 大规模语言模型

180
00:08:14,480 --> 00:08:15,140
作为一个例子

181
00:08:15,140 --> 00:08:18,020
去讲讲如何做一个混合并行

182
00:08:18,020 --> 00:08:19,520
这次的混合并行呢

183
00:08:19,520 --> 00:08:22,020
比刚才的推荐模型

184
00:08:22,020 --> 00:08:25,440
使用数据并行和模型并行更复杂一点

185
00:08:25,440 --> 00:08:28,670
它融合了流水线并行的方式

186
00:08:28,670 --> 00:08:32,180
下面这个图呢，简单的去理解一下

187
00:08:32,180 --> 00:08:34,340
一个大规模语言模型里面呢

188
00:08:34,340 --> 00:08:35,840
主要每一层绿色的

189
00:08:35,840 --> 00:08:38,180
就是代表一个 Transformer 的结构

190
00:08:38,180 --> 00:08:41,690
可以看到 Transformer 的结构层数会非常的多

191
00:08:41,690 --> 00:08:44,780
通过大量的 Transformer 进行一个堆叠

192
00:08:45,420 --> 00:08:47,840
对于大量的 Transformer 的网络模型结构

193
00:08:47,840 --> 00:08:49,900
想到的一种最好的方式

194
00:08:49,900 --> 00:08:52,150
就是对它进行一个流水线并行

195
00:08:52,150 --> 00:08:55,880
把不同的 Transformer 放在不同的 stage 里面

196
00:08:55,880 --> 00:08:59,120
然后再给不同的机器进行一个计算

197
00:08:59,120 --> 00:09:00,080
GPU 1

198
00:09:00,080 --> 00:09:04,200
这个 stage 呢，就拥有了两个全收网的网络模型结构

199
00:09:04,200 --> 00:09:06,840
然后再给 GPU 2 进行计算

200
00:09:06,840 --> 00:09:08,340
通过这种方式呢

201
00:09:08,340 --> 00:09:10,300
叫做流水线并行

202
00:09:10,300 --> 00:09:10,780
当然了

203
00:09:10,780 --> 00:09:13,120
今天要讲的就是 Megatron-LM

204
00:09:13,120 --> 00:09:16,740
在 2022 年去发表的最新的文章

205
00:09:16,740 --> 00:09:19,620
Efficient Large-Scale Language

206
00:09:19,620 --> 00:09:22,859
Model Training on GPU Cluster Using Megatron-LM

207
00:09:22,859 --> 00:09:25,709
是发表了两篇文章的

208
00:09:25,709 --> 00:09:27,980
第一篇文章主要是讲

209
00:09:28,680 --> 00:09:32,060
对于 Transformer 结构如何进行并行

210
00:09:32,060 --> 00:09:36,170
第二篇文章呢，就是讲混合并行加上流水线并行

211
00:09:36,170 --> 00:09:38,060
如何做加速的

212
00:09:38,100 --> 00:09:40,340
所以回到这个图里面呢

213
00:09:40,340 --> 00:09:41,420
可以看到

214
00:09:41,420 --> 00:09:43,280
除了利用不同的机器呢

215
00:09:43,280 --> 00:09:46,310
进行一个流水线并行之外呢

216
00:09:46,310 --> 00:09:48,260
每一层 Transformer 之间呢

217
00:09:48,260 --> 00:09:50,740
还会进行一个张量的并行

218
00:09:50,740 --> 00:09:52,420
把网络模型

219
00:09:52,420 --> 00:09:56,580
把 Transformer 的结构进行一个纵向的切分

220
00:09:56,580 --> 00:10:00,620
现在来回顾一下张量并行的一个概念

221
00:10:00,620 --> 00:10:03,200
在 Transformer 的 MLP 层里面呢

222
00:10:03,200 --> 00:10:06,230
对权重 A 呢，进行一个列的切分

223
00:10:06,230 --> 00:10:09,200
对权重 B 呢，进行一个行的切分

224
00:10:09,200 --> 00:10:12,080
使得网络模型的输入输出

225
00:10:12,080 --> 00:10:15,230
只需要经过两次通信就可以了

226
00:10:18,140 --> 00:10:20,400
第一次呢，就是对 X 进行一个广播，split 到不同的机器

227
00:10:20,400 --> 00:10:21,960
最后计算完之后呢

228
00:10:21,960 --> 00:10:24,740
通过 g 执行一次 All-Reduce 的通信

229
00:10:24,740 --> 00:10:27,710
就完成了一个 MLP 层的张量并行

230
00:10:27,710 --> 00:10:31,270
同理在 Self-Attention 层也是这种方式

231
00:10:31,270 --> 00:10:35,100
我的 q k v 作为权重来进行一个列的切分

232
00:10:35,100 --> 00:10:37,380
B 这个权重呢，进行一个行的切分

233
00:10:37,380 --> 00:10:40,110
f 进行一次 split 的通信

234
00:10:40,110 --> 00:10:42,940
而 g 呢，执行一次 All-Gather 的通信

235
00:10:42,940 --> 00:10:46,480
这就完成了 Self-Attention 层的张量并行

236
00:10:46,480 --> 00:10:49,960
那接下来的内容呢可能会稍微复杂一点

237
00:10:49,960 --> 00:10:53,290
上面的这个图呢，就是张量并行

238
00:10:53,290 --> 00:10:54,740
tensor parallelism

239
00:10:54,740 --> 00:10:57,680
但同时呢，引入了一个流水线的并行

240
00:10:57,680 --> 00:10:59,480
这是 Transformer1 的结构

241
00:10:59,480 --> 00:11:01,520
这是第二层 Transformer 的结构

242
00:11:01,520 --> 00:11:04,960
不同的 Transformer 放在不同的机器里面

243
00:11:04,960 --> 00:11:06,760
在流水线并行的时候呢

244
00:11:06,760 --> 00:11:08,770
采用的是 GPipline 的方式

245
00:11:08,770 --> 00:11:11,200
GPipline 呢，上一节里面已经讲过了

246
00:11:11,200 --> 00:11:12,680
简单的去回忆一下

247
00:11:12,680 --> 00:11:14,180
GPipline 最重要的特征

248
00:11:14,180 --> 00:11:17,760
就是把 batch 呢分成很多个 Micro-Batch

249
00:11:17,760 --> 00:11:21,510
所以每个 Device 上面都有非常多的 Micro-Batch

250
00:11:21,510 --> 00:11:22,650
从 1~8

251
00:11:22,650 --> 00:11:26,720
然后再进行反向的时候会做一个重计算

252
00:11:26,720 --> 00:11:28,550
所以反向的计算的时间

253
00:11:28,550 --> 00:11:31,080
会比正向的计算的时间要多一倍

254
00:11:31,080 --> 00:11:31,800
这样做呢

255
00:11:31,800 --> 00:11:34,950
就是为了极大的去减少动态的内存

256
00:11:34,950 --> 00:11:37,260
以计算去换空间

257
00:11:37,260 --> 00:11:39,360
计算完一次反向之后呢

258
00:11:39,360 --> 00:11:41,360
在这里面进行一个同步

259
00:11:41,360 --> 00:11:42,380
同步完之后呢

260
00:11:42,380 --> 00:11:45,440
再往下进行下一次 Batch 的计算

261
00:11:45,440 --> 00:11:48,180
这篇文章了还没有这么简单

262
00:11:48,180 --> 00:11:51,570
那上面呢，就是 PipeDream 的具体的实现方式

263
00:11:51,570 --> 00:11:55,500
PipeDream 最主要的特征就是引入了权重隐藏

264
00:11:55,500 --> 00:11:56,550
weight shading

265
00:11:56,550 --> 00:11:58,140
在稳定状态的时候

266
00:11:58,140 --> 00:12:00,300
每次进行一个前向计算的时候

267
00:12:00,300 --> 00:12:04,230
使用的是距离它最近最新的一个反向的计算

268
00:12:04,230 --> 00:12:07,880
同时去记录里面正向权重的版本维护

269
00:12:07,880 --> 00:12:09,980
一个权重的生命周期

270
00:12:12,200 --> 00:12:16,840
用的是 5 正向时候算出来的权重梯度

271
00:12:16,840 --> 00:12:18,500
Megatron-LM 这篇文章里面呢

272
00:12:18,500 --> 00:12:22,040
对 pipeline 并行又提出了一个新的并行方式

273
00:12:22,040 --> 00:12:23,750
叫做 virtual pipeline

274
00:12:23,750 --> 00:12:25,250
虚拟的 pipeline

275
00:12:25,250 --> 00:12:29,210
在下面这个图呢，看一下几个重要的特征

276
00:12:29,210 --> 00:12:30,830
第一个呢，就是 Bubble 的时间

277
00:12:30,830 --> 00:12:34,590
进一步的比刚才上面的 PipeDream 要缩短了

278
00:12:34,590 --> 00:12:37,890
第二个点就是 Mini-Batch 的 1 2 3 是更小了

279
00:12:37,890 --> 00:12:41,050
但是出现了一个浅蓝色的 1 2 3 4

280
00:12:41,050 --> 00:12:43,060
同理在反向的时候呢

281
00:12:43,060 --> 00:12:46,120
我除了有浅绿色的 1 2 3 4 之外呢

282
00:12:46,120 --> 00:12:48,700
我还有深绿色的 1 2 3 4

283
00:12:48,700 --> 00:12:52,110
那这几个浅色和深色的有什么区别呢

284
00:12:52,110 --> 00:12:54,870
以下面这个图作为例子

285
00:12:54,870 --> 00:12:57,390
左边的这个图呢，就是刚才讲到的

286
00:12:57,390 --> 00:12:59,700
对 Transformer 进行不断的堆叠

287
00:12:59,700 --> 00:13:01,860
形成的语言大模型结构

288
00:13:01,860 --> 00:13:05,520
那中间的这个方式呢就是 GPipe 或者 PipeDream

289
00:13:05,520 --> 00:13:08,340
在一个设备上面的放置多个 box

290
00:13:08,340 --> 00:13:10,260
可以看到这里面呢，有两个 box

291
00:13:10,260 --> 00:13:12,880
两个传送门层放在 GPU 0 里面

292
00:13:12,880 --> 00:13:15,220
GPU 1 呢，就放了三个 box

293
00:13:15,220 --> 00:13:17,500
三个 Transformer 的层

294
00:13:17,500 --> 00:13:19,560
而 Virtual Pipline 呢，就反道而其行

295
00:13:19,560 --> 00:13:24,060
首先呢，我 GPU 0 每一台设备上面呢，只存放一个 box

296
00:13:24,060 --> 00:13:25,320
一个 Transformer 的结构

297
00:13:25,320 --> 00:13:27,240
GPU 1 呢，也只存放一个

298
00:13:27,240 --> 00:13:29,060
GPU 1 呢，也存放一个

299
00:13:29,060 --> 00:13:30,800
但是再往下了

300
00:13:30,800 --> 00:13:33,680
这个就是刚才的浅色和深色的区别了

301
00:13:33,680 --> 00:13:37,980
GPU 0 重新放回一个 Transformer 的 box

302
00:13:37,980 --> 00:13:41,190
这种方式就实现了数量不变的情况

303
00:13:41,190 --> 00:13:44,020
分出了更多的 Pipeline stage

304
00:13:44,020 --> 00:13:47,620
用更多的通信量去换取 Bubble 的

305
00:13:47,620 --> 00:13:48,900
比例的降低

306
00:13:48,900 --> 00:13:51,120
所以可以看到这里面呢，有浅色

307
00:13:51,120 --> 00:13:51,810
有深色

308
00:13:51,810 --> 00:13:53,700
但是呢，带来的好处就是

309
00:13:53,700 --> 00:13:57,320
中间的 bubble 空载的时间减少了

310
00:13:57,320 --> 00:14:00,440
不能说在所有的 ai 集群里面呢

311
00:14:00,440 --> 00:14:02,740
这种的切分方式是最优的

312
00:14:02,740 --> 00:14:06,430
但是呢，在英伟达的 DGX A100 集群里面呢

313
00:14:06,430 --> 00:14:10,160
作者呢，就证明了这种 tensor Parallelism 来加上 Data Parallelism

314
00:14:10,160 --> 00:14:12,440
还有 Pipline Parallelism 的这种方式

315
00:14:12,440 --> 00:14:16,340
实现了整个通信的加速比是非常好的

316
00:14:16,340 --> 00:14:20,000
并行的算法的内容在这里呢，基本上就结束了

317
00:14:20,000 --> 00:14:22,820
今天了解了一个推荐的大模型

318
00:14:22,820 --> 00:14:24,980
主要是由 Embendding 层去组成的

319
00:14:24,980 --> 00:14:29,080
而 Embendding 里面呢，更多的是采用 Model Parallelism 来算

320
00:14:29,080 --> 00:14:31,000
在网络模型结构阶段呢

321
00:14:31,000 --> 00:14:33,670
更多的是采用一个 Data Parallelism

322
00:14:33,670 --> 00:14:35,500
对于语言大模型呢

323
00:14:35,500 --> 00:14:38,460
基本上所有的并行的方式

324
00:14:38,460 --> 00:14:40,560
包括数据并行

325
00:14:40,560 --> 00:14:43,860
张量并行和流水线并行都会混合在一起用

326
00:14:43,860 --> 00:14:46,440
那这种方式呢叫做混合并行

327
00:14:46,440 --> 00:14:48,580
最后我稍微打一个广告

328
00:14:48,580 --> 00:14:52,900
就是在 MindSpore 里面为了实现更多的并行方式

329
00:14:52,900 --> 00:14:54,520
提出了一个新的特性

330
00:14:54,520 --> 00:14:56,640
叫做自动混合并行

331
00:14:56,640 --> 00:15:00,180
因为刚才提到的很多的混合并行方式啊

332
00:15:00,180 --> 00:15:01,880
需要人工地去设计

333
00:15:01,880 --> 00:15:03,560
人工地去调测

334
00:15:03,560 --> 00:15:05,540
或者找到一种很好的规律

335
00:15:05,540 --> 00:15:09,520
而 MindSpore 直接使用了自动混合并行

336
00:15:09,520 --> 00:15:12,400
解决了很多用户去尝试去了解

337
00:15:12,400 --> 00:15:15,340
去深入到这个 ai 系统的细节里面

338
00:15:15,340 --> 00:15:16,540
让用户呢

339
00:15:16,540 --> 00:15:19,750
或者开发者更加聚焦于自己的算法

340
00:15:19,750 --> 00:15:22,880
他们自己的网络模型结构的搭建理念哦

341
00:15:22,900 --> 00:15:24,380
卷的不行了

342
00:15:24,380 --> 00:15:25,280
卷的不行了

343
00:15:25,280 --> 00:15:26,900
记得一键三连加关注哦

344
00:15:26,900 --> 00:15:30,320
所有的内容都会开源在下面这条链接里面

345
00:15:30,320 --> 00:15:31,360
拜了个拜