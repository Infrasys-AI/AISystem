1
00:00:00,965 --> 00:00:05,440
字幕生成：Galaxies 字幕校对：Not_Ur_77

2
00:00:05,440 --> 00:00:07,585
嗨大家好我是 ZOMI

3
00:00:07,585 --> 00:00:09,425
卷王来了卷王来了

4
00:00:09,425 --> 00:00:11,520
那今天呢给大家带来一个新的内容

5
00:00:11,520 --> 00:00:15,280
就是推理引擎或者推理系统里面的模型压缩

6
00:00:15,280 --> 00:00:19,080
也可以叫模型小型化或者模型轻量化这个工作

7
00:00:19,080 --> 00:00:22,880
那今天呢主要是给大家去带来一个新的内容

8
00:00:22,880 --> 00:00:25,840
那说到这个内容呢我可不困了

9
00:00:25,840 --> 00:00:31,240
因为这个内容叫做离线优化或离线模型压缩 

10
00:00:31,240 --> 00:00:34,880
里面呢主要去讲讲模型压缩的四件套

11
00:00:34,880 --> 00:00:36,160
哪四件套啊

12
00:00:36,160 --> 00:00:39,640
那第一个呢就是低比特的量化

13
00:00:39,640 --> 00:00:41,880
那量化有训练量化还有感知量化

14
00:00:41,880 --> 00:00:43,840
这两个呢都会去讲

15
00:00:43,840 --> 00:00:50,960
接着呢去讲讲一个现在还在学术前沿的二值化网络模型 

16
00:00:50,960 --> 00:00:56,240
所谓二值化网络模型其实就是-1 0 1 这种状态啊 

17
00:00:56,240 --> 00:00:59,000
去表示网络模型的结构

18
00:00:59,000 --> 00:01:00,400
所以说很有意思

19
00:01:00,400 --> 00:01:03,280
它其实也属于低比特量化其中的一种

20
00:01:03,280 --> 00:01:05,160
不过呢由于它的特殊性

21
00:01:05,160 --> 00:01:07,080
所以单独把它拿出来

22
00:01:07,080 --> 00:01:10,360
接着呢去看看模型的剪枝

23
00:01:10,360 --> 00:01:11,560
模型的剪枝比较简单

24
00:01:11,560 --> 00:01:15,440
就是网络模型啊有非常多的连接还有权重 

25
00:01:15,440 --> 00:01:17,760
怎么把这些参数变小

26
00:01:17,760 --> 00:01:18,920
把它们剪掉

27
00:01:18,920 --> 00:01:20,840
那这个呢就是模型剪枝

28
00:01:20,840 --> 00:01:24,080
最后一个内容就是知识蒸馏

29
00:01:24,080 --> 00:01:27,000
知识蒸馏（Knowledge Distribution）

30
00:01:27,000 --> 00:01:28,800
不知道读的对不对啊

31
00:01:28,800 --> 00:01:32,200
这四个内容就是模型压缩的四件套

32
00:01:32,200 --> 00:01:36,000
那看一下在整个推理引擎架构里面

33
00:01:36,000 --> 00:01:38,640
模型压缩一般处于哪个步骤

34
00:01:41,280 --> 00:01:42,320
下面来看一下

35
00:01:42,320 --> 00:01:45,680
这个呢就是整个推理引擎的总体架构

36
00:01:45,680 --> 00:01:47,680
那简单的去复述一下

37
00:01:47,680 --> 00:01:51,280
其实之前已经做了一个很详细很详细的介绍了

38
00:01:52,880 --> 00:01:55,200
首先呢对上有个 API 的层

39
00:01:55,200 --> 00:01:57,600
接着呢有个模型的转换

40
00:01:57,600 --> 00:01:58,520
那模型的转换呢

41
00:01:58,520 --> 00:02:01,800
就会把从不同的 AI 框架训练出来的网络模型

42
00:02:01,800 --> 00:02:04,320
转换成为推理引擎的自己的 IR

43
00:02:04,320 --> 00:02:05,640
或者自己的 Schema

44
00:02:05,640 --> 00:02:07,040
转换成为自己的 IR 之后呢

45
00:02:07,040 --> 00:02:09,720
就会经过模型压缩这个功能

46
00:02:09,720 --> 00:02:11,560
那可能会做一些量化

47
00:02:11,560 --> 00:02:13,085
蒸馏 剪枝 二值化

48
00:02:14,360 --> 00:02:17,960
有可能会把模型压缩的四件套同时用起来

49
00:02:17,960 --> 00:02:18,720
那这个时候呢

50
00:02:18,720 --> 00:02:22,520
叫做多维混和压缩算法

51
00:02:22,520 --> 00:02:23,120
那没关系

52
00:02:23,120 --> 00:02:25,000
这些名字都是随便起的

53
00:02:25,000 --> 00:02:26,440
或者怎么起的高大上

54
00:02:26,560 --> 00:02:28,400
显得怎么有竞争力也好

55
00:02:28,400 --> 00:02:29,120
但是呢

56
00:02:29,120 --> 00:02:30,040
说白到底

57
00:02:30,040 --> 00:02:32,560
它还是只有这四种算法去牵引

58
00:02:33,960 --> 00:02:35,520
实现完这个模型压缩之后呢

59
00:02:35,520 --> 00:02:38,280
就真正的去把网络模型给到 Runtime

60
00:02:38,280 --> 00:02:41,600
还有 Kernel 去执行在不同的硬件上面

61
00:02:41,600 --> 00:02:43,760
那整体流程就是这样的

62
00:02:43,760 --> 00:02:44,400
下面呢

63
00:02:44,400 --> 00:02:46,520
今天最主要的关注点呢

64
00:02:46,520 --> 00:02:49,000
就是对模型进行压缩

65
00:02:49,000 --> 00:02:51,440
把模型变得越小越好

66
00:02:51,440 --> 00:02:53,520
就减少网络模型的大小

67
00:02:53,520 --> 00:02:54,080
那第二个呢

68
00:02:54,080 --> 00:02:57,400
就是加快整个的推理的速度

69
00:02:57,400 --> 00:02:59,320
不是训练哦 是推理 

70
00:02:59,320 --> 00:03:04,080
使得在推理引擎里面跑得越快越好

71
00:03:04,080 --> 00:03:05,320
但是呢

72
00:03:05,320 --> 00:03:07,880
凡事都有一个约束

73
00:03:07,880 --> 00:03:10,960
这就是要求保持相同的精度

74
00:03:10,960 --> 00:03:13,760
或者不怎么掉精度的前提之下呢

75
00:03:13,760 --> 00:03:18,040
去减少网络模型的大小和加快推理速度

76
00:03:18,040 --> 00:03:20,560
所以保持精度很重要

77
00:03:20,560 --> 00:03:21,720
你模型变小了

78
00:03:21,760 --> 00:03:24,560
但是你不能把我的精度给改掉了哦

79
00:03:24,560 --> 00:03:28,080
那下面来看看整体的在推理流程里面

80
00:03:28,080 --> 00:03:32,000
会把很多不同 AI 框架训练出来的网络模型呢

81
00:03:32,000 --> 00:03:34,160
转换成为推理的模型

82
00:03:34,160 --> 00:03:38,320
接着大部分的都会经过一个模型的压缩模块

83
00:03:38,320 --> 00:03:43,600
通过模型的压缩 把我的模型变得又小 运行起来又快 

84
00:03:43,600 --> 00:03:45,440
而且精度还能无损

85
00:03:45,440 --> 00:03:46,640
那这是更好的

86
00:03:46,640 --> 00:03:47,760
整套流程呢

87
00:03:47,760 --> 00:03:50,280
就是在之前已经详细讲开过了

88
00:03:50,320 --> 00:03:53,280
这里面呢重点关注就是模型压缩

89
00:03:56,880 --> 00:03:59,720
好了今天的内容呢就到这里为止

90
00:03:59,720 --> 00:04:04,280
欢迎大家继续留意下一期真正的一些算法和内容

91
00:04:05,280 --> 00:04:06,080
拜了个拜

92
00:04:07,760 --> 00:04:09,440
卷的不行了卷的不行了

93
00:04:09,440 --> 00:04:11,240
记得一键三连加关注哦

94
00:04:11,240 --> 00:04:14,840
所有的内容都会开源在下面这条链接里面

95
00:04:14,840 --> 00:04:15,560
拜了个拜

