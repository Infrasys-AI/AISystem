1
00:00:00,000 --> 00:00:06,016
字幕生成：Galaxies 字幕校对：Not_Ur_77

2
00:00:06,016 --> 00:00:07,500
Hello 大家好,我是 ZOMI

3
00:00:07,500 --> 00:00:13,500
现在的位置处于推理引擎这个系列里面的模型压缩

4
00:00:13,500 --> 00:00:16,700
今天我要给大家去带来一个新的内容

5
00:00:16,700 --> 00:00:18,500
应该不算新的内容了

6
00:00:18,500 --> 00:00:22,500
在量化里面再进一步的去展开感知量化训练

7
00:00:22,500 --> 00:00:25,700
也就是大家经常说到的 QAT

8
00:00:26,200 --> 00:00:31,700
现在来到了低比特量化这个系列课程里面的第二个内容

9
00:00:31,700 --> 00:00:34,700
感知量化 QAT

10
00:00:34,700 --> 00:00:39,200
下面再来看一下在整个推理引擎架构里面

11
00:00:39,200 --> 00:00:42,200
感知量化其实处于这个阶段

12
00:00:42,200 --> 00:00:46,200
讲错了,量化是处于这个阶段

13
00:00:46,200 --> 00:00:49,200
但是感知量化是跟训练强相关的

14
00:00:49,200 --> 00:00:51,700
所以这个内容是看不到的

15
00:00:52,200 --> 00:00:54,700
在 AI 框架上面的一个特性

16
00:00:54,700 --> 00:00:57,700
例如 PyTorch 也有自己的感知量化训练的一些特性

17
00:00:57,700 --> 00:01:01,700
还有 MindSpore 也会推出自己感知量化训练的特性

18
00:01:01,700 --> 00:01:07,200
接下来来一起看一下感知量化训练它到底是怎么实现的 

19
00:01:07,200 --> 00:01:11,700
简单的一句话就是会在一个正常的网络模型栏中

20
00:01:11,700 --> 00:01:15,200
去插入一些伪量化的算子或者节点

21
00:01:15,200 --> 00:01:17,700
这个节点叫做 FakeQuant

22
00:01:17,700 --> 00:01:19,200
因为它不是真正的量化

23
00:01:19,200 --> 00:01:23,700
而是用来模拟量化的时候引入的一些误差

24
00:01:23,700 --> 00:01:25,700
而在真正端侧推理的时候

25
00:01:25,700 --> 00:01:29,200
需要把这些 FakeQuant 去进行一个折叠

26
00:01:29,200 --> 00:01:30,700
那为啥叫折叠呢?

27
00:01:30,700 --> 00:01:33,200
是因为它确实只剩下一些常量

28
00:01:33,200 --> 00:01:35,700
需要对这些常量进行折叠

29
00:01:35,700 --> 00:01:39,700
然后把相关的属性或者信息变成 Tensor 的信息

30
00:01:39,700 --> 00:01:41,700
最后再进行一个推理

31
00:01:41,700 --> 00:01:44,200
那看看下面这个图

32
00:01:44,200 --> 00:01:46,700
下面这个是一个计算图

33
00:01:46,700 --> 00:01:48,700
输进去的是一些数据

34
00:01:48,700 --> 00:01:52,200
需要在输进去的数据的时候插入一个伪量化的算子

35
00:01:52,200 --> 00:01:56,700
接着可能还需要对权重插入一个伪量化的算子

36
00:01:56,700 --> 00:02:01,200
完成卷积计算之后就会给一个 NB 层进行一个学习

37
00:02:01,200 --> 00:02:04,200
学习完之后就真正的去进入了一个 ReLU 了

38
00:02:04,200 --> 00:02:08,200
在出去 ReLU 之前也会插一个伪量化的节点在这里面

39
00:02:08,200 --> 00:02:11,700
像这种确实在一个正常的计算图里面

40
00:02:11,700 --> 00:02:13,700
去插入各种伪量化的节点

41
00:02:13,700 --> 00:02:16,700
这种方式叫做 QAT

42
00:02:17,200 --> 00:02:22,200
接下来去看看刚才大量的去提到一些伪量化的节点 Fake Quant

43
00:02:22,200 --> 00:02:25,200
那 Fake Quant 的节点有什幺用吗?

44
00:02:25,200 --> 00:02:27,200
下面就有两个比较大的作用了

45
00:02:27,200 --> 00:02:29,700
这也是伪量化节点的一个具体的作用

46
00:02:29,700 --> 00:02:33,700
首先这个伪量化节点主要是找到输入数据的分布

47
00:02:33,700 --> 00:02:38,200
也就是找到数据的一个最大值和最小值

48
00:02:38,200 --> 00:02:41,700
第二个点就是刚才简单的提到过的

49
00:02:41,700 --> 00:02:43,200
它去模拟量化操作

50
00:02:43,200 --> 00:02:50,200
就是把一些 FP32 量化到 INT8 这种低比特的时候的一些精度的损失

51
00:02:50,200 --> 00:02:53,700
把这些损失在网络模型训练或者 Fine Tuning 的时候

52
00:02:53,700 --> 00:02:57,700
作用到整个网络模型当中传递给损失函数

53
00:02:57,700 --> 00:02:58,700
就是 Loss

54
00:02:58,700 --> 00:03:01,700
让优化器在训练或者 Fine Tuning 的过程当中

55
00:03:01,700 --> 00:03:07,700
对因为量化所造成的损失进行一个优化和学习

56
00:03:07,700 --> 00:03:14,700
那至于第二个就是真正的一个伪量化节点或者伪量化算子它的核心作用了

57
00:03:14,700 --> 00:03:21,700
下面去看看伪量化节点的一个正向传播具体是怎么算的

58
00:03:21,700 --> 00:03:25,700
其实为了求出网络模型的输入

59
00:03:25,700 --> 00:03:28,700
就是 Tensor 一个比较精确的 min 和 max

60
00:03:28,700 --> 00:03:33,700
所以会在一个网络模型训练的时候插入伪量化节点

61
00:03:33,700 --> 00:03:36,200
也就是需要获取那个计算图

62
00:03:36,200 --> 00:03:40,200
然后对这个计算图进行改造插入希望的节点

63
00:03:40,200 --> 00:03:44,200
然后模拟误差得到数据的分布

64
00:03:44,200 --> 00:03:48,200
而对每一个算子都会去求输入的数据 x

65
00:03:48,200 --> 00:03:50,200
它的一个最小值还有最大值

66
00:03:50,200 --> 00:03:53,200
还记得在上一节课里面去讲量化原理的时候

67
00:03:53,200 --> 00:03:55,200
有了最小值和最大值之后

68
00:03:55,200 --> 00:03:58,200
就可以去求量化的 scale

69
00:03:58,200 --> 00:04:03,200
通过这个 scale 就可以把输入数据直接量化成 INT8

70
00:04:03,200 --> 00:04:05,700
那正向的 forward 的时候就会做这个工作

71
00:04:05,700 --> 00:04:07,700
除了记录最大值和最小值

72
00:04:07,700 --> 00:04:11,700
它还要做一个量化模拟的操作

73
00:04:11,700 --> 00:04:15,700
假设之前的数据是一个平滑的数据类似一条线性的 

74
00:04:15,700 --> 00:04:19,700
经过伪量化算子进行模拟的时候就变成了有阶梯形状

75
00:04:19,700 --> 00:04:22,700
把大部分的数据都直接消掉了

76
00:04:22,700 --> 00:04:25,700
从 FP32 的数据变成 INT8 的数据

77
00:04:25,700 --> 00:04:28,700
那这个就是伪量化算子的正向传播

78
00:04:28,700 --> 00:04:31,700
有正向是不是应该有反向啊

79
00:04:31,700 --> 00:04:34,700
那现在来看看反向传播

80
00:04:34,700 --> 00:04:36,700
这个伪量化算子具体怎么实现

81
00:04:36,700 --> 00:04:38,700
按照刚才正向传播的公式

82
00:04:38,700 --> 00:04:40,700
如果反向的时候呢

83
00:04:40,700 --> 00:04:42,700
对刚才正向的那条公式求导数

84
00:04:42,700 --> 00:04:44,700
肯定会导致权重为 0

85
00:04:44,700 --> 00:04:47,700
权重为 0 就没有办法去学习了

86
00:04:47,700 --> 00:04:48,700
于是呢反向的时候呢

87
00:04:48,700 --> 00:04:51,700
相当于一个直通的连通器

88
00:04:51,700 --> 00:04:53,700
把 delta in 直接给 delta out

89
00:04:53,700 --> 00:04:55,700
但是有点注意的就是

90
00:04:55,700 --> 00:04:57,700
输入的数据 x 呢

91
00:04:57,700 --> 00:04:59,700
必须要在量化范围之内

92
00:04:59,700 --> 00:05:01,700
如果不在的把它截断

93
00:05:01,700 --> 00:05:04,700
于是呢最终反向传播的 fake quart 呢

94
00:05:04,700 --> 00:05:09,700
一般来说都会对它的数据进行截断式的处理 

95
00:05:10,700 --> 00:05:12,700
了解完伪量化算子呢

96
00:05:12,700 --> 00:05:14,700
现在呢伪量化算子

97
00:05:14,700 --> 00:05:17,700
还有一个很重要的工作就是更新 min 和 max 

98
00:05:17,700 --> 00:05:20,700
因为每一次训练每一轮迭代

99
00:05:20,700 --> 00:05:22,700
每一个 epoch 每个 step 呢

100
00:05:22,700 --> 00:05:24,700
它都会有不同的 min 和 max

101
00:05:24,700 --> 00:05:26,700
它都有不同的数据的输入

102
00:05:26,700 --> 00:05:30,700
有点类似于 BN 算子或者 LayerNorm 算子呢 

103
00:05:30,700 --> 00:05:33,700
去更新 beta 和 gamma 的这种方式

104
00:05:33,700 --> 00:05:35,700
通过一个 runing min runing max

105
00:05:35,700 --> 00:05:37,700
还有 moving min moving max 进行计算

106
00:05:37,700 --> 00:05:40,700
如果大家看不懂没关系

107
00:05:40,700 --> 00:05:43,700
去看看 BN 这个算子呢怎么去更新 beta 和 gamma 的 

108
00:05:43,700 --> 00:05:48,700
如果大家确实很有兴趣去了解伪量化算子  真正的一些代码实现呢 

109
00:05:48,700 --> 00:05:51,700
也可以去看一下 Pytorch 的具体实现

110
00:05:51,700 --> 00:05:53,700
那下面呢去

111
00:05:53,700 --> 00:05:56,700
下面呢让观众提两个问题

112
00:05:57,700 --> 00:05:58,700
ZOMI 老师你好啊

113
00:05:58,700 --> 00:06:06,700
我想问一下在什么地方或者什幺位置去插入 FakeQuant 伪量化这个节点呢 

114
00:06:08,700 --> 00:06:11,700
诶小新你这个问题确实是灵魂拷问的 

115
00:06:11,700 --> 00:06:14,700
刚才只是简单的去给大家讲了

116
00:06:14,700 --> 00:06:17,700
FakeQuant 这个伪量化的算子是怎么去实现的 

117
00:06:17,700 --> 00:06:20,700
正向怎么把它进行一个伪量化的学习

118
00:06:20,700 --> 00:06:23,700
反向呢怎么对它进行一个截断

119
00:06:23,700 --> 00:06:27,896
那一般来说呢会在一些密集的计算的算子 

120
00:06:27,896 --> 00:06:29,700
说到密集计算算子呢其实并不多 

121
00:06:29,700 --> 00:06:32,700
有点类似于华为昇腾里面的一个 Cube 这些算子 

122
00:06:32,700 --> 00:06:34,700
举个简单的几个例子

123
00:06:34,700 --> 00:06:39,465
就是 GMM 矩阵的相乘还有卷积 

124
00:06:39,465 --> 00:06:40,700
那这些呢就是密集计算的算子

125
00:06:40,700 --> 00:06:42,700
另外还会激活算子之后

126
00:06:42,700 --> 00:06:44,700
或者之前进行插入的

127
00:06:44,700 --> 00:06:46,700
最后呢还会在网络模型输入

128
00:06:46,700 --> 00:06:49,700
输出的地方进行插入伪量化算子

129
00:06:49,700 --> 00:06:52,700
下面呢看一个更加具体的例子

130
00:06:52,700 --> 00:06:55,700
那虽然看上去左边的这个图呢很复杂 

131
00:06:55,700 --> 00:06:58,700
但实际上啊这个是一个卷积 BN ReLU

132
00:06:58,700 --> 00:07:01,700
三个简单的算子的一个计算图

133
00:07:01,700 --> 00:07:04,700
因为 BN 层呢它有很多的不同的参数 

134
00:07:04,700 --> 00:07:06,700
它要更新 Gamma 了要更新 Beta 了 

135
00:07:06,700 --> 00:07:08,700
所以你看上去很复杂

136
00:07:08,700 --> 00:07:10,700
插入伪量化算子呢

137
00:07:10,700 --> 00:07:13,700
就会在这个图的 input 里面插入在 weights 里面插入 

138
00:07:13,700 --> 00:07:17,700
可能在对 W 进行输出的时候插入一个 

139
00:07:17,700 --> 00:07:21,700
另外的话还会对激活后面插入一个 

140
00:07:21,700 --> 00:07:23,700
这个呢就是一般的插入方式

141
00:07:23,700 --> 00:07:26,700
值得注意的就是如果你研究感知量化算法呢 

142
00:07:26,700 --> 00:07:30,700
你可能会提出很多不同的插入的方式去学习 

143
00:07:30,700 --> 00:07:33,700
也可能会自己造一个伪量化的算子

144
00:07:33,700 --> 00:07:35,700
或者把伪量化算子呢改掉

145
00:07:35,700 --> 00:07:38,700
那这个呢是最原始或者最 naive 的一种方式 

146
00:07:39,700 --> 00:07:46,700
好了刚才只是简单的去给大家讲了感知量化训练呢一般的通用性的算法  

147
00:07:46,700 --> 00:07:49,700
还讲了伪量化算子是怎么实现的

148
00:07:49,700 --> 00:07:51,700
包括正向反向另外还有伪量化算子呢 

149
00:07:51,700 --> 00:07:53,700
是怎么插入到计算图里面

150
00:07:53,700 --> 00:07:58,700
接下来我想提出几个问题让大家一起去思考思考 

151
00:07:58,700 --> 00:08:04,700
第一个点呢就是如何平滑的计算伪量化阶段的两个值 Min 和 Max 

152
00:08:04,700 --> 00:08:07,700
刚才提到 batch normalization 这种算子 

153
00:08:07,700 --> 00:08:09,700
它其实也有很多参数

154
00:08:09,700 --> 00:08:13,700
需要根据输入的数据还有输出的数据进行学习的 

155
00:08:13,700 --> 00:08:17,700
像 BN 层呢一般都会有一个平滑计算的过程 

156
00:08:17,700 --> 00:08:19,700
在具体算子或者 kernel 实现的时候呢

157
00:08:19,700 --> 00:08:23,700
就会有一个 moving mean moving variance 去进行一个平滑 

158
00:08:23,700 --> 00:08:27,700
那另外一点就是我去看一些论文呢 

159
00:08:27,700 --> 00:08:34,700
基本上都会对 BN 进行一个矫正而且还会有一个 Bessel 矫正 

160
00:08:34,700 --> 00:08:37,700
为什么需要做这些矫正呢

161
00:08:37,700 --> 00:08:41,700
那这一点呢其实是跟样本有关系的 

162
00:08:41,700 --> 00:08:43,700
我也非常欢迎大家去看一下

163
00:08:43,700 --> 00:08:46,700
或者去搜索一下相关的原理

164
00:08:46,700 --> 00:08:49,700
最后一点呢如果要对 BN 进行折叠

165
00:08:49,700 --> 00:08:53,700
那计算公式或者 kernel 会不会有新的变化呢 

166
00:08:53,700 --> 00:08:57,700
在感知量化训练的过程当中确实是有的 

167
00:08:57,700 --> 00:09:02,700
在真正计算的时候呢确实需要对它进行一个特殊的插入 

168
00:09:02,700 --> 00:09:05,700
可以看到左边呢这个就是在没有融合之前 

169
00:09:05,700 --> 00:09:09,700
右边这个呢就是在融合之后的算子融合的过程当中呢 

170
00:09:09,700 --> 00:09:11,700
有同学去问我

171
00:09:11,700 --> 00:09:12,700
上面这条公式呢

172
00:09:12,700 --> 00:09:13,700
变成下面这条公式

173
00:09:13,700 --> 00:09:15,700
具体有什么不一样呢

174
00:09:15,700 --> 00:09:18,700
很明显大家看计算图就知道了

175
00:09:18,700 --> 00:09:21,700
整体的计算公式或者整体的计算形态啊

176
00:09:21,700 --> 00:09:23,700
确实改变了很多

177
00:09:23,700 --> 00:09:25,700
回到 AI 系统

178
00:09:25,700 --> 00:09:27,700
AI 框架里面去看一下

179
00:09:27,700 --> 00:09:29,700
QAT 的一个工作流程

180
00:09:29,700 --> 00:09:33,700
首先会有很多预训练的模型

181
00:09:33,700 --> 00:09:34,700
或者已经训练好的模型

182
00:09:34,700 --> 00:09:36,700
获取这个计算图之后

183
00:09:36,700 --> 00:09:38,700
需要对网络模型呢

184
00:09:38,700 --> 00:09:39,700
进行一个改造

185
00:09:39,700 --> 00:09:45,700
去插入一些刚才讲到的伪量化的算子或者这里面呢叫做观察算子 

186
00:09:45,700 --> 00:09:48,700
那统一称为伪量化算子就好了

187
00:09:48,700 --> 00:09:51,700
另外还要准备一些训练的数据

188
00:09:51,700 --> 00:09:52,700
那这个时候呢

189
00:09:52,700 --> 00:09:55,700
这里面输进去的就是已经经过改造的

190
00:09:55,700 --> 00:09:58,700
插入了伪量化算子之后的一个计算图

191
00:09:58,700 --> 00:09:59,700
另外一边呢

192
00:09:59,700 --> 00:10:01,700
需要 Fine Tuning 或者 Training 的数据

193
00:10:01,700 --> 00:10:02,700
接着呢

194
00:10:02,700 --> 00:10:04,700
真正的去执行 Training 和 Fine Tuning

195
00:10:04,700 --> 00:10:05,700
最后呢

196
00:10:05,700 --> 00:10:06,700
在学习的过程当中

197
00:10:06,700 --> 00:10:08,700
就会不断的去学习

198
00:10:08,700 --> 00:10:10,700
因为量化所造成的一些误差

199
00:10:10,700 --> 00:10:11,700
把这些误差呢

200
00:10:11,700 --> 00:10:12,700
通过学习的方式

201
00:10:12,700 --> 00:10:14,700
把它变得更小更好

202
00:10:14,700 --> 00:10:15,700
最后呢

203
00:10:15,700 --> 00:10:17,700
就输出了 QAT 的网络模型

204
00:10:17,700 --> 00:10:19,700
那这个 QAT 的网络模型

205
00:10:19,700 --> 00:10:22,700
还真没有办法马上去执行的哦

206
00:10:22,700 --> 00:10:24,700
它要经过推理系统的一个转换模块

207
00:10:24,700 --> 00:10:27,700
然后去掉一些冗余的伪量化的算子

208
00:10:27,700 --> 00:10:29,700
才能够正常的推理

209
00:10:29,700 --> 00:10:30,700
那这一部分呢

210
00:10:30,700 --> 00:10:33,700
会在下一个内容里面去给大家介绍

211
00:10:34,700 --> 00:10:37,700
接下来来聊一聊 QAT 的衍生研究

212
00:10:37,700 --> 00:10:39,700
那 QAT 的衍生研究非常多啊

213
00:10:39,700 --> 00:10:40,700
而且这方面呢

214
00:10:40,700 --> 00:10:42,700
确实有很多新的算法

215
00:10:42,700 --> 00:10:43,700
或者新的 idea 提出

216
00:10:43,700 --> 00:10:45,700
例如好像这篇文章

217
00:10:45,700 --> 00:10:47,700
例如好像这篇文章

218
00:10:47,700 --> 00:10:49,700
中文名我也翻译不出来

219
00:10:49,700 --> 00:10:50,700
它这里面呢

220
00:10:50,700 --> 00:10:52,700
就做了一个新的伪量化的算子

221
00:10:52,700 --> 00:10:53,700
那正向呢

222
00:10:53,700 --> 00:10:55,700
可能跟刚才讲的差不多

223
00:10:55,700 --> 00:10:56,700
但是反向呢

224
00:10:56,700 --> 00:10:57,700
它就不一样了

225
00:10:57,700 --> 00:10:59,700
它不是一个简单的节段

226
00:10:59,700 --> 00:11:00,700
那这种方式呢

227
00:11:00,700 --> 00:11:02,700
就是对伪量化算子进行一个创新的

228
00:11:03,700 --> 00:11:04,700
另外呢

229
00:11:04,700 --> 00:11:06,700
有一些科研类的创新的文章呢

230
00:11:06,700 --> 00:11:08,700
会对计算图啊

231
00:11:08,700 --> 00:11:10,700
或者对量化的流程

232
00:11:10,700 --> 00:11:12,700
进行一个改进

233
00:11:12,700 --> 00:11:13,700
那最后呢

234
00:11:13,700 --> 00:11:14,700
再看一下

235
00:11:14,700 --> 00:11:16,700
其实在做量化的时候呢

236
00:11:16,700 --> 00:11:17,700
刚才只是很笼统的

237
00:11:17,700 --> 00:11:19,700
说伪量化算子呢

238
00:11:19,700 --> 00:11:21,700
只是对输入的数据呢

239
00:11:21,700 --> 00:11:23,700
模拟伪量化的过程当中

240
00:11:23,700 --> 00:11:25,700
但是数据的形态是很多的

241
00:11:25,700 --> 00:11:27,700
有 NCHW

242
00:11:27,700 --> 00:11:28,700
那这个时候呢

243
00:11:28,700 --> 00:11:29,700
可以对 per-channel 进行伪量化

244
00:11:29,700 --> 00:11:30,700
或者 per-channel 进行量化

245
00:11:30,700 --> 00:11:33,700
也可以对 per-tensor 进行量化

246
00:11:33,700 --> 00:11:37,700
所以量化的方式和种类还有不同层次有非常的多  

247
00:11:37,700 --> 00:11:43,700
这里面呢 ZOMI 更欢迎大家去看看相关更多的论文和最新的量化的研究 

248
00:11:45,700 --> 00:11:50,700
最后我非常欢迎大家去阅读一下我这里面提到过的一些论文 

249
00:11:50,700 --> 00:11:55,700
这里面这些论文呢也是比较新的一些关于量化相关的一些论文 

250
00:11:55,775 --> 00:11:58,700
好了今天的内容呢就到这里为止 

251
00:11:58,700 --> 00:11:58,775
谢谢各位

252
00:11:58,775 --> 00:11:59,700
谢谢各位

253
00:11:59,700 --> 00:12:01,700
卷的不行了卷的不行了 

254
00:12:01,700 --> 00:12:02,700
记得一键三连加关注哦

255
00:12:02,700 --> 00:12:06,700
所有的内容都会开源在下面这条链接里面 

256
00:12:06,700 --> 00:12:07,700
拜了个拜

