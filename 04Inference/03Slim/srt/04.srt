1
00:00:00,000 --> 00:00:04,560
字幕生成：Galaxies 字幕校对：Not_Ur_77

2
00:00:04,560 --> 00:00:08,640
好辣呀好辣呀 今天吃了火锅特别的辣 

3
00:00:08,640 --> 00:00:11,400
那至于辣哪里呢 大家想想就知道了 

4
00:00:11,400 --> 00:00:16,400
我是 ZOMI 今天呢是在推理引擎里面的模型压缩 

5
00:00:16,400 --> 00:00:18,320
训练后量化就是 PTQ

6
00:00:18,320 --> 00:00:19,960
还有量化完之后呢

7
00:00:19,960 --> 00:00:23,720
怎么把这些量化后的网络模型进行部署起来

8
00:00:25,040 --> 00:00:26,960
ZOMI 讲话的语速呢会稍微快了一点

9
00:00:26,960 --> 00:00:28,840
所以大家有不懂的或者我讲不明白的

10
00:00:28,840 --> 00:00:31,800
也欢迎大家去给我私信和弹幕留言

11
00:00:31,800 --> 00:00:34,480
现在呢来到这个内容

12
00:00:34,480 --> 00:00:38,760
Post Training Quantization 训练后量化又叫做 PTQ 

13
00:00:38,760 --> 00:00:42,440
还有量化之后怎么去把它真正部署起来

14
00:00:42,440 --> 00:00:44,160
两个比较重要的内容

15
00:00:45,800 --> 00:00:49,000
首先第一个内容就是训练后量化 PTQ

16
00:00:49,000 --> 00:00:52,440
实际上呢 PTQ 它分为 Static 和 Dynamic

17
00:00:52,440 --> 00:00:55,120
两个一个是静态一个是动态

18
00:00:55,800 --> 00:01:00,960
先看一个比较简单一点的 就是动态离线量化 PTQ Dynamic 

19
00:01:01,800 --> 00:01:03,320
PTQ Dynamic 其实非常简单

20
00:01:03,320 --> 00:01:05,200
也是 ZOMI 做的第一个量化的项目

21
00:01:05,680 --> 00:01:08,960
简单的来说就是把网络模型权重呢

22
00:01:08,960 --> 00:01:14,440
直接从 FP32 简单的映射成为 INT8,INT16 这种方式 

23
00:01:15,720 --> 00:01:19,160
最重要的目的呢就是减少网络模型的大小

24
00:01:19,160 --> 00:01:20,760
为什么叫做动态呢

25
00:01:20,760 --> 00:01:24,640
是因为缩放因子 Scale 是一个动态去计算出来的 

26
00:01:25,640 --> 00:01:29,080
因此这种量化方式呢 是几种量化方式里面性能最差的 

27
00:01:30,280 --> 00:01:32,480
看一下一个简单的流程

28
00:01:33,200 --> 00:01:36,320
PTQ Dynamic 的算法流程呢 主要有三个模块 

29
00:01:36,320 --> 00:01:39,000
第一个就是拿到一个已经训练好的网络模型了

30
00:01:39,960 --> 00:01:43,280
接着把这个网络模型的 FP32 的网络模型的权重

31
00:01:43,280 --> 00:01:47,000
直接转换成为 INT8 这种量化的模型

32
00:01:47,000 --> 00:01:50,200
那最后呢就输出一个已经转换成为权重

33
00:01:50,200 --> 00:01:53,400
转换成为 INT8 的量化的模型对外进行输出

34
00:01:55,640 --> 00:01:57,920
接下来来到第二个内容

35
00:01:57,920 --> 00:02:00,080
那这个内容呢还是有点意思的

36
00:02:00,080 --> 00:02:04,400
像华为昇腾里面的推理引擎 ACL

37
00:02:04,400 --> 00:02:08,160
还有英伟达的 TensorRT 里面的量化模块呢

38
00:02:08,160 --> 00:02:11,920
都是采用了静态离线量化 PTQ Static

39
00:02:12,920 --> 00:02:15,760
现在看一下这种量化方式呢有什么不一样啊

40
00:02:15,760 --> 00:02:18,720
现在大部分的推理引擎或者推理框架

41
00:02:18,720 --> 00:02:21,840
都会采用离线量化的这种方式

42
00:02:21,840 --> 00:02:23,400
作为里面集成的一个模块

43
00:02:23,440 --> 00:02:26,480
所以这个模块呢稍微简单的展开一下

44
00:02:26,480 --> 00:02:29,280
那这个静态的离线量化同时叫做

45
00:02:29,280 --> 00:02:31,880
校正量化或者数据集量化

46
00:02:31,880 --> 00:02:35,920
因为里面使用了少量没有标签的校准数据

47
00:02:35,920 --> 00:02:38,480
所以它叫做校正量化或者数据集量化

48
00:02:38,480 --> 00:02:41,440
统一都理解为静态离线量化就好了

49
00:02:41,440 --> 00:02:44,000
这也是一个学术和官方的一种叫法

50
00:02:45,000 --> 00:02:47,520
而这里面呢使用无标签的校准数据呢

51
00:02:47,520 --> 00:02:50,240
主要是用在去计算 scale

52
00:02:50,680 --> 00:02:53,080
通过真实场景没有标签的校准数据

53
00:02:53,840 --> 00:02:57,000
因为在获取 scale 的时候呢 没有必要去训练或者 fine-tuning

54
00:02:57,000 --> 00:02:59,360
只需要简单的运行一些正向

55
00:02:59,360 --> 00:03:03,280
这个数据呢只需要有一些真实的数据场景就好了

56
00:03:03,280 --> 00:03:05,480
就可以根据真实的数据场景

57
00:03:05,480 --> 00:03:08,840
去获取 scale 缩放因子

58
00:03:09,960 --> 00:03:15,480
在 PTQ Static 静态离线量化里面怎么去获取 scale 呢 就是算法的内核 

59
00:03:15,480 --> 00:03:21,240
里面呢有可以通过 MinMax、KLD、ADMM、EQ 等不同的方式去展开 

60
00:03:22,240 --> 00:03:26,400
那下面呢看一下 PTQ Static 一个算法的主要流程

61
00:03:26,400 --> 00:03:29,720
首先已经有一个已经训练好的网络模型

62
00:03:29,720 --> 00:03:33,040
接着呢获取这个网络模型的计算图

63
00:03:33,040 --> 00:03:36,160
对这个计算图呢插入一些伪量化的算子

64
00:03:36,160 --> 00:03:41,120
那这个插入伪量化的算子 跟刚才在讲感知量化训练的时候的伪量化算子 

65
00:03:41,120 --> 00:03:42,600
意义上是相同的

66
00:03:42,600 --> 00:03:44,560
但是没有训练的流程

67
00:03:44,560 --> 00:03:46,280
只有校正的流程

68
00:03:46,280 --> 00:03:48,520
既然有校正就有校正的数据集

69
00:03:48,520 --> 00:03:51,200
而这个数据集呢不需要带标签

70
00:03:51,200 --> 00:03:53,120
只要从真实的数据场景里面

71
00:03:53,120 --> 00:03:56,200
去获取相关的一个小部分的数据集就好了

72
00:03:56,200 --> 00:03:58,360
然后呢就给到校正算法

73
00:03:58,360 --> 00:04:01,000
真正的去做一个量化的工作

74
00:04:01,000 --> 00:04:05,520
最后呢就输出一个 PTQ Model 的网络模型

75
00:04:05,520 --> 00:04:07,520
就经过量化后的网络模型

76
00:04:07,520 --> 00:04:11,200
然后呢就给推理部署平台进行一个真正的处理

77
00:04:15,200 --> 00:04:16,920
在静态离线量化里面呢

78
00:04:16,920 --> 00:04:19,880
为了更好的去得到 scale

79
00:04:19,880 --> 00:04:21,680
或者去计算数据分布呢

80
00:04:21,680 --> 00:04:24,640
这里面用了一种算法叫做 KL 散度

81
00:04:24,640 --> 00:04:27,000
用 KL 散度去校准数据集

82
00:04:27,000 --> 00:04:30,640
那这里面呢先看看 KL 散度的一个原理哦

83
00:04:30,640 --> 00:04:33,680
首先呢 KL 散度呢它也叫做相对熵

84
00:04:33,680 --> 00:04:37,360
这里面这套公式呢就是 KL 散度的真实的公式

85
00:04:37,360 --> 00:04:40,680
主要是去对比两个分布之间的差异

86
00:04:40,680 --> 00:04:42,600
其中 P 呢就是真实的分布

87
00:04:42,600 --> 00:04:44,360
而 Q 呢就是预测的分布

88
00:04:44,360 --> 00:04:49,280
KL 散度呢就去对比真实的分布 跟预测的分布之间的一个近似的值 

89
00:04:49,800 --> 00:04:51,200
或者它们的差异的大小

90
00:04:51,200 --> 00:04:53,560
当然了它们的差异越小越好

91
00:04:54,320 --> 00:04:56,440
诶有了这个原理之后

92
00:04:56,440 --> 00:04:58,240
那事情就变得更加简单了

93
00:04:58,240 --> 00:05:01,120
看一下 KL 散度的一个具体的算法流程

94
00:05:03,480 --> 00:05:06,160
第一步需要准备一些带校准的数据集

95
00:05:06,160 --> 00:05:07,960
那这些数据呢不需要有标签

96
00:05:07,960 --> 00:05:10,520
从真实的场景或者验证集里面

97
00:05:10,520 --> 00:05:12,480
去选择一小部分就可以了

98
00:05:13,320 --> 00:05:16,760
然后做一个正向的就是 FP32 的一个推理

99
00:05:16,760 --> 00:05:17,560
大家要注意哦

100
00:05:17,560 --> 00:05:19,720
是 FP32 没有经过任何量化的

101
00:05:19,720 --> 00:05:22,520
下面的这个 For 的步骤呢才是真正的量化

102
00:05:22,520 --> 00:05:24,360
那在 FP32 推理的时候呢

103
00:05:24,360 --> 00:05:27,800
就会对每一层做一个不同的工作

104
00:05:27,800 --> 00:05:29,200
针对每一层都要做

105
00:05:29,960 --> 00:05:32,400
那这里面呢需要去收集

106
00:05:32,400 --> 00:05:35,040
每一层的激活值的一个值方图

107
00:05:35,040 --> 00:05:38,280
就获取数据输出的一个分布

108
00:05:39,240 --> 00:05:40,760
在这里面的第二步

109
00:05:40,760 --> 00:05:43,160
就是需要去设置几个阈值

110
00:05:43,160 --> 00:05:46,600
通过不同的阈值呢去获取量化后的一个分布

111
00:05:47,120 --> 00:05:50,280
这里面呢设计了很多不同的阈值数据

112
00:05:50,280 --> 00:05:52,600
所以会得到很多不同的量化的分布

113
00:05:54,080 --> 00:05:57,240
在最后一步呢就真正的去计算 KL 散度了

114
00:05:58,360 --> 00:06:02,600
而 KL 散度的两个数值就是刚才的第一步 获取真实的数据 

115
00:06:03,160 --> 00:06:06,800
第二步呢就是通过阈值去产生 不同的量化后的分布 

116
00:06:06,800 --> 00:06:08,200
通过比较这两步

117
00:06:08,200 --> 00:06:10,040
然后得到一个最小值

118
00:06:10,840 --> 00:06:15,040
原始数据的分布跟量化后的数据的分布 是比较相似的 

119
00:06:15,080 --> 00:06:19,400
那这个具体的域子和相关的参数呢 就作为最优的参数值 

120
00:06:19,400 --> 00:06:22,440
那这里面有几个点需要去注意一下的

121
00:06:22,440 --> 00:06:24,920
就是需要准备一些小批量的数据

122
00:06:24,920 --> 00:06:28,000
也就是刚才所说的 Calibration Dataset

123
00:06:28,000 --> 00:06:30,520
需要去校准的数据集

124
00:06:30,520 --> 00:06:34,680
这里面的一般都会采用 500 到 1000 张图片左右

125
00:06:34,680 --> 00:06:35,960
就够了不用太多

126
00:06:36,600 --> 00:06:38,240
ZOMI 之前在一个项目里面

127
00:06:38,240 --> 00:06:39,720
采用了 1 万多张数据集

128
00:06:39,720 --> 00:06:43,080
后来发现其实跟 500 多张数据集差异不太大

129
00:06:43,080 --> 00:06:45,000
所以大家不用浪费太多的时间

130
00:06:45,000 --> 00:06:47,240
去调多少张不同的数据集

131
00:06:47,240 --> 00:06:49,680
更多的可以去调一调第二步

132
00:06:49,680 --> 00:06:51,240
通过什么阈值什么参数

133
00:06:51,240 --> 00:06:53,200
去产生不同的数据的分布

134
00:06:56,040 --> 00:06:59,880
下面这段伪代码就是英伟达 TensorRT 的一个关于 KL 散度

135
00:06:59,880 --> 00:07:03,920
或者静态量化后训练的一个具体的伪代码

136
00:07:03,920 --> 00:07:05,160
就不一一解析了

137
00:07:05,760 --> 00:07:10,520
有兴趣的同学可以上 ZOMI 的 Github 里面去获取相关的资料

138
00:07:11,520 --> 00:07:15,320
接着来到第二个比较重要的内容

139
00:07:15,320 --> 00:07:17,320
就是端测量化的推理部署

140
00:07:17,320 --> 00:07:19,880
刚才讲了很多种不同的量化方式

141
00:07:19,880 --> 00:07:24,800
但是实际上真正的量化方式应该是怎么样的呢

142
00:07:31,960 --> 00:07:34,920
我量化完之后真正的要去推理部署了

143
00:07:34,920 --> 00:07:36,600
那在端测量化推理部署里面

144
00:07:36,600 --> 00:07:39,640
其实非常非常的讲究

145
00:07:39,920 --> 00:07:41,640
在真正推理部署的时候

146
00:07:41,840 --> 00:07:43,680
其实有很多种方式

147
00:07:43,880 --> 00:07:46,880
下面展开三个图有三种方式

148
00:07:46,880 --> 00:07:48,520
第一种就是我的输入

149
00:07:48,520 --> 00:07:52,000
假设这个输入它不是实际上的网络模型的输入

150
00:07:52,760 --> 00:07:55,000
假设我输入的是 FP32

151
00:07:55,000 --> 00:07:57,840
接着我这个算子是一个 INT8 的算子

152
00:07:57,840 --> 00:08:01,040
我这个卷积算子对应的参数也是 INT8

153
00:08:01,040 --> 00:08:04,520
这个时候我对输入的时候就需要做一个量化

154
00:08:04,960 --> 00:08:07,920
把输进去的 FP32 的数据量化成 INT8

155
00:08:08,040 --> 00:08:10,080
接着去用 INT8 进行计算

156
00:08:10,080 --> 00:08:13,600
计算完之后数据输出来肯定是 int32 的

157
00:08:14,600 --> 00:08:17,920
假设算子是一个卷积 卷积的输入是 INT8 

158
00:08:17,920 --> 00:08:19,240
它的权重也是 INT8

159
00:08:19,240 --> 00:08:21,560
如果我的输出都是 INT8 的话

160
00:08:21,560 --> 00:08:23,000
它就会造成一个溢出

161
00:08:23,000 --> 00:08:25,800
255×255 肯定超过 255

162
00:08:25,800 --> 00:08:28,160
一般的输出会变成一个 INT32

163
00:08:28,160 --> 00:08:30,440
这个时候需要 dequantization

164
00:08:30,440 --> 00:08:33,960
就是反量化回 FP32 再给下一层输入

165
00:08:34,880 --> 00:08:38,080
另外看一下第二种方式

166
00:08:38,560 --> 00:08:43,360
第二种方式它的输入是 FP32 输出是 INT8 

167
00:08:43,360 --> 00:08:46,520
看一下里面的一个具体的内容

168
00:08:47,760 --> 00:08:49,400
FP32 输进去

169
00:08:49,400 --> 00:08:52,640
肯定需要进行一个量化回 INT8

170
00:08:52,640 --> 00:08:55,360
接着我需要去做一个具体的计算

171
00:08:55,360 --> 00:08:57,760
计算完之后还是变成 INT32

172
00:08:58,240 --> 00:09:01,640
INT32 之后会做一个重量化

173
00:09:01,640 --> 00:09:06,440
把 INT32 的数据量化成 INT8

174
00:09:06,440 --> 00:09:08,920
然后下一个算子如果也是一个卷积

175
00:09:08,920 --> 00:09:12,200
这个时候就直接串起来就行了

176
00:09:12,200 --> 00:09:14,920
所以引申成为第三种方式

177
00:09:14,920 --> 00:09:17,080
假设这个数据的输出

178
00:09:17,080 --> 00:09:20,680
是第三个数据算子的输入

179
00:09:20,680 --> 00:09:22,440
我输的是 INT8

180
00:09:22,440 --> 00:09:26,320
卷积算子计算之后输出是 INT32

181
00:09:26,320 --> 00:09:30,760
然后再做一个重量化变成 INT8 再输出

182
00:09:30,760 --> 00:09:32,760
所以在整个计算图

183
00:09:32,760 --> 00:09:35,320
在整个网络模型的过程当中

184
00:09:35,320 --> 00:09:38,320
会遇到三种不同的方式

185
00:09:40,320 --> 00:09:44,280
下面这个图我把这三种不同的方式串起来

186
00:09:44,280 --> 00:09:48,560
网络模型的数据的输进去 肯定是 FP32 的 

187
00:09:48,560 --> 00:09:51,200
于是就会经过一个量化

188
00:09:51,200 --> 00:09:53,240
接着就会插入一个量化的算子

189
00:09:53,240 --> 00:09:55,040
对这个数据进行一个量化

190
00:09:55,040 --> 00:09:57,800
然后真正的去做一些卷积

191
00:09:57,800 --> 00:10:00,280
GMM 的计算做一个重量化

192
00:10:00,280 --> 00:10:02,760
下次给下一个卷集算子

193
00:10:02,760 --> 00:10:04,440
接着再做一个重量化

194
00:10:04,440 --> 00:10:06,200
给下个卷集的算子

195
00:10:06,200 --> 00:10:08,680
在网络模型真正的输出之前

196
00:10:08,680 --> 00:10:11,640
肯定需要有一个反量化的

197
00:10:11,640 --> 00:10:14,440
所以量化和反量化这两个算子

198
00:10:14,440 --> 00:10:16,200
一定会有重量化

199
00:10:16,200 --> 00:10:18,280
会不会有需要决定于

200
00:10:18,280 --> 00:10:20,280
网络模型的具体的形态

201
00:10:22,200 --> 00:10:25,320
大家有没有发现一个比较典型的规律

202
00:10:25,320 --> 00:10:27,760
在端测量化推理部署的时候

203
00:10:27,800 --> 00:10:30,960
会多了几个不同的算子

204
00:10:30,960 --> 00:10:33,640
这些算子都是实时机器的算子

205
00:10:33,640 --> 00:10:35,640
一个就是量化的算子

206
00:10:35,640 --> 00:10:37,440
一个是反量化的算子

207
00:10:37,440 --> 00:10:39,840
还有重量化的算子

208
00:10:39,840 --> 00:10:43,160
这是跟没有量化之前最大的区别

209
00:10:43,160 --> 00:10:45,120
需要根据这些算子

210
00:10:45,120 --> 00:10:47,680
去计算量化的公式

211
00:10:49,000 --> 00:10:50,480
第一个就是量化

212
00:10:50,480 --> 00:10:53,880
需要把 FP32 的数据量化成 INT8

213
00:10:53,880 --> 00:10:55,880
在离线转换工具转换的时候

214
00:10:55,880 --> 00:10:57,440
就需要根据刚才

215
00:10:57,880 --> 00:11:00,200
不管是感知量化训练还是训练后量化

216
00:11:00,200 --> 00:11:02,360
主要是找到 X 的 max X 的 min 

217
00:11:02,360 --> 00:11:05,600
g 的分布 就是 Qmax Qmin 也需要 

218
00:11:05,600 --> 00:11:08,600
找到数据的分布呢 离线转换工具的时候 

219
00:11:08,600 --> 00:11:10,880
就会去计算 scale 还有 offset

220
00:11:10,880 --> 00:11:12,400
这两个数据很重要

221
00:11:12,400 --> 00:11:15,720
在端测推理部署的时候 就真正 runtime 去运行的时候 

222
00:11:15,720 --> 00:11:19,920
就会根据离线转换工具得到的一个 scale 跟 offset

223
00:11:19,920 --> 00:11:22,320
把 FP32 的数据转成 INT8

224
00:11:22,320 --> 00:11:24,760
这个就是量化的具体的公式

225
00:11:24,760 --> 00:11:26,920
量化很简单 计算方式也很简单 

226
00:11:27,040 --> 00:11:30,800
但是 scale 跟 offset 的求取 是一个比较麻烦的事情 

227
00:11:30,800 --> 00:11:32,520
它有很多种求取的方式

228
00:11:35,480 --> 00:11:40,600
接着看一看第二个算子 Dequantize 反量化  

229
00:11:40,600 --> 00:11:44,720
反量化它不是把 INT8 反量化成为 FP32

230
00:11:44,720 --> 00:11:49,080
而是把 INT32 反量化成为 FP32

231
00:11:49,080 --> 00:11:52,160
因为 INT8 不管是相加相乘

232
00:11:52,160 --> 00:11:53,520
它肯定会溢出的

233
00:11:53,520 --> 00:11:56,520
所以需要用 INT32 的格式进行存储

234
00:11:57,240 --> 00:12:00,800
可以看到下面的公式就变得非常的复杂

235
00:12:00,800 --> 00:12:04,160
虽然看上去很复杂 可以看到一般的卷积的计算 

236
00:12:04,160 --> 00:12:07,680
就是 X 乘以 W 这个是我推导的公式 

237
00:12:07,680 --> 00:12:11,160
然后推导完就可以得到 Xscale 然后乘以 Wscale 

238
00:12:11,160 --> 00:12:12,880
然后再乘以 INT32 的 result

239
00:12:12,880 --> 00:12:15,800
就得到反量化 y 的结果了

240
00:12:15,800 --> 00:12:18,000
这里面也非常欢迎大家自己去推理一下

241
00:12:19,640 --> 00:12:22,560
最后一个就是重量化

242
00:12:22,560 --> 00:12:26,800
把 INT32 的数据重量化为 INT8

243
00:12:26,800 --> 00:12:29,640
重量化的推导公式就如下面这条公式所示

244
00:12:29,640 --> 00:12:31,800
也是我慢慢的去推导的

245
00:12:31,800 --> 00:12:34,200
但是有一个点值得注意的

246
00:12:34,200 --> 00:12:35,440
在计算公式的时候

247
00:12:35,440 --> 00:12:38,080
不仅仅需要当前算子

248
00:12:38,080 --> 00:12:40,200
输入 input 或者权重的 scale

249
00:12:40,200 --> 00:12:42,920
更加也需要下一个 op

250
00:12:42,920 --> 00:12:46,640
就下一个算子的输入的一个 scale 和 offset

251
00:12:46,640 --> 00:12:48,680
因此在运行量化推理的过程当中

252
00:12:48,680 --> 00:12:51,680
确实需要到全图的信息

253
00:12:51,680 --> 00:12:53,680
也就是整个计算图的信息

254
00:12:57,800 --> 00:13:00,840
下面在正式结束之前

255
00:13:00,840 --> 00:13:01,960
我想提几个疑问

256
00:13:01,960 --> 00:13:04,080
也想引起大家的一个思考

257
00:13:04,080 --> 00:13:10,360
为什么模型量化技术能够对实际的部署场景起到加速的作用 

258
00:13:10,360 --> 00:13:12,560
这个其实之前已经讲过了

259
00:13:12,560 --> 00:13:15,080
也希望大家去思考回顾一下

260
00:13:15,080 --> 00:13:20,120
第二点就是为什么需要对网络模型进行量化压缩 

261
00:13:20,120 --> 00:13:23,120
也就是量化压缩到底有什么好处

262
00:13:23,120 --> 00:13:29,600
第三点就是为什么不直接训练低精度的网络模型 

263
00:13:29,600 --> 00:13:32,760
直接训练一个 INT8 的网络模型可不可以呢 

264
00:13:32,760 --> 00:13:36,560
直接训练一个二值化的网络模型可不可以呢 

265
00:13:36,560 --> 00:13:40,360
针对大模型它有千亿百亿万亿规模

266
00:13:40,360 --> 00:13:42,640
我为什么要训练一个万亿规模的大模型

267
00:13:42,640 --> 00:13:47,040
我直接训练一个十亿规模的小模型不就好了吗 

268
00:13:47,040 --> 00:13:51,000
最后一个问题就是在什么情况下不应该

269
00:13:51,000 --> 00:13:54,880
或者在什么情况下应该使用模型量化的技术

270
00:13:54,920 --> 00:13:58,840
这个问题也是我在第一节里面去给大家提问过的 

271
00:13:58,840 --> 00:14:02,160
也希望大家去思考思考

272
00:14:02,160 --> 00:14:04,600
好了谢谢各位 拜了个拜

