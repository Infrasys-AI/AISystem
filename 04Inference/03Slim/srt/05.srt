1
00:00:00,000 --> 00:00:06,025
字幕生成：Galaxies 字幕校对：Not_Ur_77

2
00:00:06,025 --> 00:00:09,050
Hi 大家好，我是 ZOMI

3
00:00:09,050 --> 00:00:12,830
如果大家觉得我的视频里面有讲解的不明白的地方

4
00:00:12,830 --> 00:00:14,878
或者讲解的比较含糊的地方呢

5
00:00:14,878 --> 00:00:19,294
也非常欢迎各位好哥哥和小姐姐呢给我留言和评论弹幕

6
00:00:19,294 --> 00:00:23,774
ZOMI 呢会根据相对的内容来进行一个更新

7
00:00:23,774 --> 00:00:25,774
废话就不多说了

8
00:00:25,774 --> 00:00:31,343
来到推定引擎模型压缩里面的模型剪枝 Pruning

9
00:00:33,135 --> 00:00:35,733
那剪枝这个工作呢还是很有意思的

10
00:00:35,733 --> 00:00:39,973
看一下这个内容里面我要给大家汇报哪些点

11
00:00:39,973 --> 00:00:43,765
那首先呢看一下剪枝跟量化具体有什么区别

12
00:00:43,765 --> 00:00:47,733
接着呢去了解一下剪枝算法的分类

13
00:00:47,733 --> 00:00:50,797
因为剪枝确实有很多种不同的剪法

14
00:00:50,797 --> 00:00:55,259
在第三个内容就去看看剪枝的流程

15
00:00:55,259 --> 00:00:58,857
根据不同的剪枝算法呢去提出对应的剪枝的流程

16
00:00:58,857 --> 00:01:05,193
最后呢以一个最开始的或者现在已经最成熟的剪枝算法作为例子

17
00:01:05,193 --> 00:01:09,904
就是 L1-norm 剪枝这个算法呢去展开或者作为最后的结束

18
00:01:12,969 --> 00:01:15,225
现在正式的来到第一个内容

19
00:01:15,225 --> 00:01:16,752
剪枝和量化有什么区别

20
00:01:16,752 --> 00:01:22,128
因为在上一节内容里面其实充分的去给大家汇报了量化所相关的知识点

21
00:01:22,128 --> 00:01:24,128
接着呢去看一下剪枝

22
00:01:24,128 --> 00:01:29,218
那现在主要是看一下剪枝和量化具体或者本质的区别在哪

23
00:01:29,218 --> 00:01:32,610
首先看一个模型压缩的概念

24
00:01:32,610 --> 00:01:36,191
模型压缩实际上呢主要是对三个部分进行优化

25
00:01:36,191 --> 00:01:40,175
第一个点就是减少密集的内存访问量

26
00:01:40,175 --> 00:01:43,450
也就是减少跟内存的访问的次数

27
00:01:43,450 --> 00:01:45,177
访问的越少越好

28
00:01:45,177 --> 00:01:47,636
因为访问内存确实太耗时间了

29
00:01:47,636 --> 00:01:51,748
那第二个呢就是提高获取模型参数的时间

30
00:01:51,748 --> 00:01:56,075
这个点说白了就是模型的参数越少越好

31
00:01:56,075 --> 00:01:58,075
模型越小越好

32
00:01:58,075 --> 00:02:00,778
第三点就是加速模型推理的时间

33
00:02:00,778 --> 00:02:05,514
在推理的情况下呢真正的能够把时延提上去

34
00:02:05,514 --> 00:02:07,242
不管是剪枝

35
00:02:07,242 --> 00:02:09,242
不管是剪枝还是量化

36
00:02:09,242 --> 00:02:11,242
其实它属于模型压缩的一部分,

37
00:02:11,242 --> 00:02:16,858
而所做的工作呢都是围绕着下面三个点进行优化的

38
00:02:19,614 --> 00:02:22,766
具体量化和剪枝的区别呢看一下

39
00:02:22,766 --> 00:02:26,550
下面这个图呢左边的这个是原始的网络模型的数据

40
00:02:26,550 --> 00:02:30,134
存的或者训练的时候呢是 FP32 进行存储的

41
00:02:30,134 --> 00:02:32,134
但是呢经过量化之后呢

42
00:02:32,134 --> 00:02:35,689
存储的数据呢就变成 8bit 或者 int8

43
00:02:35,689 --> 00:02:38,874
在实际网络模型量化推理的时候呢

44
00:02:38,874 --> 00:02:40,849
如果硬件支持 8bit 运算的指令集呢

45
00:02:40,849 --> 00:02:46,975
那这个时候呢 8bit 或者 int8 的这个数据呢就可以真正的在硬件上面部署起来

46
00:02:48,100 --> 00:02:50,532
原来的网络模型有多少参数量,,

47
00:02:50,532 --> 00:02:53,860
量化之后它仍然有多少参数量

48
00:02:53,860 --> 00:02:56,350
下面呢看一下模型的剪枝

49
00:02:56,350 --> 00:03:02,110
模型剪枝呢左边这个呢就是原始训练过后或者发推理过后的一个网络模型的一些参数量

50
00:03:02,110 --> 00:03:09,741
经过剪枝之后呢就会把一些不重要的或者冗余的非关键的权重或者数据呢把它剪掉

51
00:03:09,741 --> 00:03:12,493
剩下一些比较重要的数据

52
00:03:12,493 --> 00:03:14,493
那这个呢就是剪枝的原理

53
00:03:14,493 --> 00:03:17,869
可以看到呢剪枝跟量化是完全不一样的

54
00:03:18,750 --> 00:03:22,416
通过这两张图可以看到虽然剪枝和量化的目标是一样的

55
00:03:22,416 --> 00:03:25,202
但是他们的优化手段是不一样的哦

56
00:03:27,854 --> 00:03:32,189
ZOMI 就发现谷歌写了一篇比较有意思的文章 To prune or not to prune

57
00:03:32,189 --> 00:03:35,453
确实谷歌在 AI 这方面的研究啊非常的深厚

58
00:03:35,453 --> 00:03:40,850
这篇文章呢确实我觉得可以作为一个 prune 或者剪枝的一个白皮书去看

59
00:03:40,850 --> 00:03:43,666
也非常欢迎大家去阅读一下这篇论文

60
00:03:43,666 --> 00:03:46,482
那这篇论文呢我总结了几个观点,

61
00:03:46,482 --> 00:03:49,125
第一个就是在内存占用相同的情况下

62
00:03:49,125 --> 00:03:52,193
实际上啊一些又大又稀疏的网络模型

63
00:03:52,193 --> 00:03:57,473
会比一些小的密集的网络模型能够实现更好的精度

64
00:03:57,473 --> 00:04:00,360
所以说一般呢不希望他训练一个小模型

65
00:04:00,360 --> 00:04:02,360
而是训练一个大模型

66
00:04:02,360 --> 00:04:08,314
接着呢执行一个剪枝确实会比直接训练一个小模型呢更加有效

67
00:04:08,314 --> 00:04:12,509
这也是对应于上一个视频所分享里面提出的一个疑问,

68
00:04:14,200 --> 00:04:17,125
第二点呢就是经过剪枝之后的一个稀疏模型呢

69
00:04:17,125 --> 00:04:22,072
确实要由于同体积就相同大小的一个非稀疏的网络模型

70
00:04:22,072 --> 00:04:24,248
那第二点就充分的说明

71
00:04:24,248 --> 00:04:28,825
其实网络模型有很多的参数其实不一定是需要的

72
00:04:28,825 --> 00:04:30,825
它可能没有用

73
00:04:31,913 --> 00:04:34,465
第三点就是在资源受限的情况下

74
00:04:34,465 --> 00:04:39,825
剪枝呢属于一种比较高效或者比较有效的模型的压缩的策略和方法,

75
00:04:41,125 --> 00:04:43,749
我更多呢是关注于 123 条

76
00:04:43,749 --> 00:04:48,234
也是这篇文章啊做了大量的消融实验之后得到的一个结论

77
00:04:48,234 --> 00:04:49,898
也非常欢迎大家去看看

78
00:04:51,359 --> 00:04:53,950
下面来看一个第二个比较重要的内容、

79
00:04:53,950 --> 00:04:56,840
剪枝算法的分类

80
00:04:57,690 --> 00:05:01,018
下面呢我总结了或者我 review 了

81
00:05:01,018 --> 00:05:03,605
我看了很多的剪枝的算法了

82
00:05:03,605 --> 00:05:05,845
这里面呢其实不要求大家也看那么多

83
00:05:05,845 --> 00:05:07,637
看了这么多算法之后呢

84
00:05:07,637 --> 00:05:10,368
我总结了几个方向

85
00:05:10,368 --> 00:05:12,617
就是剪枝算法的分类

86
00:05:12,617 --> 00:05:15,368
那剪枝算法呢主要有两大类别

87
00:05:15,368 --> 00:05:20,000
一个是左边的非结构化的剪枝 Unstructured Pruning

88
00:05:20,000 --> 00:05:24,288
第二个呢就是结构化的剪枝 Structured Pruning

89
00:05:24,288 --> 00:05:27,000
非结构化的剪枝呢就像左边的这个图

90
00:05:27,000 --> 00:05:30,279
主要是对一些独立的权重或者神经元

91
00:05:30,279 --> 00:05:33,415
再或者一些神经元的链接进行剪枝

92
00:05:33,415 --> 00:05:34,415
就是随机的剪

93
00:05:34,415 --> 00:05:39,225
这也是为啥我说可能我在 16 年 17 年 18 年的时候接触剪枝算法

94
00:05:39,225 --> 00:05:41,053
他那时候还没有那么成熟

95
00:05:41,053 --> 00:05:43,475
那第二个呢就是结构化的剪枝

96
00:05:43,475 --> 00:05:46,475
右边的这三个更多的是结构化的剪枝

97
00:05:46,475 --> 00:05:48,475
结构化的剪枝就会有规律

98
00:05:48,475 --> 00:05:52,475
有顺序的对神经网络或者计算图进行剪枝

99
00:05:53,225 --> 00:05:55,475
几个比较经典的就是对 filter 进行剪枝

100
00:05:55,475 --> 00:05:57,475
对 channel 进行剪枝

101
00:05:57,475 --> 00:05:59,475
对 layer 进行剪枝

102
00:05:59,475 --> 00:06:02,475
剪枝的维度剪枝的方式不太一样

103
00:06:05,000 --> 00:06:09,000
而两种的剪枝方式呢也有它的利弊

104
00:06:09,000 --> 00:06:12,877
那现在来看一下非结构化剪枝它的一个好处

105
00:06:12,877 --> 00:06:15,592
就是剪枝算法特别的简单

106
00:06:15,592 --> 00:06:18,592
模型的压缩比例确实可以压的非常的高

107
00:06:18,592 --> 00:06:21,139
那它的缺点也是非常明显的

108
00:06:22,000 --> 00:06:25,426
第一个比较明显的问题呢就是精度不可控

109
00:06:25,426 --> 00:06:29,000
精度不可控其实是一般来说不可接受

110
00:06:29,000 --> 00:06:31,000
真的是不可接受的

111
00:06:31,000 --> 00:06:32,470
我花了这么多力气去训练一个好模型

112
00:06:32,470 --> 00:06:33,942
就是为了提高精度嘛

113
00:06:33,942 --> 00:06:36,886
你给我推理的时候精度不可控你玩呢

114
00:06:39,000 --> 00:06:43,763
一般来说啊这个非结构化的剪枝呢很少的去用

115
00:06:43,763 --> 00:06:45,107
而且非结构化的剪枝呢

116
00:06:45,107 --> 00:06:48,344
剪枝后基本上权重呢会极度的稀疏化

117
00:06:48,344 --> 00:06:50,904
没有专用的硬件呢去实现

118
00:06:50,904 --> 00:06:54,904
是很难够真正的去做一个训练推理的加速

119
00:06:54,904 --> 00:06:57,625
那第二个就是结构化的剪枝

120
00:06:57,625 --> 00:06:59,545
结构化的剪枝的好处

121
00:06:59,545 --> 00:07:04,158
主要是大部分算法在 channel 和 layer 层面去做一个剪枝

122
00:07:04,158 --> 00:07:06,526
保留了原始卷积的一个结构化

123
00:07:06,526 --> 00:07:08,190
不需要专用的硬件去实现

124
00:07:08,190 --> 00:07:11,518
而且剪枝算法呀比较有规律比较好学习

125
00:07:11,518 --> 00:07:15,870
那坏处就是剪枝的算法相对来说比较复杂

126
00:07:15,870 --> 00:07:18,423
需要真正的去投进去去了解

127
00:07:20,279 --> 00:07:23,223
下面这个图呢采自于这篇文章

128
00:07:23,223 --> 00:07:25,967
它里面呢就做了很多的消融实验

129
00:07:25,967 --> 00:07:28,364
可以看到大部分的这个是没有剪枝的

130
00:07:28,364 --> 00:07:30,847
Unpruned 之前权重参数的一个统计

131
00:07:30,847 --> 00:07:34,069
可以看到其实大部分的数据都集中在零

132
00:07:34,069 --> 00:07:36,693
零的数据有非常的多

133
00:07:36,693 --> 00:07:38,284
经过结构化的剪枝之后呢

134
00:07:38,284 --> 00:07:41,148
看到呢整个数据的分布啊

135
00:07:41,148 --> 00:07:44,148
确实更加的服从于高斯分布

136
00:07:44,850 --> 00:07:46,850
这也是希望看到的

137
00:07:46,850 --> 00:07:48,424
就不需要网络模型

138
00:07:48,424 --> 00:07:50,424
不要那么多冗余的参数

139
00:07:50,424 --> 00:07:51,850
一大堆冗余的参数

140
00:07:51,850 --> 00:07:53,850
一大堆零存来干嘛

141
00:07:53,850 --> 00:07:55,702
还不省点内存空间

142
00:07:58,373 --> 00:08:00,675
现在呢来到第二个内容了

143
00:08:00,675 --> 00:08:02,675
就是剪枝的流程

144
00:08:02,675 --> 00:08:04,675
看一下剪枝有几个流程

145
00:08:04,675 --> 00:08:06,675
剪枝一般是怎么做的

146
00:08:06,675 --> 00:08:08,675
首先可以看到剪枝啊

147
00:08:08,675 --> 00:08:11,276
一般对剪枝来说有三种常见的算法 

148
00:08:11,675 --> 00:08:14,675
第一个呢就是训练一个模型

149
00:08:14,675 --> 00:08:18,331
先训练一个模型然后呢对这个模型进行剪枝

150
00:08:18,331 --> 00:08:22,579
最后呢对剪枝后的模型呢进行微调

151
00:08:22,579 --> 00:08:26,403
第二种就是在模型训练的过程当中进行剪枝

152
00:08:26,403 --> 00:08:27,403
就边训边减了

153
00:08:27,403 --> 00:08:31,627
减完之后呢对模型再进行一个微调

154
00:08:31,627 --> 00:08:33,547
为啥都会有微调呢

155
00:08:33,547 --> 00:08:35,533
下面会讲到啊大家不要急

156
00:08:35,533 --> 00:08:38,669
第三种就是直接进行剪枝

157
00:08:38,669 --> 00:08:40,319
然后呢从头开始训练

158
00:08:40,319 --> 00:08:43,319
那第三种方式呢其实用的比较少

159
00:08:43,319 --> 00:08:45,125
可以看到刚才的

160
00:08:46,675 --> 00:08:50,387
刚才我给大家介绍的剪枝的三种常见的算法呢

161
00:08:50,387 --> 00:08:52,893
有比较规律的三个内容

162
00:08:52,893 --> 00:08:53,789
第一个就是训练

163
00:08:53,789 --> 00:08:55,197
第二个就是剪枝

164
00:08:55,197 --> 00:08:58,125
第三个就是 fine tuning 微调

165
00:08:58,125 --> 00:09:00,661
三个框框三个作用啊

166
00:09:00,661 --> 00:09:02,325
每个框框都有不同的作用哦,

167
00:09:02,825 --> 00:09:06,511
这里面呢我对剪枝呢就做了一个总结

168
00:09:06,511 --> 00:09:08,778
就是剪枝的主要的单元有三个

169
00:09:08,778 --> 00:09:11,000
一个是训练剪枝微调

170
00:09:11,000 --> 00:09:13,510
它各自起到什么作用

171
00:09:13,510 --> 00:09:14,989
现在来看一看

172
00:09:15,821 --> 00:09:19,277
训练呢作用呢主要是得到最佳的网络模型的性能

173
00:09:19,277 --> 00:09:22,093
用它作为基准就 benchmark

174
00:09:22,093 --> 00:09:25,093
在做一个 fine tuning 或者 pruning 的工作之后呢

175
00:09:25,093 --> 00:09:28,601
需要回顾一下跟训练的时候精度是不是相同的

176
00:09:28,601 --> 00:09:32,144
有没有破坏或极度的降低了模型训练的精度

177
00:09:32,144 --> 00:09:36,000
那第二个工作呢就是真正的 pruningj 剪枝

178
00:09:36,000 --> 00:09:39,700
会根据不同的算法呢对刚才训练的网络模型进行剪枝

179
00:09:40,675 --> 00:09:44,890
调整网络模型的通道数啊权重数啊等其他的参数, 

180
00:09:45,425 --> 00:09:47,962
最后一个单元呢就是微调

181
00:09:47,962 --> 00:09:50,962
需要在原始的数据集呢之上进行微调

182
00:09:50,962 --> 00:09:54,962
那这个微调的模型呢就是经过剪枝后的模型啦

183
00:09:56,089 --> 00:09:59,037
因为经过剪枝所以网络模型的结构变化呢

184
00:09:59,037 --> 00:10:01,745
于是呢通过微调来去恢复

185
00:10:01,745 --> 00:10:05,745
弥补剪枝后所丢失的精度和性能

186
00:10:08,596 --> 00:10:10,596
第一种模式也是最简单的

187
00:10:10,596 --> 00:10:13,596
就是上面这个流程的完全分解

188
00:10:13,596 --> 00:10:16,596
首先有一个已经训练好的网络模型

189
00:10:16,596 --> 00:10:20,596
接着呢对这个网络模型进行各种各样的剪枝

190
00:10:20,596 --> 00:10:21,993
虽然说是各种各样

191
00:10:21,993 --> 00:10:25,257
实际上只有结构化和非结构化两种

192
00:10:25,257 --> 00:10:30,257
然后呢对剪枝后的模型啊进行微调恢复一定的精度

193
00:10:30,257 --> 00:10:34,257
这种方式呢就是最原始最 naive 的模型剪枝的流程

194
00:10:34,257 --> 00:10:38,560
接着看一下另外两种也是用的比较多的

195
00:10:38,560 --> 00:10:42,374
首先呢还是一样拿到一个已经训练好的网络模型

196
00:10:42,374 --> 00:10:46,374
接着会有一个子的网络模型进行采样

197
00:10:46,374 --> 00:10:48,733
可能会有很多个子模型

198
00:10:48,733 --> 00:10:50,909
于是呢对这些子模型进行评估

199
00:10:50,909 --> 00:10:53,097
看哪个模型的精度性能比较好

200
00:10:53,097 --> 00:10:57,622
最后呢选择其中一个对它进行微调恢复网络模型的精度

201
00:10:59,301 --> 00:11:02,886
最后一种就是基于 NAS 自动搜索的方式

202
00:11:02,886 --> 00:11:06,210
那实际上呢在经过啊真正的工程验证当中呢

203
00:11:06,210 --> 00:11:09,346
最后一种更多的是在学术的前沿

204
00:11:09,346 --> 00:11:11,686
但是在工业界确实用的很少

205
00:11:11,686 --> 00:11:14,374
因为基于 NAS 的搜索太消耗资源了

206
00:11:14,374 --> 00:11:15,614
就没钱你训不起来

207
00:11:15,614 --> 00:11:17,374
没钱你剪不起来

208
00:11:17,374 --> 00:11:18,878
没钱你也不敢剪

209
00:11:18,878 --> 00:11:22,678
那看一下这边首先还是训练一个网络模型

210
00:11:22,678 --> 00:11:26,422
然后呢基于大规模的一个搜索算法进行剪枝

211
00:11:26,422 --> 00:11:28,649
有可能呢就是基于 NAS 的搜索方法呢

212
00:11:28,649 --> 00:11:31,529
就是剪完之后剪完之后呢我就不需要微调了直接输出

213
00:11:33,444 --> 00:11:34,828
了解完对应的剪枝算法

214
00:11:34,828 --> 00:11:37,516
现在呢实际的打开一个剪枝算法

215
00:11:37,516 --> 00:11:39,997
来去了解一下它具体怎么实现

216
00:11:39,997 --> 00:11:42,997
就是 L1 normalization,

217
00:11:45,030 --> 00:11:49,896
首先呢要讲一个具体的概念就是 L1-norm based channel pruning

218
00:11:49,896 --> 00:11:51,896
channel purning 就是结构化剪枝

219
00:11:51,896 --> 00:11:55,014
专门针对 channel 进行剪枝的

220
00:11:57,475 --> 00:11:59,607
刚才说了根据 channel 来剪

221
00:11:59,607 --> 00:12:04,607
但是剪的标准剪的法则用什么来约束呢

222
00:12:04,607 --> 00:12:06,607
就是 L1-norm

223
00:12:06,607 --> 00:12:11,204
通过计算 L1 norm 来评价卷积核到底重不重要

224
00:12:11,204 --> 00:12:13,204
如果不重要就把它剪掉

225
00:12:13,204 --> 00:12:18,000
如果 L1 norm 的值呢比较低那就证明这个卷积核这个 channel 呢不重要

226
00:12:18,000 --> 00:12:20,000
于是呢就把它剪掉

227
00:12:21,389 --> 00:12:25,333
基于这个算法原理呢看一下具体的算法步骤

228
00:12:25,333 --> 00:12:27,407
首先需要对每一个卷积核

229
00:12:27,407 --> 00:12:31,183
是每一个计算它的权重的绝对值

230
00:12:31,183 --> 00:12:32,533
那就是这条公式

231
00:12:34,033 --> 00:12:36,017
把每一个卷积核进行计算

232
00:12:36,017 --> 00:12:40,829
然后呢根据卷积核刚才算得到的 Sj 进行排序

233
00:12:40,829 --> 00:12:43,932
排完序之后就知道哪个重要哪个不重要

234
00:12:45,375 --> 00:12:46,791
第三步就是设定一个阈值

235
00:12:46,791 --> 00:12:48,791
对小于这个阈值的卷积核

236
00:12:48,791 --> 00:12:53,000
 还有它对应的 Feature map 进行剪枝剪掉它 

237
00:12:53,925 --> 00:12:55,925
在第四步工程上面很重要

238
00:12:55,925 --> 00:12:59,925
下一个卷积层与刚才减掉的 Feature map 相关的卷积核

239
00:12:59,925 --> 00:13:02,200
也需要把它减掉

240
00:13:02,831 --> 00:13:04,781
这种呢就是有点类似于连带质量关系

241
00:13:06,155 --> 00:13:11,848
最后一步就是对第 i 层和第 i+1 层的新的权重重新被创建

242
00:13:11,848 --> 00:13:14,848
那剩下的权重呢会复制到新的模型当中

243
00:13:14,848 --> 00:13:16,398
重新的执行这些步骤

244
00:13:19,200 --> 00:13:23,400
那最后呢看一下 L1 norm 这篇文章的实际的效果

245
00:13:24,375 --> 00:13:26,756
作者呢做了大量的比对实验

246
00:13:26,756 --> 00:13:29,900
包括 pruning vision 就是剪枝的比例

247
00:13:30,525 --> 00:13:33,528
在剪枝的稀疏率到 60%的前提下呢

248
00:13:33,528 --> 00:13:36,528
基本上呢也能够保持比较好的精度

249
00:13:36,528 --> 00:13:39,528
那这个呢就是 L1 norm 的一个具体的实验的结果

250
00:13:41,750 --> 00:13:44,448
ZOMI 在第一次接触剪枝算法的时候呢

251
00:13:44,448 --> 00:13:47,142
去看一下当时候标杆

252
00:13:47,142 --> 00:13:48,142
应该是 18 年的时候

253
00:13:48,142 --> 00:13:49,806
PyTorch 没有完全起来

254
00:13:49,806 --> 00:13:53,956
Tensoflow 就集成了 L1 norm 这一个剪枝的算法和公式

255
00:13:54,750 --> 00:13:57,668
所以 L1 norm 呢是一个非常经典的剪枝算法

256
00:13:57,668 --> 00:13:59,418
也非常欢迎大家去学习一下

257
00:14:02,600 --> 00:14:05,404
好了最后参考文献相关的论文,

258
00:14:05,404 --> 00:14:08,404
也欢迎大家去阅读一下相关的算法,

259
00:14:08,800 --> 00:14:10,522
谢谢各位拜了拜

260
00:14:12,275 --> 00:14:14,725
卷的不行了卷的不行了

261
00:14:14,725 --> 00:14:16,725
记得一键三连加关注哦

262
00:14:16,725 --> 00:14:18,850
所有的内容都会开源在下面这条链接里面

263
00:14:19,550 --> 00:14:20,375
拜了个拜

