1
00:00:00,000 --> 00:00:04,175
字幕生成：qiaokai  字幕校对：A 是传奇

2
00:00:05,050 --> 00:00:06,240
Hello 大家好

3
00:00:06,240 --> 00:00:09,720
下面的知识估计关注的人或者看的人就确实不多

4
00:00:09,720 --> 00:00:13,680
如果你正在看证明你可能是非常关注这个领域

5
00:00:13,680 --> 00:00:15,520
或者正在做这个领域的

6
00:00:15,520 --> 00:00:18,080
下面来到模型转换

7
00:00:18,080 --> 00:00:21,480
里面最重要的一个模块就是模型的优化

8
00:00:21,480 --> 00:00:24,760
这里面最重要的就是对计算图进行优化

9
00:00:24,760 --> 00:00:28,480
在上一节里面讲了一下怎么去制定一个计算图

10
00:00:28,480 --> 00:00:30,240
然后还有计算图的基本流程

11
00:00:30,240 --> 00:00:35,080
下面来看一下计算图的优化具体的细节内容

12
00:00:35,080 --> 00:00:38,520
就是这里面的 details 计算图优化的详解

13
00:00:38,520 --> 00:00:42,440
在计算图优化其实在上一节里面去给大家普及过

14
00:00:42,440 --> 00:00:47,080
分为 basic, extend,还有 layout 和 memory 三种的优化方式

15
00:00:47,080 --> 00:00:51,280
三种优化方式对应到整个推理引擎的计算流

16
00:00:51,280 --> 00:00:55,960
在预优化的阶段会进行很多代数相关的一些优化和简化

17
00:00:56,000 --> 00:01:01,240
接着在真正优化阶段会更多的结合神经网络的一些知识进行优化

18
00:01:01,240 --> 00:01:04,320
在后优化阶段更多的是对一些数据的格式

19
00:01:04,320 --> 00:01:05,880
内存的布局的重排

20
00:01:05,880 --> 00:01:09,880
还有一些很重要核心的重复的算子的 kernel 进行合并

21
00:01:10,880 --> 00:01:13,640
在正式进入后面硬核的内容当中

22
00:01:13,640 --> 00:01:17,440
我想希望大家去看一看我之前发的一系列的视频

23
00:01:17,440 --> 00:01:19,480
就是 AI 编译器的前端优化

24
00:01:19,480 --> 00:01:23,840
因为后面的很多图优化的一些原理都会在这里面

25
00:01:23,840 --> 00:01:26,200
后面基本上就不会讲原理了

26
00:01:26,200 --> 00:01:29,080
这里面更多的都是一些原理性的知识

27
00:01:29,080 --> 00:01:31,760
后面都是一些非常硬核的具体的内容

28
00:01:32,760 --> 00:01:36,680
注意了不是所有图优化都是基于模板去写的

29
00:01:36,680 --> 00:01:40,440
而是只有推理引擎或者大部分推理引擎都会基于模板来写

30
00:01:40,440 --> 00:01:42,640
在 AI 框架当中它是不一样的

31
00:01:42,640 --> 00:01:44,440
像 AI 框架回顾一下

32
00:01:44,440 --> 00:01:47,200
主要是在 TVM，假设以 TVM 为例子

33
00:01:47,200 --> 00:01:50,320
它的一个算子融合或者它的一个图优化的方式

34
00:01:50,400 --> 00:01:55,640
是创建了通过 AST 把 python 的代码转换成为 TVM 里面的 Relay IR

35
00:01:55,640 --> 00:02:01,160
然后便利这个 Relay IR 或者 relate 数去创建整个 DAG 图

36
00:02:01,160 --> 00:02:05,080
通过 DAG 图用于后面的支配树的分析

37
00:02:05,080 --> 00:02:09,640
有了支配树之后就会应用真正的算子融合的一些算法

38
00:02:09,640 --> 00:02:11,800
去实现计算图的优化

39
00:02:11,800 --> 00:02:16,960
可以看到像 TVM 这种更多的是去发现一些常用的规则

40
00:02:16,960 --> 00:02:19,720
去对计算图进行一个融合优化

41
00:02:21,280 --> 00:02:22,720
ZOMI 老师你好

42
00:02:22,720 --> 00:02:24,240
我有个问题

43
00:02:24,240 --> 00:02:29,320
像你刚才提到像 AI 框架或者 AI 编译器它的图优化

44
00:02:29,320 --> 00:02:34,920
采用基于规则树或者特殊的树的 IR 的方式进行融合优化吗

45
00:02:34,920 --> 00:02:39,160
那为什么推理引擎里面的图优化采用 hard code

46
00:02:39,160 --> 00:02:42,600
硬编码或者模型匹配的方式呢

47
00:02:43,200 --> 00:02:46,080
你问的这个问题非常好

48
00:02:46,080 --> 00:02:48,000
我简单复述理解一下

49
00:02:48,000 --> 00:02:51,360
其实像之前讲到的 AI 编译器或者 AI 框架

50
00:02:51,360 --> 00:02:55,720
更多的做一些计算图的优化是通过编译方式去做的

51
00:02:55,720 --> 00:02:57,800
而推理引擎的图优化

52
00:02:57,800 --> 00:03:01,640
更多的是基于模板匹配或者一些 hard code 的方式去写的

53
00:03:01,640 --> 00:03:04,880
大家都知道通过 hard code 或者模板匹配的方式

54
00:03:04,880 --> 00:03:07,200
确实不能覆盖非常多的场景

55
00:03:07,200 --> 00:03:09,600
只能覆盖一些有用常用的场景

56
00:03:09,600 --> 00:03:11,200
而通过编译的方式

57
00:03:11,320 --> 00:03:14,080
确实可以覆盖很多常规的一些应用

58
00:03:14,080 --> 00:03:15,000
正因为这个原因

59
00:03:15,120 --> 00:03:20,240
在推理引擎大部分都是针对于一些常用的一些模型进行部署

60
00:03:20,240 --> 00:03:22,840
而 AI 框架因为大家用来做创新的

61
00:03:22,840 --> 00:03:25,160
所以更多的去考虑到长尾的问题

62
00:03:25,160 --> 00:03:28,560
而 AI 框架大部分都是在计算服务中心

63
00:03:28,560 --> 00:03:31,240
或者有很强的算力平台上面去执行的

64
00:03:31,240 --> 00:03:33,960
所以说时间对它来说不是说非常重要

65
00:03:33,960 --> 00:03:37,960
可以采取很多 GIT 的编译方式来去提升一些性能

66
00:03:37,960 --> 00:03:41,240
而推理引擎图优化大部分都是离线的

67
00:03:41,240 --> 00:03:43,360
或者叫做 AOT 的方式

68
00:03:43,400 --> 00:03:45,120
进行一些模板匹配或 hard code

69
00:03:45,120 --> 00:03:48,280
更好的去覆盖主要的场景就可以了

70
00:03:48,280 --> 00:03:49,680
这也是它们最大的区别

71
00:03:51,080 --> 00:03:53,880
下面来看看计算图优化的详解

72
00:03:53,880 --> 00:03:57,080
计算图优化详解里面的内容特别的多

73
00:03:57,880 --> 00:04:01,600
像是基础的图优化的内容就特别的多了

74
00:04:01,600 --> 00:04:02,800
包括常量的折叠

75
00:04:02,920 --> 00:04:04,080
冗余节点的消除

76
00:04:04,080 --> 00:04:04,840
算子的融合

77
00:04:04,840 --> 00:04:05,520
算子替换

78
00:04:05,520 --> 00:04:06,800
用算子的前移

79
00:04:06,800 --> 00:04:08,320
会讲非常多的内容

80
00:04:08,320 --> 00:04:10,600
可能里面会分开好几个内容来去讲

81
00:04:10,600 --> 00:04:13,000
更多的是去讲真正的融合规则

82
00:04:13,120 --> 00:04:15,160
而不是去讲为什么要这么融合了

83
00:04:15,160 --> 00:04:17,120
所以说下面的内容会越来越难

84
00:04:17,120 --> 00:04:18,800
或者也越来越难懂

85
00:04:18,800 --> 00:04:20,320
大家去记住就好了

86
00:04:20,320 --> 00:04:21,920
现在看第一个内容

87
00:04:21,920 --> 00:04:24,520
就是 O1 Constant Folding

88
00:04:24,520 --> 00:04:25,760
常量折叠

89
00:04:25,760 --> 00:04:28,560
那常量折叠它其实是编译优化的一个技术

90
00:04:28,840 --> 00:04:31,640
对编译时的常量或者常量的表达式

91
00:04:31,640 --> 00:04:34,280
进行计算来去简化代码的

92
00:04:34,800 --> 00:04:37,160
在计算图里面就可以预先的去确定

93
00:04:37,160 --> 00:04:39,920
输出节点的值替换成常量

94
00:04:39,920 --> 00:04:42,200
就把常量这个折叠隐掉了

95
00:04:42,320 --> 00:04:45,400
然后对计算图进行一些结构简化的操作

96
00:04:45,400 --> 00:04:47,840
下面看一些具体的例子

97
00:04:47,840 --> 00:04:49,680
好像现在有的一些 constant fold

98
00:04:49,760 --> 00:04:51,200
就是常量的折叠

99
00:04:51,200 --> 00:04:52,560
还有 binary 的折叠

100
00:04:52,560 --> 00:04:53,680
看一下具体的图

101
00:04:53,680 --> 00:04:55,760
这里面后面的字我就不简单的读了

102
00:04:56,080 --> 00:04:58,480
像这种我有两个常量输进去

103
00:04:58,480 --> 00:05:00,360
然后有一个 Op1 和 Op2

104
00:05:00,800 --> 00:05:02,080
但是可以看到 Op1

105
00:05:02,200 --> 00:05:04,600
它是接收两个常量作为输入

106
00:05:04,600 --> 00:05:06,560
这些常量在离线的时候

107
00:05:06,840 --> 00:05:08,480
其实可以把它算出来

108
00:05:08,480 --> 00:05:09,360
把它算完之后

109
00:05:09,520 --> 00:05:10,880
作为一个新的常量

110
00:05:10,920 --> 00:05:11,920
输给 Op2

111
00:05:11,920 --> 00:05:14,040
这种就是最常见的常量折叠

112
00:05:14,920 --> 00:05:16,160
推理引擎里面最重要的

113
00:05:16,160 --> 00:05:17,600
或者 ZOMI 之前写过的

114
00:05:17,600 --> 00:05:19,120
这种一个常量折叠的公式

115
00:05:19,640 --> 00:05:21,360
其实是 BN 折叠

116
00:05:21,360 --> 00:05:23,280
BN 折叠也就是利用了这个原理

117
00:05:23,280 --> 00:05:25,080
所以大家简单的理解一下就好了

118
00:05:25,720 --> 00:05:26,880
下面看一下

119
00:05:26,880 --> 00:05:29,120
ExpandDims 的一种折叠方式

120
00:05:30,280 --> 00:05:32,320
当 ExpandDims 的第二个维度

121
00:05:32,320 --> 00:05:34,440
就指定维度的输入的时候是常量

122
00:05:34,440 --> 00:05:37,120
那就可以把它直叠进去参数的方式

123
00:05:37,120 --> 00:05:39,320
放在 ExpandDims 这个算子里面

124
00:05:39,440 --> 00:05:41,240
然后就少了一个算子

125
00:05:41,240 --> 00:05:43,200
因为 constant 它有可能是一个算子

126
00:05:43,200 --> 00:05:46,200
或者有可能是占用内存的一块空间

127
00:05:46,520 --> 00:05:48,440
下面还有 binary 折叠

128
00:05:48,560 --> 00:05:49,320
binary 折叠

129
00:05:49,440 --> 00:05:51,360
其实跟刚才的 ExpandDims 的

130
00:05:51,360 --> 00:05:52,240
折叠差不多

131
00:05:52,240 --> 00:05:54,160
它里面的输入可能是个标量

132
00:05:54,280 --> 00:05:57,600
这时候就把标量变成 binary 的一个参数

133
00:05:57,600 --> 00:05:59,440
然后进行一个计算的

134
00:05:59,600 --> 00:06:00,520
这个时候可以看到

135
00:06:00,520 --> 00:06:01,840
少了一个计算的节点

136
00:06:01,840 --> 00:06:02,920
对计算来说

137
00:06:03,040 --> 00:06:04,560
确实有很多的好处

138
00:06:05,560 --> 00:06:07,080
接着看一下第二个内容

139
00:06:07,080 --> 00:06:08,200
就是计算图的优化

140
00:06:08,200 --> 00:06:10,320
冗余节点的消除

141
00:06:10,320 --> 00:06:11,160
冗余节点的消除

142
00:06:11,160 --> 00:06:12,760
里面这些内容特别的多

143
00:06:12,880 --> 00:06:15,400
就没有用的节点进行消除

144
00:06:15,600 --> 00:06:17,000
这里面分开好几个分类

145
00:06:17,000 --> 00:06:19,000
第一个就是 op 本身没有意义

146
00:06:19,000 --> 00:06:21,600
就这个算子本身是没有意义的

147
00:06:21,600 --> 00:06:23,440
所以就会把它去掉

148
00:06:23,440 --> 00:06:25,720
例如 cast 转换之前的前后类型

149
00:06:25,720 --> 00:06:26,600
都是相同的

150
00:06:26,600 --> 00:06:29,160
concat 只有一个输入的 tensor

151
00:06:29,160 --> 00:06:30,520
还有 NoOp Print

152
00:06:30,520 --> 00:06:31,760
Assert StopGradient

153
00:06:31,760 --> 00:06:32,360
Split

154
00:06:32,440 --> 00:06:35,040
这些算子其实都可以干掉

155
00:06:35,240 --> 00:06:37,200
这个时候就会有一系列的规则

156
00:06:37,200 --> 00:06:38,600
去写一系列的模板

157
00:06:38,600 --> 00:06:40,600
去删掉这些没有用的算子

158
00:06:40,600 --> 00:06:41,680
包括 dropout 这种

159
00:06:41,680 --> 00:06:42,800
在训练的过程当中

160
00:06:42,920 --> 00:06:44,000
去有用的算子

161
00:06:44,000 --> 00:06:44,920
在退集的时候

162
00:06:45,120 --> 00:06:46,480
或者在推理引擎转换的时候

163
00:06:46,920 --> 00:06:48,440
都会把它干掉

164
00:06:48,440 --> 00:06:51,080
那么简单的看几个图

165
00:06:51,080 --> 00:06:53,760
像这里面有一个冗余的算子

166
00:06:53,760 --> 00:06:54,720
输入进来的时候

167
00:06:54,800 --> 00:06:56,640
就会把这个算子干掉

168
00:06:56,840 --> 00:06:59,400
这种算子的输入跟输出

169
00:06:59,400 --> 00:07:01,080
会把上一个算子的输入跟输出

170
00:07:01,080 --> 00:07:02,320
把它连回来

171
00:07:02,520 --> 00:07:05,080
但是有种就是这个算子的输出

172
00:07:05,240 --> 00:07:07,080
对下一个算子是没有意义的

173
00:07:07,280 --> 00:07:10,360
这个时候就会把它切成两个子图

174
00:07:10,360 --> 00:07:12,080
一个子图就是 op1 input

175
00:07:12,080 --> 00:07:15,080
一个子图就是 op2 进行一个具体的计算

176
00:07:16,200 --> 00:07:19,600
第三种情况就是像冗余的算子

177
00:07:19,600 --> 00:07:21,880
它的输入对它来说是没用的

178
00:07:22,160 --> 00:07:24,400
既然这个输入对它来说是没用的

179
00:07:24,400 --> 00:07:26,880
那前面的计算是不是也是没用的

180
00:07:27,360 --> 00:07:28,080
既然是这样

181
00:07:28,080 --> 00:07:30,640
那就会迭代式的去网上

182
00:07:30,640 --> 00:07:33,160
轮循删除往上的节点

183
00:07:33,160 --> 00:07:34,840
只要这个节点的输入没有意义

184
00:07:35,000 --> 00:07:37,280
证明这个节点它是没有意义的

185
00:07:37,280 --> 00:07:39,000
因为它的输出没有人接

186
00:07:39,160 --> 00:07:41,160
这个时候就可以把这个算子干掉

187
00:07:41,160 --> 00:07:42,840
它的算子如果也是这种情况

188
00:07:43,000 --> 00:07:44,560
也会一直轮循

189
00:07:44,560 --> 00:07:46,640
把它往上的算子也干掉

190
00:07:46,640 --> 00:07:49,760
这种就是删除 op 就是这个算子

191
00:07:49,760 --> 00:07:50,920
本身没有意义的算子

192
00:07:50,920 --> 00:07:52,040
它就有三种方式

193
00:07:52,800 --> 00:07:56,040
接下来看一下 op 的参数没有意义

194
00:07:56,040 --> 00:07:58,600
也就是说这个算子其实是有意义的

195
00:07:58,600 --> 00:08:02,400
但是当你设定为具体某些参数的时候

196
00:08:02,400 --> 00:08:03,520
或者某些区别的时候

197
00:08:03,760 --> 00:08:04,720
它就没有意义

198
00:08:04,800 --> 00:08:06,920
举个最典型的例子

199
00:08:06,920 --> 00:08:08,000
就 tensor 的 cast

200
00:08:08,360 --> 00:08:11,000
cast 的算子主要是对数据的排布

201
00:08:11,000 --> 00:08:12,560
进行一个转换的

202
00:08:12,560 --> 00:08:14,400
假设我的输入的参数

203
00:08:14,400 --> 00:08:16,280
等于输出的参数的时候

204
00:08:16,880 --> 00:08:18,040
假设我现在有个数据

205
00:08:18,160 --> 00:08:19,080
NCHW

206
00:08:19,080 --> 00:08:21,640
我把它 cast 成 NCHW

207
00:08:22,080 --> 00:08:23,760
我 cast 前后都是相同的

208
00:08:23,880 --> 00:08:25,000
我干嘛要这个算子

209
00:08:25,320 --> 00:08:27,800
所以就可以把这个算子干掉

210
00:08:28,840 --> 00:08:30,320
当然了还有很多种情况

211
00:08:30,440 --> 00:08:32,280
像是 list 的 index 大等于 0

212
00:08:32,280 --> 00:08:34,920
index_end 等于 channel-1 的时候

213
00:08:35,080 --> 00:08:36,160
这个算子是没有意义的

214
00:08:36,160 --> 00:08:37,280
像 expand 的时候

215
00:08:37,440 --> 00:08:40,320
当输出的 shape 等于输入的 shape 的时候

216
00:08:40,320 --> 00:08:41,520
也是没有意义的

217
00:08:41,760 --> 00:08:43,800
当 pooling 的参数或者滑窗的等于 1 乘 1

218
00:08:43,800 --> 00:08:45,200
它也是没有用的

219
00:08:45,200 --> 00:08:47,200
所以看一下下面的图

220
00:08:48,120 --> 00:08:49,480
假设 cast 的算子

221
00:08:49,480 --> 00:08:51,440
它的 source 等于 destination 的时候

222
00:08:51,640 --> 00:08:52,600
这个算子就没有意义

223
00:08:52,600 --> 00:08:54,640
把 cast 的算子干掉

224
00:08:54,920 --> 00:08:56,320
像这种 ExpandDims 的时候

225
00:08:56,320 --> 00:08:58,720
假设这个 shape 跟输入的 shape 是一样的

226
00:08:58,920 --> 00:09:01,520
就把这个算子干掉

227
00:09:01,840 --> 00:09:02,800
像 pooling 的时候

228
00:09:02,880 --> 00:09:04,480
等于 1 乘 1 也是没有用的

229
00:09:04,480 --> 00:09:06,880
像 start 等于某些特殊特性的时候

230
00:09:06,880 --> 00:09:07,720
也是没有用的

231
00:09:07,720 --> 00:09:09,760
也把这个算子干掉

232
00:09:10,040 --> 00:09:12,240
像这种确实在神经网络里面

233
00:09:12,360 --> 00:09:14,040
会出现大量的冗余节点

234
00:09:14,040 --> 00:09:15,960
而总比在具体的实践当中

235
00:09:16,160 --> 00:09:17,800
就我之前在项目交付的时候

236
00:09:17,960 --> 00:09:19,600
会做过相关的工作

237
00:09:19,600 --> 00:09:21,640
确实也会把这些算子干掉之后

238
00:09:21,800 --> 00:09:23,240
性能提升了非常的多

239
00:09:23,240 --> 00:09:26,200
而且网络模型确实简化了非常的多

240
00:09:26,200 --> 00:09:29,560
另外还有一些 OP 的位置没有意义的

241
00:09:30,080 --> 00:09:31,440
这里面有非常的多

242
00:09:31,600 --> 00:09:34,400
所以大家如果真想了解里面的细节

243
00:09:34,600 --> 00:09:36,720
可以翻看我 Github 上面里面的

244
00:09:36,720 --> 00:09:39,000
关于推理引擎的很多的内容

245
00:09:39,000 --> 00:09:40,640
很多的 slide 就 PPT

246
00:09:40,640 --> 00:09:42,440
我都已经开源开放了

247
00:09:42,440 --> 00:09:43,720
还有一些对应的 video

248
00:09:43,720 --> 00:09:45,000
也会放在这里面

249
00:09:45,920 --> 00:09:47,280
回到话题

250
00:09:47,280 --> 00:09:49,320
这里面就不一一去过了

251
00:09:49,320 --> 00:09:51,760
看一个具体的一些例子

252
00:09:52,000 --> 00:09:53,240
确实非常的多

253
00:09:53,400 --> 00:09:54,680
这里面除了 Flatten 的消除

254
00:09:54,680 --> 00:09:55,680
重复的消除

255
00:09:55,680 --> 00:09:56,760
还有很多

256
00:09:56,960 --> 00:09:59,280
看一些具体的一些图

257
00:09:59,400 --> 00:10:02,360
看图说话确实非常之乐观

258
00:10:02,360 --> 00:10:04,200
像这种 Cast 的算子是没有意义的

259
00:10:04,200 --> 00:10:05,560
就会把它删掉

260
00:10:05,560 --> 00:10:07,320
像 UnSqueeze 的算子时候没有意义

261
00:10:07,320 --> 00:10:08,600
也把它删掉

262
00:10:08,840 --> 00:10:11,040
有时候我的 input 给 OP1 的时候

263
00:10:11,200 --> 00:10:12,480
OP1 的输出

264
00:10:12,480 --> 00:10:13,440
它是没有输出的

265
00:10:13,440 --> 00:10:14,240
没有算子接的

266
00:10:14,360 --> 00:10:15,200
我算来干嘛了

267
00:10:15,640 --> 00:10:16,440
它没有人要

268
00:10:16,560 --> 00:10:20,080
所以这个时候就可以把这条分支给干掉

269
00:10:20,920 --> 00:10:22,560
最后还有就是 Global pooling

270
00:10:22,560 --> 00:10:23,920
它后面接一些 Reshape

271
00:10:23,920 --> 00:10:24,800
或者 Flatten 的时候

272
00:10:24,880 --> 00:10:26,040
其实也是没有意义的

273
00:10:26,040 --> 00:10:28,680
也其实可以把这些算子干掉

274
00:10:28,800 --> 00:10:30,440
所以说干掉的这些算子

275
00:10:30,440 --> 00:10:32,040
可以还有非常多

276
00:10:32,720 --> 00:10:33,840
像冗余节点消除

277
00:10:33,840 --> 00:10:35,880
确实里面写了非常多的 past

278
00:10:36,320 --> 00:10:38,240
后面这两个图其实比较相似

279
00:10:38,240 --> 00:10:39,600
假设我有两个 Reshape

280
00:10:39,600 --> 00:10:41,440
两个 Reshape 都是相反的时候

281
00:10:41,440 --> 00:10:43,640
这个时候就可以把它都干掉

282
00:10:43,640 --> 00:10:45,040
Cast A 到 B

283
00:10:45,040 --> 00:10:46,600
然后 Cast B 到 A

284
00:10:46,800 --> 00:10:49,240
我还不如把这两个算子直接干掉就好了

285
00:10:49,240 --> 00:10:50,440
它的语义相反

286
00:10:50,840 --> 00:10:54,840
这里面就讲到了 OP 前后两个的语义相反

287
00:10:55,080 --> 00:10:57,960
这个时候两个 OP 都可以把它干掉

288
00:10:58,560 --> 00:11:00,880
这里面也有非常多

289
00:11:00,880 --> 00:11:02,520
例如 Squeeze 跟 Expand

290
00:11:02,600 --> 00:11:04,040
它确实语义相反的

291
00:11:04,040 --> 00:11:05,240
还有两个 Cast

292
00:11:05,480 --> 00:11:07,160
它可能也是语义相反的

293
00:11:07,160 --> 00:11:09,360
像量化和反量化

294
00:11:09,360 --> 00:11:10,680
假设它连在一起

295
00:11:10,680 --> 00:11:11,840
也是没有意义的

296
00:11:11,840 --> 00:11:12,920
可以把它删掉

297
00:11:12,920 --> 00:11:15,680
Concat 跟 Slice 也是可以删掉

298
00:11:15,680 --> 00:11:17,400
语义相反的都可以干掉

299
00:11:17,400 --> 00:11:19,400
可以看一下下面的具体的图

300
00:11:19,400 --> 00:11:21,800
像 ExpandDims 就是扩充的维度

301
00:11:21,800 --> 00:11:23,880
Squeeze 就把不同的维度合在一起

302
00:11:23,880 --> 00:11:25,640
这种确实也可以干掉

303
00:11:25,640 --> 00:11:27,040
我一个扩充一个合并

304
00:11:27,160 --> 00:11:28,960
我就干脆啥都不做就行了

305
00:11:28,960 --> 00:11:31,240
当然它里面的参数或者里面的轴

306
00:11:31,680 --> 00:11:32,680
大家要注意一下

307
00:11:32,680 --> 00:11:35,080
就得相对应才行

308
00:11:35,440 --> 00:11:37,040
后面像这种 Concat 跟 Slice

309
00:11:37,640 --> 00:11:39,440
还不如我直接 Slice 就完了

310
00:11:39,440 --> 00:11:42,560
所以说它基本上很多很相似的地方

311
00:11:42,560 --> 00:11:44,160
下面看一下

312
00:11:44,640 --> 00:11:46,640
冗余节点消除的第 4 个内容

313
00:11:46,760 --> 00:11:48,680
冗余节点消除特别的多

314
00:11:48,680 --> 00:11:51,720
第 4 个内容就是公共子图的消除

315
00:11:51,720 --> 00:11:54,800
公共子图就是把一些有大模块的

316
00:11:54,800 --> 00:11:56,320
两个完全相同的子图

317
00:11:56,480 --> 00:11:57,280
把它干掉

318
00:11:57,360 --> 00:11:58,240
看一下这个图

319
00:11:58,240 --> 00:12:00,160
假设有三个输入

320
00:12:00,160 --> 00:12:03,600
三个输入对应的是给 OP1 OP2 OP3 去执行

321
00:12:03,600 --> 00:12:05,280
假设右边又有一个分支

322
00:12:05,280 --> 00:12:07,840
同样给 OP1 OP2 OP3 去执行的时候

323
00:12:07,840 --> 00:12:09,720
这个时候可以把红色

324
00:12:09,720 --> 00:12:11,200
把黄色的这块干掉

325
00:12:11,200 --> 00:12:12,600
黄色我没有色盲

326
00:12:13,240 --> 00:12:14,240
把黄色的这块干掉

327
00:12:14,240 --> 00:12:16,080
就剩下左边的子图了

328
00:12:16,080 --> 00:12:18,160
这种就是公共子图的消除

329
00:12:19,520 --> 00:12:21,920
其实刚才讲了很多算法

330
00:12:21,920 --> 00:12:23,120
怎么去能寻的图

331
00:12:23,120 --> 00:12:25,000
其实大部分都是一些经典的

332
00:12:25,040 --> 00:12:26,680
leetcode 算法的题目

333
00:12:26,680 --> 00:12:27,880
例如公共子图消除

334
00:12:28,000 --> 00:12:29,760
就是寻找公共子树

335
00:12:29,760 --> 00:12:31,000
所以有兴趣的同学

336
00:12:31,160 --> 00:12:33,240
或者你不明白为什么要刷 leetcode

337
00:12:33,240 --> 00:12:35,360
就你发现为什么我写的代码

338
00:12:35,360 --> 00:12:36,480
基本上都跟你关系相关的

339
00:12:36,480 --> 00:12:38,520
为什么都是做树的检索

340
00:12:38,520 --> 00:12:41,960
所以其实这个就是刷 leetcode 的好处

341
00:12:41,960 --> 00:12:44,640
或者为什么很多大厂做面试的时候

342
00:12:44,800 --> 00:12:47,400
也是需要进行一个 leetcode 的面试

343
00:12:47,400 --> 00:12:49,800
这真的是有它的好处和理由的

344
00:12:49,800 --> 00:12:52,000
可能你平时在做一些简单的应用

345
00:12:52,000 --> 00:12:52,640
API 的时候

346
00:12:52,640 --> 00:12:54,160
只是调用人家一些库

347
00:12:54,200 --> 00:12:56,080
你觉得你根本没有必要去写

348
00:12:56,080 --> 00:12:58,200
但是你去写一些内核的代码

349
00:12:58,200 --> 00:13:00,560
或者真正做一些开创性的内容的时候

350
00:13:00,560 --> 00:13:01,360
你就会发现

351
00:13:01,840 --> 00:13:02,720
里面很多知识

352
00:13:02,720 --> 00:13:04,560
真的是可以借鉴过来的

353
00:13:04,920 --> 00:13:06,640
好了今天的内容就这么多

354
00:13:06,640 --> 00:13:08,240
将会在下一节当中

355
00:13:08,240 --> 00:13:12,560
分享更多的图优化的一些算法和内容

356
00:13:12,560 --> 00:13:13,520
谢谢各位

357
00:13:13,520 --> 00:13:14,960
拜了个拜

358
00:13:15,920 --> 00:13:17,600
卷的不行了

359
00:13:17,600 --> 00:13:19,040
记得一键三连加关注

360
00:13:19,440 --> 00:13:20,800
所有的内容都会开源

361
00:13:20,800 --> 00:13:22,600
在下面这条链接里面

362
00:13:22,600 --> 00:13:24,000
拜了个拜

