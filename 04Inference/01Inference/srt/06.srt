1
00:00:00,000 --> 00:00:04,560
字幕生成: BLACK 字幕校对: 方鸿渐

2
00:00:06,840 --> 00:00:09,640
哈喽大家好,我就是 ZOMI

3
00:00:09,640 --> 00:00:13,160
上今天的班,睡昨天的觉的 ZOMI

4
00:00:13,160 --> 00:00:16,760
新冠之后呢,其实我的更新频率变得慢了很多

5
00:00:16,760 --> 00:00:19,480
是因为真的是最近特别的累

6
00:00:20,040 --> 00:00:23,680
现在来到推理系统系列里面的推理引擎

7
00:00:23,680 --> 00:00:25,800
也就是我觉得比较重要的一块

8
00:00:25,800 --> 00:00:28,320
那在推理引擎这里面呢

9
00:00:28,320 --> 00:00:33,000
我主要是介绍整体的架构和推理引擎的工作流程

10
00:00:35,280 --> 00:00:38,560
下面一起来了解一下整体的架构

11
00:00:38,560 --> 00:00:40,040
Architecture

12
00:00:40,040 --> 00:00:41,440
虽然我的英文不太准

13
00:00:41,440 --> 00:00:43,120
但是我还是喜欢练两句

14
00:00:43,120 --> 00:00:45,440
大家就着来听吧

15
00:00:46,680 --> 00:00:49,000
首先看一下右边这个图

16
00:00:49,000 --> 00:00:52,160
就是整个推理引擎的整体的架构

17
00:00:52,160 --> 00:00:54,920
可能每一个推理引擎都有不一样的样子

18
00:00:54,920 --> 00:00:57,400
但是这里面我做了一个整体的抽象

19
00:00:57,440 --> 00:00:58,840
然后去给大家汇报的

20
00:01:00,920 --> 00:01:03,960
在推理引擎架构里面主要分开两大块

21
00:01:04,760 --> 00:01:06,360
第一块就是上面

22
00:01:06,360 --> 00:01:09,720
就我这条黄线往上的蓝色的这一块

23
00:01:09,720 --> 00:01:12,520
叫做优化的阶段

24
00:01:12,520 --> 00:01:16,080
那在优化阶段会做非常多的不同的工作

25
00:01:16,080 --> 00:01:17,680
第一个就是模型的转化

26
00:01:17,680 --> 00:01:18,960
模型的压缩

27
00:01:18,960 --> 00:01:20,600
对外提供一些 API 的接口

28
00:01:20,600 --> 00:01:24,600
还可能会提供一些 Benchmark APP 的 Demo

29
00:01:24,640 --> 00:01:26,160
那后面会展开

30
00:01:26,160 --> 00:01:29,760
往下是运行的阶段

31
00:01:29,760 --> 00:01:31,280
就是整体的推理引擎

32
00:01:31,280 --> 00:01:33,880
真正能干活的一些功能点

33
00:01:33,880 --> 00:01:34,960
包括 Runtime

34
00:01:34,960 --> 00:01:37,880
还有 Kernel 两层是最重要的

35
00:01:39,760 --> 00:01:42,760
下面逐层的去打开看一下

36
00:01:42,760 --> 00:01:45,280
第一个就是还没到这个

37
00:01:45,280 --> 00:01:47,960
我先介绍一下网上的 API 层

38
00:01:47,960 --> 00:01:50,480
不管哪个 AI 框架

39
00:01:50,480 --> 00:01:51,800
包括训练也好

40
00:01:51,800 --> 00:01:52,600
推理也好

41
00:01:52,720 --> 00:01:54,400
面向开发者

42
00:01:54,400 --> 00:01:57,400
都需要提供一些 API 的接口

43
00:01:57,400 --> 00:02:00,320
那可能为了做一些端云统一的时候

44
00:02:00,320 --> 00:02:03,280
会提供一个 Python 的 API 的接口

45
00:02:03,280 --> 00:02:04,760
对于一些云服务

46
00:02:04,760 --> 00:02:06,640
会提供一些 Go 的接口

47
00:02:06,640 --> 00:02:09,120
对于集成一些在相关硬件上面

48
00:02:09,120 --> 00:02:10,880
会提供一些 C++的接口

49
00:02:10,880 --> 00:02:13,120
可能对于一些网页上面的服务

50
00:02:13,120 --> 00:02:14,960
会提供一些 JS 的接口

51
00:02:14,960 --> 00:02:17,040
所以 API 的接口是非常的重要

52
00:02:17,040 --> 00:02:18,200
每一个推理引擎

53
00:02:18,200 --> 00:02:20,560
都会有对应的 API 接口

54
00:02:20,560 --> 00:02:22,240
只是多和少的问题

55
00:02:23,600 --> 00:02:26,760
来到正式的第一个内容

56
00:02:26,760 --> 00:02:29,880
就是模型转换工具

57
00:02:30,960 --> 00:02:32,000
模型转换工具

58
00:02:32,000 --> 00:02:33,400
就是左边的这一块

59
00:02:33,560 --> 00:02:34,560
这里面最重要的

60
00:02:34,560 --> 00:02:36,760
就是模型的格式转换

61
00:02:36,760 --> 00:02:38,640
把不同的框架的一些格式

62
00:02:38,840 --> 00:02:40,520
转换到自己

63
00:02:40,520 --> 00:02:45,000
推理引擎的一个 IR 或者格式

64
00:02:45,440 --> 00:02:47,680
就是下面有一层金黄色

65
00:02:47,680 --> 00:02:49,600
或者屎黄色的一层 IR

66
00:02:49,600 --> 00:02:51,480
就中间表达过了对应的 Schema

67
00:02:51,680 --> 00:02:55,560
就是为了去对接到自己的一个格式

68
00:02:55,560 --> 00:02:57,760
另外在转换的时候

69
00:02:57,880 --> 00:03:00,360
可能会有一些编译器的工作

70
00:03:00,360 --> 00:03:02,040
或者编译的功能

71
00:03:02,600 --> 00:03:03,880
在之前

72
00:03:03,880 --> 00:03:04,680
大家可以翻一下

73
00:03:04,680 --> 00:03:06,280
我的那些历史的视频

74
00:03:06,280 --> 00:03:07,160
在之前的分享

75
00:03:07,280 --> 00:03:09,480
会讲了很多 AI 编译器里面的

76
00:03:09,480 --> 00:03:11,120
一些图优化的工作

77
00:03:11,120 --> 00:03:12,360
或者在 AI 框架里面

78
00:03:12,440 --> 00:03:14,760
会有很多图优化的工作

79
00:03:14,960 --> 00:03:17,640
这些工作其实是交叉相互融合的

80
00:03:18,640 --> 00:03:21,840
接着来到下一个模块

81
00:03:21,840 --> 00:03:23,200
就是模型压缩

82
00:03:23,600 --> 00:03:24,920
模型压缩其实讲了

83
00:03:24,920 --> 00:03:25,880
它有一个很大的问题

84
00:03:25,880 --> 00:03:26,960
就是网络模型

85
00:03:27,040 --> 00:03:29,320
希望它的精度是越高越好

86
00:03:29,320 --> 00:03:30,440
但是模型的大小

87
00:03:30,560 --> 00:03:32,040
我又希望它越小越好

88
00:03:32,040 --> 00:03:33,560
这是不可调和的矛盾

89
00:03:33,560 --> 00:03:36,520
于是就提供了模型压缩的一个模块

90
00:03:36,520 --> 00:03:38,000
这个模块会对模型

91
00:03:38,000 --> 00:03:39,120
做一些低比特的量化

92
00:03:39,480 --> 00:03:40,360
对网络模型

93
00:03:40,360 --> 00:03:42,160
做一些 student 和 teacher 的蒸馏

94
00:03:42,560 --> 00:03:45,160
然后可能会做一些系数的剪枝

95
00:03:46,120 --> 00:03:47,920
最后一个是二值化

96
00:03:47,920 --> 00:03:50,600
二值化可能会对应一些

97
00:03:50,600 --> 00:03:53,200
比较学术的前沿的研究

98
00:03:53,200 --> 00:03:54,880
做一些相关的工作

99
00:03:54,880 --> 00:03:57,160
变成 0 和 1 的一些相关的模型

100
00:03:59,440 --> 00:04:01,960
下面看另外一个

101
00:04:01,960 --> 00:04:02,640
往右看一看

102
00:04:02,640 --> 00:04:05,280
这个时候有一些端侧学习

103
00:04:05,480 --> 00:04:08,000
端侧学习主要是有两种学习的方式

104
00:04:08,000 --> 00:04:09,800
第一种就是联邦学习

105
00:04:09,800 --> 00:04:11,240
另外一种是增量学习

106
00:04:11,480 --> 00:04:12,680
联邦学习还也好

107
00:04:12,680 --> 00:04:13,760
增量学习也好

108
00:04:13,880 --> 00:04:16,840
在 AI 系统第三节课推理的方式

109
00:04:16,840 --> 00:04:18,160
或者推理的工作流程里面

110
00:04:18,280 --> 00:04:21,320
去讲的一个具体的学习的方式

111
00:04:21,320 --> 00:04:23,960
但是其实这两块是一个非常大

112
00:04:23,960 --> 00:04:26,000
或者非常新的一个前沿的研究

113
00:04:26,000 --> 00:04:28,640
它又衍生了自己很多相关的工作

114
00:04:28,640 --> 00:04:30,600
而要支持这些相关的工作

115
00:04:30,600 --> 00:04:32,080
在做推理引擎的时候

116
00:04:32,400 --> 00:04:34,440
其实同是这条虚线框

117
00:04:34,840 --> 00:04:35,840
有相关的工作

118
00:04:35,840 --> 00:04:37,240
第一个可能会提供一些

119
00:04:37,240 --> 00:04:37,960
数据的处理

120
00:04:38,280 --> 00:04:39,640
会提供一些 trainer

121
00:04:39,960 --> 00:04:42,200
还会提供一些简单的

122
00:04:42,200 --> 00:04:43,000
optimizer

123
00:04:43,040 --> 00:04:44,600
就是优化器和损失函数

124
00:04:44,600 --> 00:04:46,320
去支持在端侧

125
00:04:46,320 --> 00:04:49,720
做联邦学习和做增量学习的工作

126
00:04:49,720 --> 00:04:51,160
而联邦学习又分为

127
00:04:51,160 --> 00:04:53,000
纵向联邦和横向联邦

128
00:04:53,000 --> 00:04:55,560
它们不同的联邦的学习的方式

129
00:04:55,560 --> 00:04:57,080
不同的增量学习的方式

130
00:04:57,360 --> 00:04:59,280
需求的模块是不同的

131
00:04:59,280 --> 00:05:01,280
这里面只是简单的讲了一下

132
00:05:01,280 --> 00:05:02,160
增量学习

133
00:05:02,160 --> 00:05:04,440
可能现在需要一些数据的处理

134
00:05:04,560 --> 00:05:07,280
trainer opt 和 loss 的模块

135
00:05:07,880 --> 00:05:11,160
再往后就是其他模块了

136
00:05:13,600 --> 00:05:14,360
在其他模块

137
00:05:14,480 --> 00:05:16,080
我觉得现在来说

138
00:05:16,320 --> 00:05:18,880
昇腾里面做的不一定说最好

139
00:05:18,880 --> 00:05:21,040
或者我觉得甚至有点缺失的

140
00:05:21,440 --> 00:05:22,640
现在其实很多时候

141
00:05:22,640 --> 00:05:24,040
做一个推理框架

142
00:05:24,040 --> 00:05:25,120
我觉得很重要的

143
00:05:25,120 --> 00:05:26,720
就是要有 benchmark

144
00:05:26,720 --> 00:05:28,320
你要有性能的对比

145
00:05:28,320 --> 00:05:29,640
你要有模型的对比

146
00:05:29,640 --> 00:05:31,880
用户或者开发者要知道

147
00:05:31,880 --> 00:05:33,720
你到底针对每一个模块

148
00:05:33,720 --> 00:05:35,720
到底做的怎么样了

149
00:05:35,720 --> 00:05:36,760
你的性能

150
00:05:36,760 --> 00:05:39,000
你的效果到底好还是不好

151
00:05:39,280 --> 00:05:41,440
大家一看就会有一个清晰的认知

152
00:05:41,440 --> 00:05:42,440
如果你不做

153
00:05:42,640 --> 00:05:44,200
到底是你懒得没做

154
00:05:44,200 --> 00:05:46,040
还是因为你的性能不够变好

155
00:05:47,320 --> 00:05:49,880
第二个就是很多端侧的推理引擎

156
00:05:50,000 --> 00:05:50,800
都会有的

157
00:05:50,800 --> 00:05:53,800
针对 iOS 可能会提供相关的 demo

158
00:05:53,800 --> 00:05:54,760
去告诉用户

159
00:05:55,120 --> 00:05:56,920
这些 API 是怎么去对接的

160
00:05:56,920 --> 00:05:58,080
怎么去应用的

161
00:05:58,080 --> 00:06:00,760
可能安卓也会有对应的 APP 的 demo

162
00:06:00,760 --> 00:06:01,760
有这些 demo

163
00:06:01,840 --> 00:06:03,080
包括现在的 TF Lite

164
00:06:03,240 --> 00:06:04,440
还是 Pytorch Mobile

165
00:06:04,840 --> 00:06:06,840
都会有对应的一个 demo

166
00:06:06,840 --> 00:06:08,040
我觉得是非常好的

167
00:06:09,000 --> 00:06:11,560
非常方便直接去集成和学习的

168
00:06:13,720 --> 00:06:16,080
接着往下看看

169
00:06:16,440 --> 00:06:18,120
往下看看就是中间表达

170
00:06:18,120 --> 00:06:18,760
中间表达了

171
00:06:18,760 --> 00:06:21,000
为啥会一条屎黄色的线

172
00:06:21,440 --> 00:06:23,480
横穿在各个模块

173
00:06:29,720 --> 00:06:32,240
是因为不管是哪个框架

174
00:06:32,240 --> 00:06:34,000
或者哪种方式去对接

175
00:06:34,000 --> 00:06:36,440
在下面正式跑的时候

176
00:06:36,560 --> 00:06:38,280
必须要有自己的格式

177
00:06:38,280 --> 00:06:40,040
和一个序列化的表达

178
00:06:40,840 --> 00:06:42,720
这个时候每个推理框架

179
00:06:42,720 --> 00:06:44,240
其实都有自己的定义

180
00:06:44,240 --> 00:06:45,880
为了提供自己的定义

181
00:06:45,880 --> 00:06:47,080
包括图优化

182
00:06:47,080 --> 00:06:48,120
我基于哪个图优化

183
00:06:48,400 --> 00:06:49,880
肯定是基于 IR

184
00:06:49,880 --> 00:06:51,320
自己的定义的 IR

185
00:06:51,320 --> 00:06:52,320
我做压缩剪枝

186
00:06:52,320 --> 00:06:53,840
我在哪一个图上面去做

187
00:06:54,120 --> 00:06:55,560
肯定有自己的定义

188
00:06:55,560 --> 00:06:56,600
包括我的端侧学习

189
00:06:56,600 --> 00:06:58,280
我在什么一个图上面

190
00:06:58,280 --> 00:06:59,720
去做一个应用

191
00:07:00,480 --> 00:07:02,360
这些都需要有自己的计算图

192
00:07:02,360 --> 00:07:03,000
自己的 IR

193
00:07:03,000 --> 00:07:03,920
自己的中间表达

194
00:07:03,920 --> 00:07:04,840
自己的 Schema

195
00:07:04,960 --> 00:07:07,480
去做一个统一的表述

196
00:07:07,480 --> 00:07:10,480
才能够更好的进行一些优化转化

197
00:07:10,880 --> 00:07:12,080
所有的功能

198
00:07:14,960 --> 00:07:16,680
在下面就是真正

199
00:07:16,680 --> 00:07:18,760
在整个推理引擎去运行的

200
00:07:18,760 --> 00:07:20,720
推理引擎最重要的功能

201
00:07:20,720 --> 00:07:21,640
就是 Runtime

202
00:07:21,640 --> 00:07:23,720
真正的一个执行的模块

203
00:07:23,720 --> 00:07:24,440
那执行模块

204
00:07:24,560 --> 00:07:25,720
其实有很多

205
00:07:25,720 --> 00:07:27,160
需要注意的技术点

206
00:07:27,160 --> 00:07:28,640
第一个我觉得比较重要的

207
00:07:28,640 --> 00:07:29,680
就是动态的 Batch

208
00:07:29,680 --> 00:07:31,440
还有异构的执行内存的分配

209
00:07:31,960 --> 00:07:33,760
可能在 ARM 上面

210
00:07:33,880 --> 00:07:35,360
会经常强调

211
00:07:35,360 --> 00:07:37,040
大小核的调度

212
00:07:37,040 --> 00:07:39,600
因为 ARM 它有分开大核和小核

213
00:07:40,120 --> 00:07:42,400
大小核可能会就着来用

214
00:07:42,560 --> 00:07:44,480
大可能会跑一些主要的 APP

215
00:07:44,680 --> 00:07:47,080
有可能主要的 APP 已经在跑了

216
00:07:47,080 --> 00:07:49,440
这个时候就占满它大核的一个

217
00:07:49,720 --> 00:07:50,480
计算资源

218
00:07:50,480 --> 00:07:52,640
可能就会做一些小核的调度

219
00:07:52,640 --> 00:07:53,960
就真正的把 AI

220
00:07:54,040 --> 00:07:55,080
跑在小核上面

221
00:07:55,080 --> 00:07:56,040
它不一定要具体

222
00:07:56,040 --> 00:07:58,240
根据具体的业务去分配的

223
00:07:58,560 --> 00:08:00,720
当然还会有一些多副本的

224
00:08:00,720 --> 00:08:02,400
并且还有装箱的功能

225
00:08:02,480 --> 00:08:04,920
这些在后面都会一一展开

226
00:08:06,920 --> 00:08:08,800
最后的一个模块

227
00:08:09,080 --> 00:08:12,200
就会有一些很多高性能的

228
00:08:12,200 --> 00:08:15,320
算子库或者算子去出现

229
00:08:15,520 --> 00:08:16,960
这里面要做很多工作

230
00:08:16,960 --> 00:08:18,040
第一个就是算子流化

231
00:08:18,040 --> 00:08:18,560
算子的执行

232
00:08:18,560 --> 00:08:20,000
还有算子的调度

233
00:08:20,240 --> 00:08:21,440
非常多相关的工作

234
00:08:21,440 --> 00:08:23,720
而可能最后面

235
00:08:23,800 --> 00:08:25,360
会给大家展示一下

236
00:08:25,680 --> 00:08:26,960
我以 MNN 为例子

237
00:08:27,120 --> 00:08:29,800
里面 MNN 的代码有一半都是

238
00:08:29,800 --> 00:08:31,400
Kernel 有一半都是算子

239
00:08:31,680 --> 00:08:33,640
而上面这些功能的代码量

240
00:08:33,760 --> 00:08:34,960
其实占不到一半

241
00:08:35,080 --> 00:08:37,440
就 Kernel 层的代码就占了非常多

242
00:08:37,960 --> 00:08:39,240
为啥会占这么多

243
00:08:40,360 --> 00:08:41,680
其实原因很简单

244
00:08:41,800 --> 00:08:43,120
针对无卡后端

245
00:08:43,120 --> 00:08:45,800
可能会提供 100 个算子是无卡的

246
00:08:45,920 --> 00:08:47,920
然后可能针对 OpenCL

247
00:08:48,000 --> 00:08:49,680
又会提供 100 多个算子

248
00:08:49,680 --> 00:08:50,600
是 OpenCL 的

249
00:08:50,720 --> 00:08:51,800
针对 Meta

250
00:08:51,960 --> 00:08:53,840
可能还会提供 100 多个算子

251
00:08:53,840 --> 00:08:54,760
是 Meta 的

252
00:08:55,360 --> 00:08:56,000
每个后端

253
00:08:56,000 --> 00:08:58,280
都会提供一些对应的算子

254
00:08:58,280 --> 00:08:59,760
可能 NEO 指令集

255
00:08:59,800 --> 00:09:01,880
又会提供对应的算子

256
00:09:01,880 --> 00:09:03,320
就跑在不同的设备上面

257
00:09:03,320 --> 00:09:04,320
跑在手机上面

258
00:09:04,320 --> 00:09:05,920
我提供 100 个算子

259
00:09:05,920 --> 00:09:06,600
跑在这个上面

260
00:09:06,600 --> 00:09:08,120
我跑的提供 100 个算子

261
00:09:08,120 --> 00:09:09,200
就合起来了

262
00:09:09,320 --> 00:09:10,400
就变得算子层

263
00:09:10,400 --> 00:09:11,880
就极度的爆炸了

264
00:09:12,080 --> 00:09:13,960
这个其实这个矛盾

265
00:09:13,960 --> 00:09:15,360
一直都是不可调和的

266
00:09:15,360 --> 00:09:16,800
更多的是针对不同的设备

267
00:09:16,800 --> 00:09:18,480
提供更多的算子

268
00:09:18,640 --> 00:09:21,240
可能有一些在 GPU 里面

269
00:09:21,360 --> 00:09:22,720
可能 OpenCL 提供几个

270
00:09:22,720 --> 00:09:23,720
无卡提供几个

271
00:09:23,720 --> 00:09:26,440
分别的去异构的去调度算子

272
00:09:26,440 --> 00:09:28,000
也有这种方式

273
00:09:29,000 --> 00:09:33,520
最后就我没有在下一个 slide 里面呈现的

274
00:09:33,520 --> 00:09:36,920
就是里面会跑的非常多的不同的设备

275
00:09:36,920 --> 00:09:38,000
推理引擎

276
00:09:38,280 --> 00:09:40,800
要在不同的设备去执行

277
00:09:42,680 --> 00:09:44,920
下面来看看具体的工作流程

278
00:09:44,920 --> 00:09:46,760
就推理引擎的工作流程

279
00:09:46,760 --> 00:09:48,240
工作流程比较简单

280
00:09:48,240 --> 00:09:50,680
首先会把各种 AI 框架

281
00:09:50,680 --> 00:09:53,520
MindSpore Tensorflow Pytorch PaddlePaddle 也好

282
00:09:53,520 --> 00:09:56,280
都会把这些 AI 框架训练得到的模型

283
00:09:56,480 --> 00:09:59,000
就给模型转换工具

284
00:09:59,000 --> 00:10:00,600
然后去做一个转换

285
00:10:00,600 --> 00:10:03,680
成为自己的一个推理引擎的格式

286
00:10:04,000 --> 00:10:05,560
这个推理引擎的格式

287
00:10:05,640 --> 00:10:06,680
叫做推理模型

288
00:10:06,680 --> 00:10:08,480
这可能还是个离线模型

289
00:10:08,480 --> 00:10:12,160
接着会对离线模型进行一个压缩

290
00:10:12,160 --> 00:10:13,120
一般都会压缩

291
00:10:13,120 --> 00:10:15,000
不压缩的情况下其实很少的

292
00:10:15,000 --> 00:10:16,360
然后压缩完之后

293
00:10:16,560 --> 00:10:18,520
再做一个环境的准备

294
00:10:18,520 --> 00:10:19,280
环境的准备

295
00:10:19,360 --> 00:10:21,200
会做好多大量的配置

296
00:10:21,200 --> 00:10:22,520
包括大小核的调度

297
00:10:23,160 --> 00:10:25,680
模型文档从哪里拉取

298
00:10:25,800 --> 00:10:28,000
这些都是环境的准备

299
00:10:28,000 --> 00:10:29,160
准备完这些环境之后

300
00:10:29,320 --> 00:10:32,560
就会做开发和编译推理的进程

301
00:10:32,560 --> 00:10:34,920
就是这个推理进程是真正开发出来的

302
00:10:34,920 --> 00:10:36,080
推理引擎

303
00:10:36,240 --> 00:10:38,520
是在真正执行的时候用的

304
00:10:38,720 --> 00:10:40,840
推理引擎刚才提供的 API

305
00:10:41,160 --> 00:10:43,320
是在这个阶段去使用的

306
00:10:43,320 --> 00:10:45,200
就给用户提供一些 API

307
00:10:45,200 --> 00:10:47,040
做一些它对应的模块

308
00:10:47,040 --> 00:10:48,680
或者对应的任务的开发

309
00:10:48,880 --> 00:10:50,080
这个时候开发工程师

310
00:10:50,080 --> 00:10:51,840
大部分都是按照这个流程来走

311
00:10:52,000 --> 00:10:54,720
接着开发完自己的一个对应的任务之后

312
00:10:54,880 --> 00:10:57,560
就做一个推理的引擎推理的执行

313
00:10:57,560 --> 00:11:00,040
让它真正在 Runtime 里面去跑起来

314
00:11:00,880 --> 00:11:01,560
跑起来的时候

315
00:11:01,720 --> 00:11:04,640
就依赖于输入和输出的结果了

316
00:11:05,320 --> 00:11:07,400
这个就是在线执行

317
00:11:07,400 --> 00:11:10,520
就是刚才往下面分开两部分

318
00:11:10,520 --> 00:11:12,120
一部分是离线的优化

319
00:11:12,120 --> 00:11:13,600
一部分是在线的优化

320
00:11:13,600 --> 00:11:15,600
对于架构图也是比较清晰的

321
00:11:15,600 --> 00:11:16,440
对应下来

322
00:11:17,920 --> 00:11:20,360
接下来看一下开发一个推理进程

323
00:11:20,360 --> 00:11:22,680
需要执行或者需要做哪些步骤

324
00:11:22,800 --> 00:11:24,760
这一面我就不一一念了

325
00:11:24,760 --> 00:11:26,480
我给大家去看看这个图

326
00:11:26,480 --> 00:11:28,360
一般来说会做哪些工作

327
00:11:28,480 --> 00:11:29,480
开发一个推理进程

328
00:11:29,480 --> 00:11:32,840
首先会把其他模型转换过来

329
00:11:32,840 --> 00:11:34,200
转换到自己的一个 Schema

330
00:11:34,200 --> 00:11:35,320
或者自己的 IR

331
00:11:35,320 --> 00:11:36,720
通过 converter

332
00:11:36,720 --> 00:11:39,000
就是离线转换工具转换过来

333
00:11:39,280 --> 00:11:40,360
接着转换过来

334
00:11:40,480 --> 00:11:43,240
转换过程当中压缩或者图优化的工作

335
00:11:43,440 --> 00:11:44,880
这里面就不再提了

336
00:11:44,880 --> 00:11:45,840
转换完之后

337
00:11:46,040 --> 00:11:49,520
就去做一些推理的配置的管理

338
00:11:49,720 --> 00:11:51,280
这个时候会把模型

339
00:11:51,400 --> 00:11:53,160
我的模型权重在哪里拉取

340
00:11:53,280 --> 00:11:55,080
是线上拉取还是本地托管

341
00:11:55,480 --> 00:11:57,640
运行的 kernel 是大小或者哪个

342
00:11:58,000 --> 00:11:59,480
我的配置优化的选项

343
00:11:59,480 --> 00:12:00,440
就刚才 Runtime

344
00:12:00,440 --> 00:12:02,960
其实有很多优化的一些方式

345
00:12:03,160 --> 00:12:03,840
这些配置

346
00:12:04,160 --> 00:12:05,760
都通过 config

347
00:12:05,760 --> 00:12:08,360
或者通过 pb 或者 yaml 文档的去写

348
00:12:08,360 --> 00:12:09,120
写好之后

349
00:12:09,480 --> 00:12:12,000
就需要去调用推理引擎

350
00:12:12,000 --> 00:12:14,280
把它作为一个具体的对象

351
00:12:14,440 --> 00:12:16,440
这个就是推理引擎提供的 API

352
00:12:16,440 --> 00:12:17,600
接着第 4 步

353
00:12:17,600 --> 00:12:19,800
需要去处理数据

354
00:12:20,400 --> 00:12:21,280
以安卓手机

355
00:12:21,280 --> 00:12:23,080
人脸检测或人脸 landmark

356
00:12:23,080 --> 00:12:24,320
作为一个例子

357
00:12:24,600 --> 00:12:26,760
这个时候需要从 pipeline

358
00:12:26,760 --> 00:12:28,720
就是 asp 的 pipeline

359
00:12:28,720 --> 00:12:30,200
摄像头的 pipeline

360
00:12:30,200 --> 00:12:32,000
去捕获一张图片

361
00:12:32,000 --> 00:12:33,880
然后做很多的预处理

362
00:12:34,280 --> 00:12:35,040
这些很多工作

363
00:12:35,160 --> 00:12:37,000
其实不在推理引擎里面

364
00:12:37,000 --> 00:12:38,320
做完这些工作之后

365
00:12:38,440 --> 00:12:39,680
就拿到数据

366
00:12:39,680 --> 00:12:41,480
推理引擎就提供了一个 API 接口

367
00:12:41,480 --> 00:12:42,120
to tensor

368
00:12:42,120 --> 00:12:43,200
把这些数据

369
00:12:43,200 --> 00:12:44,200
把人脸的数据

370
00:12:44,600 --> 00:12:46,440
或者把从 pipeline 里面得到的

371
00:12:46,440 --> 00:12:47,280
y1 的数据

372
00:12:47,480 --> 00:12:50,240
去输入进去推理引擎的 API

373
00:12:50,240 --> 00:12:52,440
接着推理引擎去执行 run

374
00:12:52,440 --> 00:12:54,160
就把真正的去调起来

375
00:12:54,160 --> 00:12:54,720
run 了之后

376
00:12:55,000 --> 00:12:56,680
真正的去执行了

377
00:12:56,680 --> 00:12:58,120
最后会有推理结果

378
00:12:58,120 --> 00:12:58,640
推理结果

379
00:12:58,760 --> 00:13:01,800
可能就会把它转换成为对应的 tensor

380
00:13:01,800 --> 00:13:04,080
或者转换成为内存可识别的模块

381
00:13:04,080 --> 00:13:05,360
最后做一些后处理

382
00:13:05,360 --> 00:13:07,360
那后处理就是交给 APP

383
00:13:07,360 --> 00:13:08,760
或者具体的任务

384
00:13:08,760 --> 00:13:10,480
然后做一些显示的工作

385
00:13:10,760 --> 00:13:12,120
然后打点的工作

386
00:13:12,240 --> 00:13:15,560
或者真正推给服务器去处理

387
00:13:15,760 --> 00:13:16,600
这些很多工作

388
00:13:16,920 --> 00:13:18,520
不在推理引擎里面

389
00:13:18,520 --> 00:13:21,240
这个就是整体的开发的进程了

390
00:13:22,320 --> 00:13:22,920
好了

391
00:13:22,920 --> 00:13:24,920
那今天主要是讲了

392
00:13:24,920 --> 00:13:27,280
推理引擎的整体的架构

393
00:13:27,280 --> 00:13:29,200
主要分开 5 个部分

394
00:13:29,200 --> 00:13:30,800
第一个就是 API

395
00:13:30,800 --> 00:13:32,280
转换模块压缩模块

396
00:13:32,280 --> 00:13:35,040
还有真正的运行时的优化

397
00:13:35,040 --> 00:13:35,640
Runtime

398
00:13:35,640 --> 00:13:36,880
还有 kernel 层

399
00:13:39,280 --> 00:13:41,600
有了整个架构的了解之后

400
00:13:41,760 --> 00:13:42,840
有多少个模块

401
00:13:42,840 --> 00:13:44,560
就会去看看

402
00:13:44,680 --> 00:13:46,840
这些模块之间是怎么配合工作的

403
00:13:46,840 --> 00:13:47,880
就去看了一下

404
00:13:47,880 --> 00:13:49,840
整个推理引擎的工作流程

405
00:13:49,840 --> 00:13:51,080
有了工作流程之后

406
00:13:51,440 --> 00:13:54,120
又带着大家去汇报了一下

407
00:13:54,120 --> 00:13:55,120
针对工作流程

408
00:13:55,120 --> 00:13:57,360
要开发一个具体的应用

409
00:13:57,360 --> 00:13:58,920
它应该有哪些步骤

410
00:13:58,920 --> 00:14:02,040
它应该怎么去用推理引擎

411
00:14:02,040 --> 00:14:04,240
那今天的内容就到这里为止

412
00:14:04,240 --> 00:14:04,800
好了

413
00:14:04,800 --> 00:14:05,520
谢谢各位

414
00:14:05,520 --> 00:14:07,480
卷的不行了

415
00:14:07,480 --> 00:14:08,880
记得一键三连加关注

416
00:14:09,240 --> 00:14:10,640
所有的内容都会开源

417
00:14:10,640 --> 00:14:12,440
在下面这条链接里面

418
00:14:12,840 --> 00:14:13,760
拜拜

