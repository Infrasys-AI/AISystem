1
00:00:00,000 --> 00:00:05,050
字幕生成: BLACK 字幕校对: 方鸿渐

2
00:00:05,050 --> 00:00:08,200
Hello 大家好，我是 ZOMI

3
00:00:08,200 --> 00:00:10,600
今天来到推理系统系列里面

4
00:00:10,600 --> 00:00:13,080
去真正的了解一下什么是推理系统

5
00:00:13,080 --> 00:00:15,640
那推理系统它不是推理引擎

6
00:00:15,640 --> 00:00:17,400
所以可以区分下来

7
00:00:17,400 --> 00:00:21,640
接下来的内容主要是聚焦于推理系统的整体的介绍

8
00:00:21,640 --> 00:00:25,120
今天我要去给大家分享的内容可能会稍微多了一点

9
00:00:25,120 --> 00:00:29,240
但是我尽量的简洁一点去给大家汇报一下相关的内容

10
00:00:30,880 --> 00:00:34,520
第一个就是去了解一下训练和推理到底有什么区别

11
00:00:34,520 --> 00:00:37,200
接着看看什么是推理系统

12
00:00:37,200 --> 00:00:41,160
第三点就是去看一下推理系统的优化目标和约束

13
00:00:41,160 --> 00:00:43,960
这些更多的是一些宏观上的概念

14
00:00:43,960 --> 00:00:48,600
最后看看推理系统和推理引擎到底要研究哪些内容

15
00:00:48,600 --> 00:00:53,520
其实推理系统对于 ZOMI 来说应该是最熟悉的一部分了

16
00:00:53,520 --> 00:00:56,280
这个也是我之前在刚进入华为的时候

17
00:00:56,280 --> 00:01:01,560
做了大量非常底层相关的一些代码的研发或者代码的开发工作

18
00:01:03,560 --> 00:01:06,080
下面看一下整体的生命周期

19
00:01:06,080 --> 00:01:07,680
首先很简单

20
00:01:07,680 --> 00:01:11,640
其实大家应该搞 AI 或者做算法的人特别熟

21
00:01:11,640 --> 00:01:14,880
可能做系统的人不是说非常熟

22
00:01:14,880 --> 00:01:17,720
所以我这里面简单的去给大家汇报一下

23
00:01:17,720 --> 00:01:21,320
首先可能会收集非常多的一些训练的数据

24
00:01:21,320 --> 00:01:22,560
公开的数据集也好

25
00:01:22,560 --> 00:01:23,840
自有的数据集也好

26
00:01:23,880 --> 00:01:27,600
然后就会对神经网络模型进行训练

27
00:01:27,600 --> 00:01:31,320
训练完之后就得到一个固定化的网络模型

28
00:01:31,320 --> 00:01:34,600
这个模型就真正的在部署的阶段了

29
00:01:34,600 --> 00:01:36,800
就会在云端做一些服务的请求

30
00:01:36,800 --> 00:01:38,440
还有服务的响应

31
00:01:38,440 --> 00:01:39,840
而在推理引擎

32
00:01:39,840 --> 00:01:43,320
更多是聚焦于这一个深度学习的模型

33
00:01:43,320 --> 00:01:44,800
怎么跑起来更快

34
00:01:44,800 --> 00:01:50,160
而推理系统整个系统需要把端到端的流程串起来

35
00:01:50,160 --> 00:01:52,040
把服务化把它做好

36
00:01:52,160 --> 00:01:53,680
这个就是最大的区别

37
00:01:54,680 --> 00:01:58,880
现在去区分一下训练任务和推理任务之间的一个最大的区别

38
00:01:58,880 --> 00:02:02,040
训练任务更多的是在训练的时候

39
00:02:02,040 --> 00:02:04,960
会采用中心化的一个训练

40
00:02:04,960 --> 00:02:08,400
一般来说训练就需要非常多的时间了

41
00:02:08,400 --> 00:02:11,520
就把数据去跟算法匹配起来

42
00:02:11,520 --> 00:02:13,280
而且需要比较大的数据

43
00:02:13,280 --> 00:02:18,960
然后这个时候对中心服务器的吞吐量还是比较要求高的

44
00:02:19,000 --> 00:02:22,920
而训练的模型的精度和准确率也是比较高的

45
00:02:22,920 --> 00:02:26,120
就尽可能模型的精度和性能比较好

46
00:02:27,880 --> 00:02:30,360
而推理基本上我去想想

47
00:02:31,360 --> 00:02:34,240
HMS Core 里面的推理引擎用的是 MindSpore

48
00:02:34,240 --> 00:02:37,040
然后它基本上就是因为要服务全球

49
00:02:37,040 --> 00:02:40,160
所以会 7×24 小时的去服务

50
00:02:40,160 --> 00:02:44,760
而且每天的调用量可能就超过 5 亿次

51
00:02:44,760 --> 00:02:48,560
每天调用量超过 5 亿次的服务响应请求

52
00:02:48,560 --> 00:02:52,880
这个时候整个推理的任务的要求是非常高的

53
00:02:52,880 --> 00:02:56,960
而且模型一般稳定收敛的情况下不会重新训练

54
00:02:56,960 --> 00:02:59,760
而且推一个新的模型是非常的谨慎的

55
00:02:59,760 --> 00:03:02,000
因为服务压力请求是非常大的

56
00:03:02,000 --> 00:03:07,040
这个就是深度学习里面生命周期训练和推理的最大的区别

57
00:03:08,800 --> 00:03:12,240
现在看看他们具体的遇到的一些挑战

58
00:03:12,240 --> 00:03:15,320
或者他们的不同的特点

59
00:03:15,360 --> 00:03:17,720
上面这个就是训练场景的

60
00:03:17,720 --> 00:03:19,800
下面就是推理场景的

61
00:03:20,840 --> 00:03:23,560
训练场景会用非常多的数据

62
00:03:23,560 --> 00:03:24,920
就 big BatchSize

63
00:03:24,920 --> 00:03:27,360
然后在云服务器上面去跑

64
00:03:27,360 --> 00:03:30,160
这个时候会做一些前向的推理

65
00:03:30,160 --> 00:03:31,040
反向的传播

66
00:03:31,040 --> 00:03:32,200
然后去更新

67
00:03:32,200 --> 00:03:34,720
不断的去训练整个网络模型

68
00:03:34,720 --> 00:03:38,040
使得网络模型的精度肯定是越高越好

69
00:03:38,800 --> 00:03:39,960
在训练完之后

70
00:03:40,120 --> 00:03:43,000
就会把网络模型固定化下来

71
00:03:43,520 --> 00:03:45,960
给到 IoT 设备或者 Web Service

72
00:03:45,960 --> 00:03:47,640
去做一个推理服务的

73
00:03:48,040 --> 00:03:50,360
而推理服务需要的数据量

74
00:03:50,880 --> 00:03:52,880
不会像训练这么大

75
00:03:52,880 --> 00:03:56,880
这个时候使用的更多的是真实的数据场景

76
00:03:56,880 --> 00:03:58,480
然后去部署起来

77
00:03:58,480 --> 00:04:01,520
真正的做一个简单的前向的推理

78
00:04:01,520 --> 00:04:03,000
然后预测到结果

79
00:04:03,200 --> 00:04:05,520
这个结果就会返回给用户

80
00:04:05,520 --> 00:04:08,000
或者返回给 Web 服务器请求

81
00:04:08,000 --> 00:04:10,160
做一些传统的工作

82
00:04:10,360 --> 00:04:13,280
这个就是他们最大的一些区别

83
00:04:14,480 --> 00:04:16,480
现在看一些遇到的一些挑战

84
00:04:16,480 --> 00:04:18,920
就是模型在推理的时候

85
00:04:18,920 --> 00:04:20,440
需要长期的运行

86
00:04:20,440 --> 00:04:23,600
而且推理会有更加苛刻的资源的要求

87
00:04:23,600 --> 00:04:24,760
因为 IoT 设备

88
00:04:25,160 --> 00:04:27,360
它的计算资源是非常有限的

89
00:04:27,360 --> 00:04:30,080
而且还是不能做一个反向传播

90
00:04:30,080 --> 00:04:32,040
就基本上不用学习

91
00:04:32,040 --> 00:04:33,440
做个推理就好了

92
00:04:33,440 --> 00:04:35,480
而且部署的型号非常多

93
00:04:36,440 --> 00:04:38,280
可能对于我一个应用来说

94
00:04:38,280 --> 00:04:39,640
用户看到的是一个应用

95
00:04:39,640 --> 00:04:40,880
但是在厂商来说

96
00:04:40,880 --> 00:04:44,440
我看到有 100 台不同的手机

97
00:04:44,440 --> 00:04:47,320
100 台不同的设备去做一个服务的

98
00:04:47,320 --> 00:04:49,240
所以对于要求还是非常高的

99
00:04:51,840 --> 00:04:54,160
现在来看一下推理系统

100
00:04:54,160 --> 00:04:55,200
就是整个推理系统

101
00:04:55,200 --> 00:04:56,440
要做哪些内容呢

102
00:04:56,440 --> 00:04:58,000
就是首先推理系统

103
00:04:58,000 --> 00:05:01,240
最重要的就是管理网络模型

104
00:05:01,240 --> 00:05:02,360
因为训练出来了

105
00:05:02,360 --> 00:05:04,240
会有非常多的网络模型

106
00:05:04,240 --> 00:05:06,840
而这些模型就是对应的算法

107
00:05:06,840 --> 00:05:08,880
要真正的部署给用户的

108
00:05:08,920 --> 00:05:10,760
而整个推理的服务系统里面

109
00:05:10,840 --> 00:05:11,800
做很多工作

110
00:05:12,000 --> 00:05:13,720
第一个就是模型的加载了

111
00:05:13,720 --> 00:05:15,160
模型的版本管理了

112
00:05:15,160 --> 00:05:16,360
还有数据的管理了

113
00:05:16,360 --> 00:05:17,880
还有服务的接口

114
00:05:17,880 --> 00:05:20,920
所以基本上它跟 AI 训练

115
00:05:20,920 --> 00:05:22,600
或者 AI 算法不相关

116
00:05:22,600 --> 00:05:25,000
更多的是平台性的工作

117
00:05:25,000 --> 00:05:26,840
然后最后会去除一些

118
00:05:27,200 --> 00:05:30,000
服务器端和客户端的一些请求和响应

119
00:05:30,000 --> 00:05:32,720
完成整个端到端的功能

120
00:05:32,720 --> 00:05:34,880
这个是推理系统要做的工作

121
00:05:36,560 --> 00:05:37,840
而所谓的推理系统

122
00:05:38,000 --> 00:05:40,920
其实不仅仅是以数据中心

123
00:05:40,920 --> 00:05:42,120
为一个服务

124
00:05:42,120 --> 00:05:44,080
作为一个主要的方式

125
00:05:44,080 --> 00:05:45,640
而且它要兼顾边缘的

126
00:05:45,640 --> 00:05:47,000
移动设备的场景

127
00:05:48,000 --> 00:05:51,120
而提到的整个推理服务的策略

128
00:05:52,560 --> 00:05:53,760
不仅仅需要考虑到

129
00:05:53,760 --> 00:05:55,080
数据中心的一个推理

130
00:05:55,080 --> 00:05:57,200
还要考虑到边缘设备的推理

131
00:05:57,200 --> 00:05:59,240
所以推理系统还是很复杂的

132
00:05:59,240 --> 00:06:01,160
它是一个非常复杂的系统工程

133
00:06:01,160 --> 00:06:03,400
下面提几个问题

134
00:06:04,240 --> 00:06:06,720
就是深度学习的推理系统

135
00:06:06,800 --> 00:06:08,480
要做设计的时候

136
00:06:08,480 --> 00:06:11,160
一般要考虑哪些问题

137
00:06:12,320 --> 00:06:13,680
因为考虑的这些问题

138
00:06:13,680 --> 00:06:16,200
可能会影响后面的架构的设计

139
00:06:16,200 --> 00:06:18,040
还有技术的方案

140
00:06:19,080 --> 00:06:21,640
第二点就是推理系统

141
00:06:21,640 --> 00:06:23,680
跟传统的服务系统

142
00:06:23,680 --> 00:06:25,640
有哪些新的挑战

143
00:06:26,640 --> 00:06:28,040
大家可以一起回顾一下

144
00:06:28,040 --> 00:06:32,080
刚才所讨论所探讨的一些内容

145
00:06:33,480 --> 00:06:35,280
最后一个就是云侧

146
00:06:35,280 --> 00:06:36,640
就是中心服务器

147
00:06:36,640 --> 00:06:38,720
还有端侧到手机

148
00:06:38,720 --> 00:06:40,120
或者 IoT 设备上面

149
00:06:40,120 --> 00:06:44,000
整个推理系统的服务有什么不同

150
00:06:44,000 --> 00:06:46,080
有什么各自的侧重点

151
00:06:46,080 --> 00:06:48,600
我觉得大家可以停下来几分钟

152
00:06:48,600 --> 00:06:51,200
去思考一下具体是怎么做的

153
00:06:51,200 --> 00:06:53,200
因为这个对架构的设计

154
00:06:53,200 --> 00:06:55,000
有非常大的一个挑战

155
00:06:55,000 --> 00:06:56,720
和不同的技术选行

156
00:06:59,640 --> 00:07:02,120
接下来我会去给大家汇报一下

157
00:07:02,120 --> 00:07:04,800
推理系统的一个优化的目标和约束

158
00:07:04,920 --> 00:07:06,840
现在这个更多的是一些

159
00:07:07,720 --> 00:07:08,720
宏观的概念

160
00:07:08,840 --> 00:07:11,320
大家其实可以不用太在乎

161
00:07:11,320 --> 00:07:12,640
或者听一听就完了

162
00:07:14,440 --> 00:07:17,680
现在还是以淘宝作为例子

163
00:07:17,680 --> 00:07:19,760
就是包括在线新闻

164
00:07:19,760 --> 00:07:20,560
推理这些

165
00:07:20,560 --> 00:07:22,200
都是推荐一些

166
00:07:22,200 --> 00:07:24,040
比较喜欢的一些服务

167
00:07:24,040 --> 00:07:25,320
包括抖音它的推荐

168
00:07:25,320 --> 00:07:26,680
其实也是相同的

169
00:07:26,680 --> 00:07:28,480
可能要考虑到低延迟

170
00:07:28,680 --> 00:07:30,120
还有一个高吞吐

171
00:07:30,120 --> 00:07:31,000
还有扩展性

172
00:07:31,000 --> 00:07:32,320
还有准确性的问题

173
00:07:32,320 --> 00:07:33,960
就提一点

174
00:07:34,080 --> 00:07:35,040
就是低延迟

175
00:07:35,040 --> 00:07:37,400
网络上的文章的推荐

176
00:07:37,400 --> 00:07:39,720
或者产品的推荐

177
00:07:39,720 --> 00:07:41,480
希望延迟越少越好

178
00:07:41,480 --> 00:07:43,320
因为我不断的刷刷刷

179
00:07:43,520 --> 00:07:44,840
这个刷刷刷的过程

180
00:07:45,160 --> 00:07:47,960
就要求系统不断的响应

181
00:07:48,120 --> 00:07:49,040
有时候我淘宝

182
00:07:49,040 --> 00:07:50,120
或者我经常刷抖音

183
00:07:50,440 --> 00:07:52,040
也是经常刷刷刷

184
00:07:52,040 --> 00:07:53,360
然后看他不满意的

185
00:07:53,360 --> 00:07:54,600
就下一个

186
00:07:55,280 --> 00:07:56,160
它的推荐过程

187
00:07:56,360 --> 00:07:58,640
是在后面不断的去计算

188
00:07:58,640 --> 00:07:59,960
我哪个视频喜欢

189
00:07:59,960 --> 00:08:02,040
哪个视频是不喜欢的

190
00:08:02,080 --> 00:08:04,000
系统为了算法要考虑很多问题

191
00:08:04,240 --> 00:08:05,440
第二个就是

192
00:08:05,480 --> 00:08:06,720
除了刚才讲的

193
00:08:06,720 --> 00:08:07,880
一些业务上的问题

194
00:08:07,880 --> 00:08:10,560
回到真正跟 AI 内容相关的

195
00:08:10,560 --> 00:08:11,080
就是

196
00:08:11,400 --> 00:08:12,480
我整个推理系统

197
00:08:12,480 --> 00:08:13,840
假设这个是推理系统

198
00:08:13,840 --> 00:08:14,720
要考虑到

199
00:08:14,720 --> 00:08:16,080
对接很多不同的

200
00:08:16,080 --> 00:08:18,680
AI 框架训练出来的模型

201
00:08:18,720 --> 00:08:20,960
接着可能还会考虑到

202
00:08:20,960 --> 00:08:22,600
我要部署在非常多的

203
00:08:22,600 --> 00:08:24,120
不同的硬件上面

204
00:08:24,120 --> 00:08:25,160
而整个推理系统

205
00:08:25,280 --> 00:08:27,120
它有很多模块去组成

206
00:08:27,120 --> 00:08:27,840
不同的模块

207
00:08:28,000 --> 00:08:30,640
又有不同的一个具体的功能

208
00:08:30,680 --> 00:08:32,000
所以说整个推理系统

209
00:08:32,000 --> 00:08:33,880
它是要考虑很多的问题

210
00:08:35,120 --> 00:08:36,160
现在看一下

211
00:08:36,160 --> 00:08:38,440
在设计推理系统时候

212
00:08:38,440 --> 00:08:40,240
需要考虑的几个问题

213
00:08:40,240 --> 00:08:42,560
也就是刚才一个提问

214
00:08:42,560 --> 00:08:44,000
所对应的回答

215
00:08:44,000 --> 00:08:45,200
第一个就是

216
00:08:45,200 --> 00:08:45,920
低延迟

217
00:08:45,920 --> 00:08:46,680
高吞吐

218
00:08:46,680 --> 00:08:47,280
高效率

219
00:08:47,280 --> 00:08:47,840
灵活性

220
00:08:47,840 --> 00:08:48,440
扩展性

221
00:08:48,440 --> 00:08:51,560
下面我简单的去展开一下

222
00:08:51,560 --> 00:08:52,640
对应的内容

223
00:08:52,640 --> 00:08:54,120
第一个就是灵活性

224
00:08:54,120 --> 00:08:55,480
就是 AI 部署

225
00:08:55,840 --> 00:08:57,320
AI 服务的部署

226
00:08:57,520 --> 00:08:59,840
其实对于优化和系统维护来说

227
00:08:59,840 --> 00:09:01,840
是比较困难的

228
00:09:01,840 --> 00:09:04,200
因为需要对接非常多的框架

229
00:09:04,360 --> 00:09:05,040
可以看到

230
00:09:05,040 --> 00:09:06,840
AI 框架有非常多

231
00:09:06,840 --> 00:09:08,800
而且对应的硬件系统

232
00:09:08,800 --> 00:09:10,040
也是非常的复杂

233
00:09:10,040 --> 00:09:12,560
所以要求整个 AI 系统

234
00:09:12,560 --> 00:09:15,800
是要求它整个灵活性要比较高

235
00:09:15,800 --> 00:09:17,680
这个是对于系统开发工程师

236
00:09:17,680 --> 00:09:19,200
或者一些厂商来说

237
00:09:19,200 --> 00:09:22,120
第二个就是整体的灵活性

238
00:09:22,760 --> 00:09:23,800
系统的灵活性

239
00:09:23,920 --> 00:09:25,640
就需要可以支持非常多的

240
00:09:25,640 --> 00:09:27,000
不同的 AI 的模型

241
00:09:27,240 --> 00:09:29,200
而且 AI 的框架的版本迭代

242
00:09:29,320 --> 00:09:31,800
这个是对挑战是非常高的

243
00:09:31,800 --> 00:09:34,200
要维护非常多的版本

244
00:09:34,200 --> 00:09:36,880
第三个就是跟不同的语言的对接

245
00:09:36,880 --> 00:09:38,920
还有不同的逻辑的应用的结合

246
00:09:38,920 --> 00:09:40,760
因为应用五花八门

247
00:09:40,760 --> 00:09:43,280
还有语言也是非常的多

248
00:09:43,280 --> 00:09:44,760
包括部署在 iOS

249
00:09:44,760 --> 00:09:46,320
部署在网页上面

250
00:09:46,320 --> 00:09:48,960
部署在安卓手机

251
00:09:48,960 --> 00:09:51,280
都是不同的语言的 API 的接口

252
00:09:51,280 --> 00:09:52,600
所以说整体来说

253
00:09:53,680 --> 00:09:56,840
可能会需要去做一个开放性的协议

254
00:09:56,840 --> 00:09:58,400
例如 onnx 这种转换

255
00:09:58,400 --> 00:10:00,560
另外接口可能需要进行

256
00:10:00,560 --> 00:10:01,840
一系列的抽象

257
00:10:01,840 --> 00:10:04,600
可能还会做一些容器的一些封装

258
00:10:04,600 --> 00:10:06,280
例如经常用的 docker

259
00:10:06,280 --> 00:10:08,560
最后可能还会需要一些 IPC

260
00:10:08,560 --> 00:10:11,640
就跨语言跨进程的一些通讯的协议

261
00:10:13,360 --> 00:10:15,520
接着看一下延迟

262
00:10:15,520 --> 00:10:17,920
延迟其实是对于在线系统来说

263
00:10:17,920 --> 00:10:20,000
是非常高的要求

264
00:10:20,000 --> 00:10:21,520
就刚才举的一个例子

265
00:10:21,520 --> 00:10:22,560
刷抖音的时候

266
00:10:22,560 --> 00:10:24,680
需要希望尽可能的

267
00:10:24,680 --> 00:10:26,640
去给我提供一个低延迟

268
00:10:26,760 --> 00:10:29,000
而且需要满足到有限的

269
00:10:29,000 --> 00:10:30,040
长尾的用户

270
00:10:30,040 --> 00:10:31,040
长尾的应用

271
00:10:31,880 --> 00:10:33,040
因为每个用户

272
00:10:33,280 --> 00:10:35,400
千人千面在推进系统的时候

273
00:10:35,400 --> 00:10:37,600
所以很多一些长尾的问题

274
00:10:37,600 --> 00:10:39,160
需要去解决的

275
00:10:39,160 --> 00:10:42,480
那最后还有一些吞吐量的问题

276
00:10:42,480 --> 00:10:44,680
就是面向一些传统的

277
00:10:44,680 --> 00:10:45,480
并发的问题

278
00:10:45,480 --> 00:10:46,600
并发的请求

279
00:10:46,600 --> 00:10:49,200
那这个时候怎么去解决吞吐量

280
00:10:51,080 --> 00:10:52,880
最后还有一个效率

281
00:10:52,880 --> 00:10:55,440
效率其实是对于系统来说

282
00:10:55,440 --> 00:10:56,440
不是说非常重要

283
00:10:56,480 --> 00:10:58,960
但是对于推理引擎来说

284
00:10:59,160 --> 00:11:00,360
就特别的重要了

285
00:11:00,800 --> 00:11:04,440
因为端侧的资源是有限的

286
00:11:05,200 --> 00:11:08,560
云侧的运算的预算也是有限的

287
00:11:08,560 --> 00:11:10,760
所以经常会对网络模型

288
00:11:10,960 --> 00:11:11,920
去做一些压缩

289
00:11:12,320 --> 00:11:14,760
去使用一些高效的 AI 的推理芯片

290
00:11:15,160 --> 00:11:18,000
而这一块就催生了非常多的一些

291
00:11:18,000 --> 00:11:19,160
第三方的厂商

292
00:11:19,160 --> 00:11:22,480
去做很多推出新的推理芯片

293
00:11:23,960 --> 00:11:26,240
那最后一个还有扩展性

294
00:11:26,360 --> 00:11:29,160
那扩展性就是应对用户的请求

295
00:11:29,160 --> 00:11:30,120
五花八门

296
00:11:30,120 --> 00:11:32,680
然后整个推理系统的吞吐量

297
00:11:32,680 --> 00:11:34,280
是非常的复杂的

298
00:11:35,000 --> 00:11:36,520
这要求整个推理系统

299
00:11:36,680 --> 00:11:39,440
需要提供非常强大的推理的吞吐

300
00:11:39,560 --> 00:11:41,560
还有让整个的推理系统

301
00:11:41,720 --> 00:11:43,240
更加的可靠

302
00:11:44,440 --> 00:11:44,880
另外的话

303
00:11:44,880 --> 00:11:47,560
可能还会做一些平台化的部署

304
00:11:47,560 --> 00:11:48,400
例如 Kubernetes

305
00:11:48,400 --> 00:11:50,200
还有 Docker 相关的部署

306
00:11:50,200 --> 00:11:52,640
使得整个系统更加的自动化

307
00:11:52,640 --> 00:11:55,160
而不是人工地去参与很多的维护

308
00:11:55,280 --> 00:11:58,280
因为里面的配置是非常的复杂

309
00:11:58,760 --> 00:12:00,320
一个 APP 里面

310
00:12:00,320 --> 00:12:02,400
就涉及到非常多的算法

311
00:12:02,400 --> 00:12:04,800
例如在淘宝页面看到的推荐

312
00:12:04,800 --> 00:12:06,920
跟淘宝子页面看到的推荐

313
00:12:06,920 --> 00:12:07,760
是不一样的

314
00:12:07,760 --> 00:12:09,480
你个人搜了一个内容

315
00:12:09,480 --> 00:12:11,760
里面的推荐的内容也是不一样

316
00:12:11,760 --> 00:12:14,000
用到的推荐算法也是不一样

317
00:12:14,000 --> 00:12:14,720
所以这里面

318
00:12:14,800 --> 00:12:16,720
就维护了非常多不同的算法

319
00:12:16,720 --> 00:12:18,560
非常多不同的副本

320
00:12:18,560 --> 00:12:21,000
而且很多用户大量涌入一个入口的时候

321
00:12:21,240 --> 00:12:23,760
怎么去解决这些负载的问题

322
00:12:23,800 --> 00:12:26,040
怎么解决后台服务器的问题

323
00:12:26,040 --> 00:12:28,720
这些都要求有非常高的扩展性

324
00:12:32,160 --> 00:12:34,480
接下来更多的是去理解一下

325
00:12:34,480 --> 00:12:37,560
或者了解一下推理系统和推理引擎的区别

326
00:12:38,160 --> 00:12:41,400
下面推理系统的一个简图

327
00:12:41,560 --> 00:12:42,160
丑了一点

328
00:12:42,160 --> 00:12:43,280
大家不要介意

329
00:12:43,280 --> 00:12:45,160
推理系统第一个

330
00:12:45,160 --> 00:12:50,880
很多时候需要去响应和请求一些服务的处理

331
00:12:50,880 --> 00:12:52,440
接着有了这些处理之后

332
00:12:52,560 --> 00:12:55,520
在系统里面需要做一些调度的队列

333
00:12:55,520 --> 00:12:57,000
和调度的排布

334
00:12:57,680 --> 00:12:58,800
有了调度的排布之后

335
00:12:58,920 --> 00:13:00,040
真正在推理了

336
00:13:00,040 --> 00:13:01,800
需要去执行算法

337
00:13:01,800 --> 00:13:04,440
会有一个推理引擎去执行的

338
00:13:04,440 --> 00:13:06,720
而推理引擎它需要有另外一个输入

339
00:13:06,720 --> 00:13:08,080
就是模型

340
00:13:08,080 --> 00:13:09,760
而推理系统很重要的

341
00:13:09,760 --> 00:13:11,080
就是对模型的系统

342
00:13:11,160 --> 00:13:11,880
模型的版本

343
00:13:12,240 --> 00:13:14,720
模型的算法进行管理

344
00:13:14,720 --> 00:13:18,240
而这个模型更多是来自于训练的一个流水线

345
00:13:18,240 --> 00:13:20,520
和模型库里面去获取的

346
00:13:20,720 --> 00:13:23,480
另外一个在最后或者比较全面的

347
00:13:23,480 --> 00:13:27,760
需要对整个过程进行监控和调度

348
00:13:27,760 --> 00:13:28,840
还有分发

349
00:13:28,840 --> 00:13:32,600
这个时候监控模块就显得非常的重要了

350
00:13:32,600 --> 00:13:34,400
这个就是整个推理系统了

351
00:13:34,400 --> 00:13:36,600
而推理系统可能会跑在一些 GPU

352
00:13:36,600 --> 00:13:39,520
CPU、NPU 各种不同的 PU 上面

353
00:13:41,120 --> 00:13:43,680
而推理系统需要考虑那些问题

354
00:13:43,680 --> 00:13:45,760
考虑的问题可能还真有点多

355
00:13:45,760 --> 00:13:47,560
第一个就是刚才提到的

356
00:13:47,560 --> 00:13:49,760
怎么实现低延迟高吞吐

357
00:13:50,280 --> 00:13:52,440
怎么分配调度请求

358
00:13:52,720 --> 00:13:53,760
调度算法

359
00:13:54,200 --> 00:13:56,720
怎么去管理整个 AI 的生命周期

360
00:13:57,200 --> 00:14:00,280
模型怎么去管理的各种版本

361
00:14:02,200 --> 00:14:06,160
接下来看一下推理引擎的一个架构图

362
00:14:06,280 --> 00:14:10,600
这个架构图我综合了非常多的一些推理框架

363
00:14:10,600 --> 00:14:12,880
去或者推理引擎去汇总的

364
00:14:12,880 --> 00:14:15,880
现在看上去有点那个花花绿绿的

365
00:14:15,880 --> 00:14:16,760
大家不要介意

366
00:14:16,880 --> 00:14:20,080
就将就着来看需要做哪些内容

367
00:14:20,080 --> 00:14:22,960
首先推理引擎有一些 API 的接口

368
00:14:22,960 --> 00:14:25,040
面向用户会提供 API 的接口

369
00:14:25,040 --> 00:14:28,480
接着需要把不同的 AI 框架去做一个转换

370
00:14:28,480 --> 00:14:30,680
转换成 AI 引擎

371
00:14:30,680 --> 00:14:31,880
就上一步

372
00:14:32,720 --> 00:14:35,680
在推理系统推理引擎能够去执行的

373
00:14:35,680 --> 00:14:38,360
具体的模型就自己的 Schema

374
00:14:38,360 --> 00:14:40,440
然后去做一个 runtime

375
00:14:40,440 --> 00:14:44,200
那 runtime 就真正的跑起来的一个调度的服务

376
00:14:44,200 --> 00:14:45,400
真正跑起来之后

377
00:14:45,800 --> 00:14:47,840
它其实是依赖于 kernel

378
00:14:47,840 --> 00:14:50,960
就是算子层去实现的

379
00:14:50,960 --> 00:14:55,080
而算子层其实是跑在不同的一些具体的设备上面了

380
00:14:55,080 --> 00:14:58,120
那这个时候推理系统需要考虑哪些问题呢

381
00:14:59,360 --> 00:15:02,800
第一个就是怎么去保持在训练的时候

382
00:15:02,800 --> 00:15:04,360
训练的时候精度越高越好

383
00:15:04,840 --> 00:15:07,840
但是在推理的时候希望减少模型的尺寸

384
00:15:07,840 --> 00:15:10,080
那模型越小肯定是越好

385
00:15:10,080 --> 00:15:11,880
但是精度怎么维持呢

386
00:15:11,880 --> 00:15:14,600
第二个就是怎么对不同的 AI 框架

387
00:15:14,600 --> 00:15:18,440
训练出来的结果或者训练的模型进行转换呢

388
00:15:19,000 --> 00:15:21,800
如何加快整体的调度和执行

389
00:15:21,800 --> 00:15:23,400
因为当 AI 系统

390
00:15:23,400 --> 00:15:26,720
需要对很多资源内存进行调度的

391
00:15:26,720 --> 00:15:29,000
特别是在一台手机设备上面

392
00:15:29,000 --> 00:15:32,320
一台 SoC 里面就包括 DPU NPU GPU

393
00:15:32,320 --> 00:15:35,760
还有 SPU 或者 NPU 各种 PU 了

394
00:15:35,760 --> 00:15:37,400
就集成一块 SoC

395
00:15:37,400 --> 00:15:39,200
这个时候面对这么多资源

396
00:15:39,200 --> 00:15:42,000
这么多 PU 怎么加快调度执行

397
00:15:42,600 --> 00:15:45,920
那还有就是怎么去提高算子的性能

398
00:15:45,920 --> 00:15:48,520
因为在手机设备呢

399
00:15:48,520 --> 00:15:51,600
例如那个 ARM 可能会提供一些 NEO 的指令集

400
00:15:51,600 --> 00:15:54,600
在 X86 可能会提供一些 AVX 的指令集

401
00:15:54,600 --> 00:15:58,760
那一台手机设备上面面向不同的设备

402
00:15:58,760 --> 00:16:01,240
具有不同的那个 Vulkan Metal

403
00:16:01,240 --> 00:16:03,400
或者 OpenCL 的使用的方式

404
00:16:03,400 --> 00:16:04,840
可能面向专用的设备

405
00:16:04,840 --> 00:16:07,520
有 TIC TVM 不同的 Kernel

406
00:16:07,520 --> 00:16:09,200
或者编程的体系

407
00:16:09,200 --> 00:16:13,400
另外可能会有一些单独提供的高性能的算子库

408
00:16:13,400 --> 00:16:15,640
那这个时候面向这么多不同的算子

409
00:16:15,640 --> 00:16:20,720
什么是提高整个算子的性能和综合的一个压力呢

410
00:16:22,240 --> 00:16:25,160
今天主要给大家去汇报了一下

411
00:16:25,160 --> 00:16:26,560
整个推理引擎

412
00:16:26,560 --> 00:16:29,960
有哪些服务为什么要去做推理系统或者推理引擎

413
00:16:29,960 --> 00:16:33,160
里面是因为遇到了大量的 AI 的算法

414
00:16:33,160 --> 00:16:34,160
新的服务

415
00:16:34,160 --> 00:16:37,960
现在很多传统的算法都转为 AI 的算法去代替了

416
00:16:38,000 --> 00:16:41,000
接着去探讨了一个训练和推理服务

417
00:16:41,000 --> 00:16:42,120
有什么不一样

418
00:16:42,120 --> 00:16:44,640
到底训练和推理侧重点是啥

419
00:16:44,640 --> 00:16:47,200
接着去系统的回顾了一下

420
00:16:47,200 --> 00:16:48,640
什么是推理系统

421
00:16:48,640 --> 00:16:53,120
推理系统的一个具体的约束和优化的目标到底是什么

422
00:16:53,120 --> 00:16:54,200
卷的不行了

423
00:16:54,200 --> 00:16:55,080
卷的不行了

424
00:16:55,080 --> 00:16:56,880
记得一键三连加关注哦

425
00:16:56,880 --> 00:17:00,520
所有的内容都会开源在下面这条链接里面

426
00:17:00,520 --> 00:17:01,240
拜了个拜

