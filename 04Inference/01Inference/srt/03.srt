1
00:00:00,000 --> 00:00:04,560
字幕生成: BLACK 字幕校对: 方鸿渐

2
00:00:05,920 --> 00:00:08,120
我是那个改革春风吹满地

3
00:00:08,120 --> 00:00:11,120
新的一年要争气的 ZOMI

4
00:00:11,120 --> 00:00:14,520
其实我今天要给大家去分享的内容

5
00:00:14,520 --> 00:00:17,320
还是集中在推理系统里面

6
00:00:17,320 --> 00:00:20,720
而去看看整个推理的流程的全景

7
00:00:20,720 --> 00:00:23,800
这里面总要分开三个内容给大家去汇报的

8
00:00:23,800 --> 00:00:25,240
第一个就是部署态

9
00:00:25,240 --> 00:00:27,360
跟之前的一个训练态的区别

10
00:00:27,480 --> 00:00:30,520
接着去看看云侧推理的整体的流程

11
00:00:30,520 --> 00:00:32,520
还有边缘部署的方式

12
00:00:32,520 --> 00:00:37,640
这里面去区别云侧和边缘的一个具体的不同

13
00:00:38,840 --> 00:00:41,160
而聊到这里其实我想插一个话题

14
00:00:41,160 --> 00:00:43,600
就是前几天的领导找我谈话说

15
00:00:43,600 --> 00:00:46,240
我可能只懂 AI 训练框架

16
00:00:46,240 --> 00:00:48,520
我要更多的去了解一下其他

17
00:00:48,520 --> 00:00:51,200
这个时候其实我是想反驳

18
00:00:51,200 --> 00:00:52,160
但是也不想说啥

19
00:00:52,160 --> 00:00:54,880
因为整个 AI 系统或者 AI 全栈

20
00:00:54,880 --> 00:00:59,800
在整个 SE 团队里面算是比较资深

21
00:01:03,520 --> 00:01:04,560
不再吐槽和抱怨

22
00:01:04,560 --> 00:01:06,720
来到回到真正的内容

23
00:01:06,720 --> 00:01:09,320
部署态的一个真正的区别

24
00:01:09,320 --> 00:01:12,080
首先看看什么叫做真正的部署态

25
00:01:12,080 --> 00:01:14,760
就 AI 系统一般都会部署在云端

26
00:01:14,760 --> 00:01:17,240
或者一个边缘的设备上面

27
00:01:17,240 --> 00:01:20,400
而云端更多的是集中在一些 web 的服务

28
00:01:20,400 --> 00:01:24,120
而在边缘更多的可能是一些边缘的服务器

29
00:01:24,120 --> 00:01:25,160
边缘的设备基站

30
00:01:25,160 --> 00:01:27,000
还有一些边缘的设备

31
00:01:27,000 --> 00:01:29,520
最熟悉的手机手环耳机

32
00:01:29,640 --> 00:01:31,240
这些就是 IoT 设备

33
00:01:32,240 --> 00:01:34,600
当然了部署态其实有另外一个

34
00:01:34,600 --> 00:01:35,840
就是云侧

35
00:01:35,840 --> 00:01:37,480
大家不要觉得谈到推理的

36
00:01:37,480 --> 00:01:39,920
大部分都是在 IoT 上面去运行的

37
00:01:39,920 --> 00:01:41,040
其实云侧

38
00:01:41,360 --> 00:01:42,680
就是整个推理服务

39
00:01:42,880 --> 00:01:44,880
是承载了非常多的服务的

40
00:01:44,880 --> 00:01:47,440
因为云侧就是整个云侧服务器

41
00:01:47,560 --> 00:01:49,360
它有非常多的算力内存

42
00:01:49,360 --> 00:01:51,240
而且电力非常充足

43
00:01:51,240 --> 00:01:52,920
只需要通过一个网线

44
00:01:52,920 --> 00:01:56,560
或者 RPC 或者 HTTP 的一些客户的请求和响应

45
00:01:56,680 --> 00:01:59,640
就能够完成整体的一个服务了

46
00:01:59,800 --> 00:02:03,040
这个时候提供的更多的是一个推理的服务

47
00:02:03,040 --> 00:02:04,480
或者云侧的服务

48
00:02:06,120 --> 00:02:08,440
现在看一下在部署态里面

49
00:02:08,440 --> 00:02:11,120
特别是在云端的一个部署态的特点

50
00:02:11,480 --> 00:02:13,960
我简单的给大家列一下

51
00:02:13,960 --> 00:02:15,520
可能语速有点快没关系

52
00:02:15,520 --> 00:02:18,200
现在的内容还是更多的是概念性的问题

53
00:02:18,560 --> 00:02:20,040
第一个就是对功耗温度

54
00:02:20,160 --> 00:02:21,320
还有模型大小

55
00:02:21,440 --> 00:02:23,000
是没有太严格的限制的

56
00:02:23,000 --> 00:02:24,080
因为云侧

57
00:02:24,280 --> 00:02:26,960
云端的资源还是非常的丰富的

58
00:02:27,280 --> 00:02:28,120
接着看一下

59
00:02:28,120 --> 00:02:30,800
其实云侧它不仅是用推理

60
00:02:30,800 --> 00:02:33,440
其实推理和训练都是有的

61
00:02:33,440 --> 00:02:35,720
只是在服务的时候

62
00:02:35,720 --> 00:02:38,120
更多的是集中在推理

63
00:02:38,360 --> 00:02:39,640
接着在云侧

64
00:02:39,640 --> 00:02:42,240
其实有比较集中的数据管理的

65
00:02:42,520 --> 00:02:46,360
在云侧有非常集中的一个数据的管理

66
00:02:46,360 --> 00:02:49,480
因为数据的管理是非常严峻的一个挑战

67
00:02:49,520 --> 00:02:51,680
数据不仅是数据库的数据

68
00:02:51,680 --> 00:02:55,000
而且更多的是一些非结构化的数据

69
00:02:55,320 --> 00:02:56,480
接着可以看到

70
00:02:56,600 --> 00:02:59,480
其实模型在云端更好的去保护

71
00:02:59,480 --> 00:03:01,840
现在对模型的保护其实是比较弱的

72
00:03:01,840 --> 00:03:04,520
算法的侵入其实是比较简单的

73
00:03:04,520 --> 00:03:07,640
这块又衍生了一个 AI 安全的问题

74
00:03:08,080 --> 00:03:09,080
大家听完就算了

75
00:03:09,080 --> 00:03:13,000
接下来看一下云端也会遇到一些问题

76
00:03:13,240 --> 00:03:17,520
云端的问题就是人工智能的服务成本非常高昂

77
00:03:17,720 --> 00:03:19,840
因为需要云端的服务

78
00:03:19,840 --> 00:03:21,440
你就需要大量的机器

79
00:03:21,440 --> 00:03:22,440
大量的容灾

80
00:03:22,440 --> 00:03:24,480
大量的各种设备维护

81
00:03:24,480 --> 00:03:25,400
机房

82
00:03:25,400 --> 00:03:27,560
所以它整体的成本是很高的

83
00:03:27,560 --> 00:03:29,480
然后看一下云端的服务

84
00:03:29,480 --> 00:03:31,840
对网络的依赖是非常高的

85
00:03:31,840 --> 00:03:32,840
如果断网了

86
00:03:32,840 --> 00:03:34,960
基本上你的推理服务就断了

87
00:03:34,960 --> 00:03:36,760
做推荐的时候滑着滑着

88
00:03:37,040 --> 00:03:38,680
没有网络了你就滑不动了

89
00:03:38,960 --> 00:03:41,920
我经常晚上刷抖音的时候会遇到这种问题

90
00:03:42,120 --> 00:03:43,640
就自己断网了

91
00:03:43,920 --> 00:03:47,480
接着看一下数据隐私是个很大的问题

92
00:03:47,880 --> 00:03:49,680
用户的数据如果不出端

93
00:03:49,960 --> 00:03:53,400
这时候推理服务就怎么去保护用户

94
00:03:53,400 --> 00:03:55,680
数据的隐私是个很大的挑战

95
00:03:55,680 --> 00:03:58,920
接着数据的传输也是个很大的成本

96
00:03:59,360 --> 00:04:02,160
现在传一张图片比较简单

97
00:04:02,720 --> 00:04:05,080
假设是端侧的摄像头

98
00:04:05,560 --> 00:04:07,720
要传大量的视频数据的时候

99
00:04:07,920 --> 00:04:11,440
这个时候对数据传输压力或者安全隐私

100
00:04:11,440 --> 00:04:12,680
是个很大的挑战

101
00:04:12,960 --> 00:04:17,200
最后云端基本上很少会做一些定制化的模型

102
00:04:17,360 --> 00:04:19,200
一般都会做一些通用的模型

103
00:04:19,440 --> 00:04:21,360
因为定制化的模型是非常难的

104
00:04:21,360 --> 00:04:22,720
就千人千面这个问题

105
00:04:22,840 --> 00:04:25,840
还是一直是努力去解决的问题

106
00:04:27,120 --> 00:04:30,440
接下来看一下部署态的另外一面

107
00:04:30,440 --> 00:04:32,440
就是 Edge 端侧

108
00:04:34,200 --> 00:04:38,400
端侧端侧的设备和资源是非常紧张的

109
00:04:38,400 --> 00:04:40,400
特别是手机或者 IoT 的设备

110
00:04:40,400 --> 00:04:42,720
里面是因为靠一块

111
00:04:42,960 --> 00:04:46,080
像手指夹那么大小的一块芯片去支撑

112
00:04:46,240 --> 00:04:48,400
也只能靠一块电池

113
00:04:49,000 --> 00:04:51,840
如果算法把所有里面的计算资源都用起来了

114
00:04:51,840 --> 00:04:54,800
耗电量是很大的

115
00:04:56,160 --> 00:04:58,000
不过有个好处就是它的响应

116
00:04:58,200 --> 00:05:00,360
在设备里面就自我完成了

117
00:05:00,360 --> 00:05:01,280
就自闭环了

118
00:05:01,840 --> 00:05:04,840
就不需要去消耗云端的一些资源

119
00:05:06,200 --> 00:05:09,120
看一下端侧的一些挑战

120
00:05:09,480 --> 00:05:12,840
端侧的挑战就是会有非常严格的一个功耗

121
00:05:12,840 --> 00:05:15,240
热量模型大小所受限

122
00:05:15,560 --> 00:05:20,120
因为之前说的内存只有两个 G 或者 4 个 G

123
00:05:20,120 --> 00:05:21,960
大家所有的设备去共用的

124
00:05:21,960 --> 00:05:25,560
功耗或者一块手机里面

125
00:05:25,920 --> 00:05:28,160
就 400 毫安完全不够用

126
00:05:28,160 --> 00:05:30,920
在这个时候就有非常多的约束

127
00:05:30,920 --> 00:05:34,600
而且硬件的算力对于推理来说是远远不够的

128
00:05:34,600 --> 00:05:38,560
对于算力的需求是非常的多的

129
00:05:38,560 --> 00:05:40,680
而且数据非常分散

130
00:05:40,680 --> 00:05:44,280
就我之前第一份工作的时候是做情景感知

131
00:05:44,480 --> 00:05:48,880
就会去收集非常多的手机的一个传感器的信息

132
00:05:48,880 --> 00:05:53,160
一台手机就可以获取接近 30 多种传感器的设备了

133
00:05:53,160 --> 00:05:55,240
所以数据是非常分散的

134
00:05:55,240 --> 00:05:58,320
而且模型在边缘更容易受到攻击

135
00:05:58,320 --> 00:06:01,000
就我去攻击你的模型是很简单的

136
00:06:01,000 --> 00:06:04,800
而且这个时候平台非常多样化

137
00:06:04,800 --> 00:06:07,720
不同的设备可能会有自己不同的平台

138
00:06:08,200 --> 00:06:11,800
包括有一款推力引擎部署在端侧

139
00:06:12,080 --> 00:06:16,840
这个时候端侧跟在手机上面用的又是不同的一款

140
00:06:16,840 --> 00:06:18,240
这款叫做 Macro

141
00:06:18,240 --> 00:06:20,880
这款叫做 Lite 推理的引擎

142
00:06:20,880 --> 00:06:23,360
所以说它基本上没法做到一个通用

143
00:06:24,880 --> 00:06:27,000
下面看一下端侧

144
00:06:27,000 --> 00:06:30,920
就是 Edge 端的一些要解决的一些技术点

145
00:06:31,040 --> 00:06:33,000
第一个就是应用的算法优化

146
00:06:33,000 --> 00:06:34,480
需要对算法优化

147
00:06:34,480 --> 00:06:36,400
就是训练的时候跟推理的时候

148
00:06:36,400 --> 00:06:38,040
希望越小越好

149
00:06:38,040 --> 00:06:39,680
但是精度越高越好

150
00:06:39,680 --> 00:06:41,160
其实这就是个矛盾

151
00:06:41,280 --> 00:06:44,440
接着去看看高效率的模型的设计

152
00:06:44,440 --> 00:06:46,600
这个时候希望模型越小越好

153
00:06:46,600 --> 00:06:49,080
因为不管是在手机上面跑起来小

154
00:06:49,080 --> 00:06:51,440
网络传输的时候也很小

155
00:06:51,720 --> 00:06:53,000
装一个 APP

156
00:06:53,000 --> 00:06:55,120
APP 里面有非常多的算法

157
00:06:55,120 --> 00:06:57,280
这些算法不可能都放在 APP 的

158
00:06:57,280 --> 00:07:00,560
而是在运行的时候从网络加载进来的

159
00:07:00,560 --> 00:07:04,760
这个时候对模型的尺寸要求是非常高的

160
00:07:04,760 --> 00:07:07,480
最后端侧是有非常多的推力引擎的

161
00:07:07,480 --> 00:07:10,840
自己也是推出了一个 MindSpore Lite 推力引擎

162
00:07:10,840 --> 00:07:13,480
这个是 HMS Core 里面去跑的一个后端

163
00:07:13,520 --> 00:07:17,840
另外还会要去对接很多不同的芯片

164
00:07:17,840 --> 00:07:20,360
这些就是它的一个主要的特点

165
00:07:22,200 --> 00:07:27,120
下面更多是在云侧部署和端侧部署的一个具体的区别

166
00:07:27,120 --> 00:07:30,560
分开算力、时间、网络能耗很多的方面去对比

167
00:07:30,560 --> 00:07:31,760
大家看一下就好了

168
00:07:31,760 --> 00:07:34,080
我就不再给大家去慢慢的列了

169
00:07:35,760 --> 00:07:38,440
接下来我去给大家汇报另外一个内容

170
00:07:38,440 --> 00:07:42,200
就是云侧部署和推理的方式

171
00:07:42,200 --> 00:07:46,400
云侧部署其实之前比较系统的给大家汇报过

172
00:07:46,400 --> 00:07:50,440
整个推力系统在云侧可能会做很多的请求

173
00:07:50,440 --> 00:07:52,960
监控、调度还有推力引擎

174
00:07:52,960 --> 00:07:55,800
更多的是模型的管理这些相关的工作

175
00:07:55,800 --> 00:07:58,400
这块这里面就不详细的展开

176
00:07:59,800 --> 00:08:02,040
去到另外一个内容

177
00:08:02,040 --> 00:08:04,920
就是边缘部署的推理的方式

178
00:08:05,360 --> 00:08:07,080
这里面有 5 种不同的

179
00:08:07,120 --> 00:08:08,360
都是我刚画完的图案

180
00:08:08,360 --> 00:08:10,320
画这个图案还挺复杂的

181
00:08:10,320 --> 00:08:12,480
希望大家可以给我一个赞

182
00:08:14,440 --> 00:08:18,720
首先第一种就是纯粹在边缘里面去做一个推理的

183
00:08:18,720 --> 00:08:21,960
包括在手机、耳机还有手环上面

184
00:08:21,960 --> 00:08:23,920
去做一个简单的推理

185
00:08:23,920 --> 00:08:26,920
这个时候对时延和对模型

186
00:08:26,920 --> 00:08:28,520
就这个只能是小模型

187
00:08:28,520 --> 00:08:30,280
对模型要求是非常高的

188
00:08:31,560 --> 00:08:33,240
第二种方式就是我的模型

189
00:08:33,240 --> 00:08:35,640
我的算法是跑在我的云端的

190
00:08:35,840 --> 00:08:39,520
这种方式更多是在推荐的场景

191
00:08:39,520 --> 00:08:43,640
会把边缘设备的一些功能去做一个加密

192
00:08:43,640 --> 00:08:45,240
加密完之后丢给云端

193
00:08:45,240 --> 00:08:48,080
云端算完之后传给边缘的设备

194
00:08:48,960 --> 00:08:54,120
第三种方式就是边缘设备跟云端服务器做一个联动的

195
00:08:54,120 --> 00:08:56,920
首先边缘设备更多是跑一个小模型

196
00:08:56,920 --> 00:08:59,680
跑一个小模型得到一个简单的结果

197
00:08:59,680 --> 00:09:03,560
这个简单的结果就会做一个简单的预测

198
00:09:03,680 --> 00:09:06,360
但是可能有一些部分的数据

199
00:09:06,360 --> 00:09:09,520
部分的结果是没有办法去在手机端上面去跑的

200
00:09:09,520 --> 00:09:13,240
这个时候就会把这些数据传给边缘的服务器

201
00:09:13,240 --> 00:09:15,640
用较大的模型去跑出一个结果

202
00:09:15,640 --> 00:09:18,760
这种就是小模型跟大模型的联动

203
00:09:18,760 --> 00:09:21,880
在边缘的设备里面做一些简单的预测

204
00:09:23,520 --> 00:09:25,920
为啥要有这种这么奇怪的方式

205
00:09:26,080 --> 00:09:29,240
是因为其实有一些数据是不出端的

206
00:09:29,240 --> 00:09:31,640
用户对这些数据的隐私的保护

207
00:09:31,640 --> 00:09:32,680
要求非常严格的

208
00:09:32,680 --> 00:09:34,600
例如举个很简单的例子

209
00:09:34,600 --> 00:09:36,840
就是相册的推荐

210
00:09:37,600 --> 00:09:41,400
点开华为相册的发现就会看到

211
00:09:41,640 --> 00:09:45,000
其实对人像地点事物拍照的方式

212
00:09:45,280 --> 00:09:48,160
还有一些美食做了一个归类的

213
00:09:48,400 --> 00:09:51,000
像这种归类这些数据其实是不出端的

214
00:09:51,000 --> 00:09:53,880
用户对这些数据的隐私要求非常高

215
00:09:53,880 --> 00:09:55,960
这个时候会做一个小模型

216
00:09:55,960 --> 00:09:57,720
在这里面做一个简单的决策

217
00:09:57,720 --> 00:09:59,240
但是这个小模型怎么来

218
00:09:59,400 --> 00:10:00,880
可能会通过大模型

219
00:10:00,920 --> 00:10:02,920
对大量的数据进行训练

220
00:10:02,920 --> 00:10:04,520
训练完之后再推过来

221
00:10:04,520 --> 00:10:07,080
要变成一个小模型做一个简单的决策的

222
00:10:07,080 --> 00:10:09,640
这个就是第三种方式

223
00:10:09,640 --> 00:10:11,680
边缘跟云端联动

224
00:10:13,360 --> 00:10:16,200
第四个方式就是分布式计算的

225
00:10:16,480 --> 00:10:17,560
看一下这个图

226
00:10:17,680 --> 00:10:19,120
这个字就不念了

227
00:10:19,120 --> 00:10:22,240
然后这个图就是我有非常多的边缘的设备

228
00:10:22,240 --> 00:10:23,400
有 IoT 的设备

229
00:10:23,400 --> 00:10:24,720
还有边缘的服务器

230
00:10:25,000 --> 00:10:28,360
这个时候我通过网络去把这些数据

231
00:10:28,360 --> 00:10:29,600
去把这些基站

232
00:10:29,640 --> 00:10:32,040
去把这些边缘的设备把它串通起来

233
00:10:32,040 --> 00:10:35,760
这种方式就是充分的去利用了我的不同的边

234
00:10:35,760 --> 00:10:38,960
不同的 IoT 的设备的一个算力

235
00:10:39,200 --> 00:10:42,720
这种方式用的更多的是联邦学习

236
00:10:42,720 --> 00:10:44,400
这是一个非常大的

237
00:10:44,400 --> 00:10:47,400
又是另外一个谷歌提出来的一个 idea

238
00:10:47,400 --> 00:10:48,520
他当时候提出来了

239
00:10:48,520 --> 00:10:50,560
就希望大量的谷歌的设备

240
00:10:50,920 --> 00:10:53,720
想把它的一些算力给用起来

241
00:10:53,720 --> 00:10:55,760
在睡觉和充了电的时候

242
00:10:56,040 --> 00:10:58,960
能不能把手机上面的算力用起来

243
00:10:58,960 --> 00:11:00,160
去做一些计算

244
00:11:00,160 --> 00:11:00,960
做一些训练

245
00:11:00,960 --> 00:11:01,640
做一些学习

246
00:11:02,200 --> 00:11:04,800
这个时候就会用到分布式计算的功能了

247
00:11:07,880 --> 00:11:09,920
最后一个就是方式 5

248
00:11:09,920 --> 00:11:12,240
跨设备的一个 offloading

249
00:11:12,480 --> 00:11:13,840
上面字我也不念了

250
00:11:13,840 --> 00:11:15,600
看一下下面的图

251
00:11:15,800 --> 00:11:19,160
这种方式其实应用我是没有想太懂的

252
00:11:19,160 --> 00:11:21,240
但是确实有这种方式存在

253
00:11:21,240 --> 00:11:23,600
第一个就是边缘设备

254
00:11:23,880 --> 00:11:25,840
去跑模型的一部分

255
00:11:25,840 --> 00:11:26,760
就是模型的切片

256
00:11:27,280 --> 00:11:28,960
然后边缘服务器

257
00:11:29,200 --> 00:11:31,000
因为不可能说

258
00:11:31,000 --> 00:11:33,520
每一台边缘的设备直接连到云端的

259
00:11:33,520 --> 00:11:36,080
有可能中间是通过边缘服务器

260
00:11:36,080 --> 00:11:40,080
去做一个缓存或者做一个交接的工作

261
00:11:40,320 --> 00:11:43,400
这个时候这个部分就跑一些模型的切片

262
00:11:43,680 --> 00:11:47,400
云端又去跑另外一部分模型的切片

263
00:11:48,120 --> 00:11:51,320
可以把这一个上面的边缘设备

264
00:11:51,320 --> 00:11:51,960
边缘服务器

265
00:11:52,040 --> 00:11:53,320
还有云端数据中心

266
00:11:53,440 --> 00:11:54,520
看成一个整体

267
00:11:54,520 --> 00:11:57,040
它去一起去训练一个模型

268
00:11:57,280 --> 00:11:59,880
这种就是跨设备的一种 offloading

269
00:12:00,400 --> 00:12:02,320
跨设备的一个边缘推理的方式

270
00:12:04,080 --> 00:12:05,680
好了来回顾一下

271
00:12:05,680 --> 00:12:07,280
今天去看了一下

272
00:12:07,280 --> 00:12:10,400
部署态的一个跟训练态的具体的区别

273
00:12:10,400 --> 00:12:13,000
特别是部署它云侧跟边缘的一个

274
00:12:13,000 --> 00:12:13,920
有什么不一样

275
00:12:13,920 --> 00:12:15,680
它们之间的一个特点

276
00:12:15,680 --> 00:12:17,200
了解完这个之后

277
00:12:17,520 --> 00:12:18,920
就给大家去汇报

278
00:12:18,920 --> 00:12:22,120
云侧推理流程到底有哪些功能

279
00:12:23,120 --> 00:12:25,160
最后去看了一下

280
00:12:25,160 --> 00:12:27,280
边缘部署的方式有 5 种

281
00:12:27,280 --> 00:12:30,480
每种方式都有对应的应用场景

282
00:12:30,480 --> 00:12:35,120
这个时候就衍生了非常多新的技术点了

283
00:12:35,120 --> 00:12:36,840
这个也是对推理系统

284
00:12:36,840 --> 00:12:39,120
有一个比较大的压力的挑战

285
00:12:39,120 --> 00:12:42,280
它需要去适配很多不同的部署的方式

286
00:12:42,280 --> 00:12:44,200
今天的内容就到这里为止

287
00:12:44,200 --> 00:12:45,400
好了谢谢各位

288
00:12:45,400 --> 00:12:47,560
卷的不行了

289
00:12:47,560 --> 00:12:49,000
记得一键三连加关注

290
00:12:49,320 --> 00:12:51,040
所有的内容都会开源在

291
00:12:51,080 --> 00:12:52,600
下面这条链接里面

292
00:12:53,040 --> 00:12:53,920
拜了拜

