1
00:00:00,140 --> 00:00:04,700
字幕生成: 粟君杰 字幕校对: 粟君杰

2
00:00:04,700 --> 00:00:05,240
嗨

3
00:00:05,240 --> 00:00:05,870
兄弟们

4
00:00:06,609 --> 00:00:09,249
这是大模型与分布式训练系列

5
00:00:09,249 --> 00:00:11,460
里面的数据并行啊

6
00:00:11,460 --> 00:00:14,760
实际上之前呢，讲了大模型的算法

7
00:00:14,760 --> 00:00:17,580
但是呢，只是聚焦于算法的结构本身

8
00:00:17,580 --> 00:00:18,480
并没有去展开

9
00:00:18,480 --> 00:00:21,640
这些算法在大规模服务器或者集群里面

10
00:00:21,640 --> 00:00:23,230
是怎么样并行操作的

11
00:00:23,230 --> 00:00:24,400
接下来的内容呢

12
00:00:24,400 --> 00:00:25,900
希望去分享一下

13
00:00:25,900 --> 00:00:28,290
分布式并行的一些具体的操作

14
00:00:28,290 --> 00:00:29,430
那分布式并行

15
00:00:29,430 --> 00:00:32,890
其实主要是分为数据并行和模型并行

16
00:00:32,890 --> 00:00:34,330
而模型并行里面呢

17
00:00:34,330 --> 00:00:36,840
又有了张量并行和流水线并行

18
00:00:36,840 --> 00:00:40,320
最后把之前学到的几个并行的操作

19
00:00:40,320 --> 00:00:41,100
放在一起

20
00:00:41,100 --> 00:00:42,510
做一个混合并行

21
00:00:42,510 --> 00:00:45,720
让之前聊到的大模型的算法结构

22
00:00:45,720 --> 00:00:48,720
能够在集群上面

23
00:00:48,720 --> 00:00:50,980
更好更快地训练起来

24
00:00:51,260 --> 00:00:53,440
当手头上只有一个英特尔

25
00:00:53,440 --> 00:00:55,600
酷睿i3笔记本的时候

26
00:00:55,600 --> 00:00:58,780
想玩大模型还是洗洗睡吧

27
00:00:58,780 --> 00:01:02,360
但是呢，当钱已经多到花不完了

28
00:01:02,360 --> 00:01:05,960
那可以去购买昇腾服务器或者V100

29
00:01:05,960 --> 00:01:07,980
或者用华为云的AI集群

30
00:01:07,980 --> 00:01:10,710
这个时候就可以去享受

31
00:01:10,710 --> 00:01:13,560
分布式训练给带来的乐趣

32
00:01:13,560 --> 00:01:15,810
但是如果我有那么多钱的时候

33
00:01:15,810 --> 00:01:18,360
我为什么要当算法工程师呢

34
00:01:18,360 --> 00:01:19,740
唉，造孽

35
00:01:20,820 --> 00:01:22,080
玩笑归玩笑啊

36
00:01:22,080 --> 00:01:24,660
可以看到今天主要来讲的内容呢

37
00:01:24,660 --> 00:01:25,860
就是数据

38
00:01:25,860 --> 00:01:26,580
并行

39
00:01:26,580 --> 00:01:28,400
那数据并行呢

40
00:01:28,400 --> 00:01:31,820
其实它有很多种数据并行的方式

41
00:01:31,820 --> 00:01:33,560
我一开始对数据并行

42
00:01:33,560 --> 00:01:35,680
理解的概念也是非常之粗俗的

43
00:01:35,680 --> 00:01:39,040
我以为数据并行只是简单的对一些Data

44
00:01:39,040 --> 00:01:42,120
就是训练的数据进行一个并行的操作

45
00:01:42,120 --> 00:01:44,820
随着我对这个知识的深入的了解

46
00:01:44,820 --> 00:01:47,200
我发现数据并且还没有那么简单

47
00:01:47,200 --> 00:01:48,880
第一个呢，就是我刚才聊到的

48
00:01:48,880 --> 00:01:51,700
需要对数据数，所有的数据进行并行

49
00:01:51,700 --> 00:01:55,880
第二个，会对网络模型的参数进行并行

50
00:01:55,880 --> 00:01:58,580
第三个呢，计算完反向的时候呢

51
00:01:58,580 --> 00:02:00,950
会对梯度进行并行

52
00:02:00,950 --> 00:02:04,850
第四个呢，还会对优化器的状态进行并行

53
00:02:04,850 --> 00:02:08,340
所以数据并行的操作和方式有非常多

54
00:02:08,340 --> 00:02:11,010
这里面呢，又分为简单的数据并行

55
00:02:11,010 --> 00:02:12,940
还有分布式数据并行

56
00:02:12,940 --> 00:02:15,280
还有全切片数据并行

57
00:02:15,280 --> 00:02:16,960
下面呢会以Pytorch

58
00:02:16,960 --> 00:02:18,520
第三种并行的方式

59
00:02:18,520 --> 00:02:23,000
去了解一下数据并行的不同形态

60
00:02:23,000 --> 00:02:24,980
在正式的内容开始之前呢

61
00:02:24,980 --> 00:02:26,030
我想提个疑问

62
00:02:26,030 --> 00:02:29,120
一般网络模型或者CV卷积网络模型呢

63
00:02:29,120 --> 00:02:30,580
一般是输入一张图

64
00:02:30,580 --> 00:02:33,370
就是256x256长和宽

65
00:02:33,370 --> 00:02:35,560
再乘以一个三通道的图片

66
00:02:35,560 --> 00:02:37,760
给神经网络进行训练的

67
00:02:37,760 --> 00:02:39,920
但是像下面的这些图啊

68
00:02:39,920 --> 00:02:41,600
其实都是一些卫星图

69
00:02:41,600 --> 00:02:44,660
卫星的图像是非常的大的

70
00:02:44,660 --> 00:02:46,160
那像卫星的图片呢

71
00:02:46,160 --> 00:02:47,240
可能以一个例子

72
00:02:47,240 --> 00:02:49,700
就是我现在有一张卫星的图片

73
00:02:49,700 --> 00:02:52,320
左边的那种图层也是非常多的

74
00:02:52,320 --> 00:02:55,200
它的图片的大小也是非常夸张的

75
00:02:55,200 --> 00:02:57,480
已经到万级乘以万级了

76
00:02:57,480 --> 00:03:00,510
那这个时候对这个卫星的图像

77
00:03:00,510 --> 00:03:02,910
丢给神经网络去处理的时候

78
00:03:02,910 --> 00:03:05,280
它还是做数据并行吗

79
00:03:05,280 --> 00:03:07,980
这个问题呢，等分享完这一节内容的时候

80
00:03:07,980 --> 00:03:11,880
再来一起回顾一下

81
00:03:13,200 --> 00:03:17,060
首先要分享的第一个内容就是数据并行

82
00:03:17,060 --> 00:03:18,920
最原始的数据并行呢

83
00:03:18,920 --> 00:03:20,940
假设现在有两台设备

84
00:03:20,940 --> 00:03:23,130
然后把数据切成一半

85
00:03:23,130 --> 00:03:25,650
一半的数据就给设备一

86
00:03:25,650 --> 00:03:28,200
另外一半的数据给设备二

87
00:03:28,200 --> 00:03:29,880
通过前向的计算

88
00:03:29,880 --> 00:03:30,720
反向的计算

89
00:03:30,720 --> 00:03:32,100
得到梯度

90
00:03:32,100 --> 00:03:35,760
最后对数据进行同步和梯度累积之后呢

91
00:03:35,760 --> 00:03:40,000
就完成了数据并行的第一个step的训练

92
00:03:40,000 --> 00:03:42,940
简单的对训练的数据进行并行呢

93
00:03:42,940 --> 00:03:45,640
它的代码实现起来还是比较简单的

94
00:03:45,640 --> 00:03:48,700
这是实施的过程的第一个训练

95
00:03:48,700 --> 00:03:51,640
数据的并行是使用多线程去实现的

96
00:03:51,640 --> 00:03:55,720
python的多线程呢，就会被GIL所约束

97
00:03:55,720 --> 00:03:57,160
在原理上

98
00:03:57,160 --> 00:03:59,830
每台机器都会有一个独立的模型

99
00:03:59,830 --> 00:04:03,540
只是利用了集群的单节点的算力

100
00:04:03,540 --> 00:04:05,580
这样就会引起一个问题

101
00:04:05,580 --> 00:04:09,440
每台机器计算完之后都会有自己独立的梯度

102
00:04:09,440 --> 00:04:12,200
需要把这些梯度呢，进行汇合

103
00:04:12,200 --> 00:04:13,620
然后做梯度累积

104
00:04:13,620 --> 00:04:14,880
那谈到梯度累积

105
00:04:14,880 --> 00:04:16,710
就会引起两个问题

106
00:04:16,710 --> 00:04:20,040
第一个就是什么时间节点进行梯度累积

107
00:04:20,040 --> 00:04:25,980
第二个就是如何更高效地去进行梯度累积

108
00:04:26,400 --> 00:04:30,580
下面来看一下梯度累积的一个具体的概念

109
00:04:30,580 --> 00:04:32,620
提了好几次梯度累积

110
00:04:32,620 --> 00:04:34,600
但是梯度累积到底是什么呢

111
00:04:34,600 --> 00:04:36,820
层次的这个呢，就是数据

112
00:04:36,820 --> 00:04:39,320
现在把数据分成Mini-Batch

113
00:04:39,320 --> 00:04:42,140
对数据进行分布式并行操作

114
00:04:42,140 --> 00:04:43,860
放在不同的机器去执行

115
00:04:43,860 --> 00:04:47,280
那每一台机器呢就会维护自己的一个网络模型

116
00:04:47,280 --> 00:04:48,840
进行一个前向的计算

117
00:04:48,840 --> 00:04:52,260
反向的计算，计算梯度

118
00:04:52,840 --> 00:04:53,880
梯度累积

119
00:04:53,880 --> 00:04:57,660
就是把不同设备的梯度进行一个求和汇总

120
00:04:57,660 --> 00:05:00,000
然后跟更新服务器参数的网络模型

121
00:05:00,000 --> 00:05:02,000
最后分发给每一台机器

122
00:05:02,000 --> 00:05:04,910
然后再进行下一次数据的迭代

123
00:05:04,910 --> 00:05:06,260
下面的这一部分呢

124
00:05:06,260 --> 00:05:08,890
就是梯度累积的过程和操作

125
00:05:10,060 --> 00:05:12,780
梯度累积的时间节点呢，分为两种

126
00:05:12,780 --> 00:05:15,520
一种是同步梯度累积的方式

127
00:05:15,520 --> 00:05:17,080
同步的梯度累积呢

128
00:05:17,080 --> 00:05:20,140
就是每一台机器都算完自己的梯度之后呢

129
00:05:20,140 --> 00:05:22,120
聚集到PS服务器里面

130
00:05:22,120 --> 00:05:23,960
然后统一进行更新

131
00:05:23,960 --> 00:05:27,500
这里面呢，就会把三个箭头进行统一更新

132
00:05:27,500 --> 00:05:30,680
这种方式就是严格的按照时间序列

133
00:05:30,680 --> 00:05:32,090
来去进行执行

134
00:05:32,090 --> 00:05:35,930
保证网络模型的收敛能够得到保证

135
00:05:35,930 --> 00:05:38,330
但是快速可以看到像Device 2

136
00:05:38,330 --> 00:05:40,220
它的计算时间特别长

137
00:05:40,220 --> 00:05:42,530
而Device 1 计算时间特别短

138
00:05:42,530 --> 00:05:45,520
这里面呢，就会出现严重的资源损耗

139
00:05:45,520 --> 00:05:48,940
里面的白色的这些框框叫做bubble

140
00:05:48,940 --> 00:05:50,260
出现了大量的bubble

141
00:05:50,260 --> 00:05:51,700
大量的时间浪费

142
00:05:51,700 --> 00:05:55,090
还会浪费大量的通讯的开销时间

143
00:05:55,090 --> 00:05:58,900
另外一种梯度累积的方式呢，就是异步更新

144
00:05:58,900 --> 00:06:00,400
异步更新比较简单

145
00:06:00,400 --> 00:06:03,340
我每一个网络模型都是单独的去更新

146
00:06:03,340 --> 00:06:04,980
自己的服务器参数

147
00:06:04,980 --> 00:06:07,500
这种方式带来的好处就是没有了

148
00:06:07,500 --> 00:06:09,600
刚才白色的bubble 

149
00:06:09,600 --> 00:06:11,280
Device 1 进行完前向计算了

150
00:06:11,280 --> 00:06:12,600
再进行反向的计算

151
00:06:12,600 --> 00:06:13,980
再进行正向的计算

152
00:06:13,980 --> 00:06:16,000
再进行反向的计算

153
00:06:16,000 --> 00:06:18,250
极大的减少了通讯的过程

154
00:06:18,250 --> 00:06:19,780
但是带来的问题就是

155
00:06:19,780 --> 00:06:22,260
我的网络模型会很难去收敛

156
00:06:22,260 --> 00:06:24,960
我的更新的过程是不断的去迭代

157
00:06:24,960 --> 00:06:27,780
不断的去覆盖原来的副本的

158
00:06:29,080 --> 00:06:31,320
因此在实际的训练环节里面

159
00:06:31,320 --> 00:06:34,960
更多的是采用同步的梯度更新的方式

160
00:06:34,960 --> 00:06:38,460
下面来看看梯度累积的通讯的具体的方式

161
00:06:38,460 --> 00:06:41,940
假设下面以左边的GPU 0作为参数服务器

162
00:06:41,940 --> 00:06:44,800
也就是左边的这块卡作为参数服务器

163
00:06:44,800 --> 00:06:46,600
在做梯度累积的时候呢

164
00:06:46,600 --> 00:06:48,160
会把Worker 2，Worker 4

165
00:06:48,160 --> 00:06:52,960
Worker 3的数据都同时回传到Master Worker 1里面

166
00:06:52,960 --> 00:06:55,180
然后进行统一的更新之后呢

167
00:06:55,180 --> 00:06:57,100
再分发给不同的Worker

168
00:06:57,100 --> 00:06:58,660
这种呢，就是GPU 0

169
00:06:58,660 --> 00:07:01,270
作为参数服务器的同步更新方式

170
00:07:01,270 --> 00:07:05,140
另外一种就是参数服务器分布在所有GPU里面

171
00:07:05,140 --> 00:07:06,580
让所有GPU呢

172
00:07:06,580 --> 00:07:09,220
实际上网络呢，会形成一个环

173
00:07:09,220 --> 00:07:10,360
通过这个环呢

174
00:07:10,360 --> 00:07:13,500
Worker 1的参数会给Worker 2进行同步

175
00:07:13,500 --> 00:07:15,120
Worker 2给Worker 4

176
00:07:15,120 --> 00:07:16,980
Worker 3给Worker 1

177
00:07:16,980 --> 00:07:19,200
通过之前的分享内容

178
00:07:19,200 --> 00:07:21,800
只要进行两次遍历环之后

179
00:07:21,800 --> 00:07:25,700
四个参数服务器都有所有数据的备份

180
00:07:25,700 --> 00:07:30,000
这时候呢，就完成整个梯度累积的同步了

181
00:07:31,000 --> 00:07:33,540
现在来看看Pytorch的分布式

182
00:07:33,540 --> 00:07:35,940
数据并行的具体的实现方式

183
00:07:35,940 --> 00:07:39,060
DDP呢，采用的是多进程的实现方式

184
00:07:39,060 --> 00:07:41,460
所以呢，没有了Python的GIL锁

185
00:07:41,460 --> 00:07:42,300
更加灵活

186
00:07:42,300 --> 00:07:44,880
更加方便的去启用多个进程

187
00:07:44,880 --> 00:07:49,260
第二个就是每个进程并不是同步所有的参数

188
00:07:49,260 --> 00:07:52,180
而是同步梯度的误差

189
00:07:52,180 --> 00:07:55,540
带来的好处就是减少了通讯的数据

190
00:07:55,540 --> 00:07:58,420
第三个呢，就采用Ring All-Reduce的方式

191
00:07:58,420 --> 00:08:00,520
去提升通讯效率

192
00:08:00,520 --> 00:08:04,120
右边的这个图呢，就是DDP的原理图

193
00:08:04,120 --> 00:08:06,580
跟DP的原理呢，是不一样的

194
00:08:06,580 --> 00:08:10,380
DP刚才只是对训练的数据进行同步

195
00:08:10,380 --> 00:08:13,260
DDP呢，就是对梯度进行更新

196
00:08:13,260 --> 00:08:15,240
就是规点可以看到层次的

197
00:08:15,240 --> 00:08:16,800
这个呢，就是规点

198
00:08:16,800 --> 00:08:20,180
而梯度的同步呢，并不是算完整个网络模型

199
00:08:20,180 --> 00:08:22,490
最后把所有的服务器进行更新的

200
00:08:22,490 --> 00:08:25,240
而是每一次呢，进行一个分桶

201
00:08:25,240 --> 00:08:28,660
所以可以在图中看到有好几个bucket

202
00:08:28,660 --> 00:08:31,740
然后每一台服务器都有自己的bucket

203
00:08:31,740 --> 00:08:33,120
这就是第一步操作

204
00:08:33,120 --> 00:08:34,980
对梯度进行分桶

205
00:08:34,980 --> 00:08:38,040
第二点就是对梯度进行逆向排序

206
00:08:38,040 --> 00:08:42,520
去确定什么时候哪个梯度先进行更新

207
00:08:42,520 --> 00:08:46,060
那第三个呢，就是跳过一些已经很久没有更新

208
00:08:46,060 --> 00:08:48,780
或者时间太慢的一些梯度

209
00:08:48,780 --> 00:08:51,120
第四点就是进行一个集合通讯

210
00:08:51,120 --> 00:08:53,580
让从右边的这个图来看看

211
00:08:53,580 --> 00:08:55,860
首先在进行计算的时候呢

212
00:08:55,860 --> 00:08:57,600
就会有很多梯度

213
00:08:57,600 --> 00:09:00,720
那梯度呢，会把它们放在不同的桶里面

214
00:09:00,720 --> 00:09:03,660
假设我鼠标所在的addmm2这一层

215
00:09:03,660 --> 00:09:05,140
已经完成了计算了

216
00:09:05,140 --> 00:09:07,960
那我bucket的数据两边都已经具备了

217
00:09:07,960 --> 00:09:11,030
这时候我就会直接进行一个的通讯

218
00:09:11,030 --> 00:09:12,860
在通讯的同时

219
00:09:12,860 --> 00:09:16,280
我的admm1 这一层已经开始计算了

220
00:09:16,280 --> 00:09:20,420
这样就极大地去利用了执行时间的异步

221
00:09:20,420 --> 00:09:24,340
最大化地利用了通讯的空载时间

222
00:09:24,340 --> 00:09:27,140
下面的这个图呢，会更加清晰

223
00:09:27,140 --> 00:09:29,090
每一次呢，都会对规点

224
00:09:29,090 --> 00:09:31,400
不同的梯度呢，进行分桶

225
00:09:31,400 --> 00:09:32,480
分完桶之后呢

226
00:09:32,480 --> 00:09:34,260
再进行一个同步的操作

227
00:09:34,260 --> 00:09:36,960
然后下次计算的时候再进行分桶

228
00:09:36,960 --> 00:09:38,370
然后再进行同步

229
00:09:38,370 --> 00:09:41,720
就利用了计算和通讯的时间

230
00:09:43,020 --> 00:09:44,540
在通讯方式里面呢

231
00:09:44,540 --> 00:09:46,200
之前已经讲过了

232
00:09:46,200 --> 00:09:48,420
多对多的同步的通讯方式呢

233
00:09:48,420 --> 00:09:52,339
其实可以由一对多或者多对一进行组成的

234
00:09:52,339 --> 00:09:54,679
而这里面All Reduce的这种方式

235
00:09:54,679 --> 00:09:58,290
可以用Reduce-Scatter加上All-Gether两种方式

236
00:09:58,290 --> 00:10:01,470
刚才无论是DP还是DP的使用

237
00:10:01,470 --> 00:10:03,930
都是All-Reduce的集合通讯的方式

238
00:10:03,930 --> 00:10:06,760
对梯度进行同步更新

239
00:10:07,220 --> 00:10:11,260
实际上这波操作可以拆解为Reduce-Scatter和All-Gether

240
00:10:11,260 --> 00:10:14,900
回顾一下DP呢，是对训练的数据进行并行

241
00:10:14,900 --> 00:10:18,520
而DPP呢，是对梯度的数据进行并行

242
00:10:18,520 --> 00:10:21,700
而网络模型除了这两种数据

243
00:10:21,700 --> 00:10:23,800
其实还有网络模型的参数

244
00:10:23,800 --> 00:10:25,540
和优化器的状态

245
00:10:25,540 --> 00:10:28,780
同样可以对所有网络模型的数据

246
00:10:28,780 --> 00:10:30,500
进行并行更新

247
00:10:30,500 --> 00:10:32,900
这时候呢，又引入了一个新的数据

248
00:10:32,900 --> 00:10:33,620
并行的方式

249
00:10:33,620 --> 00:10:38,260
叫做FSDP全数据切片的并行方式

250
00:10:38,260 --> 00:10:40,060
这种数据并行方式呢

251
00:10:40,060 --> 00:10:43,000
就把刚才讲到的网络模型的参数梯度

252
00:10:43,000 --> 00:10:45,980
还有优化器的状态都进行并行更新

253
00:10:45,980 --> 00:10:48,410
另外还会对静态的内存

254
00:10:48,410 --> 00:10:50,660
去卸载到CPU里面

255
00:10:50,660 --> 00:10:53,540
进一步的去提升NPU的内存

256
00:10:53,540 --> 00:10:56,720
下面这个图就是FSDP具体的执行方式

257
00:10:56,720 --> 00:10:59,920
首先第一种就是会对数据进行简单的并行

258
00:10:59,920 --> 00:11:01,480
进行一个前向的计算

259
00:11:01,480 --> 00:11:03,490
然后再进行一个反向的计算

260
00:11:03,490 --> 00:11:04,660
反向计算完之后

261
00:11:04,660 --> 00:11:08,330
会对网络模型的权重参数进行

262
00:11:08,330 --> 00:11:10,260
Reduce-Scatter的同步方式

263
00:11:10,260 --> 00:11:12,360
然后去更新网络模型

264
00:11:12,360 --> 00:11:14,040
权重更新完之后呢

265
00:11:14,040 --> 00:11:16,520
在这里面会执行一个All-Gather

266
00:11:16,520 --> 00:11:19,780
把数据同步给每一块卡

267
00:11:19,780 --> 00:11:21,100
对比起DDP

268
00:11:21,100 --> 00:11:23,020
把All-Reduce分开

269
00:11:23,020 --> 00:11:27,140
两个通讯的操作变成了Reduce-Scatter加上All-Gather

270
00:11:27,140 --> 00:11:31,310
另外还会对网络模型的参数和优化器的状态

271
00:11:31,310 --> 00:11:32,900
进行同步更新

272
00:11:32,900 --> 00:11:35,600
进一步的去利用了计算时候

273
00:11:35,600 --> 00:11:37,780
网络通讯空载的时间

274
00:11:37,780 --> 00:11:41,320
更进一步的利用了集群的性能

275
00:11:41,320 --> 00:11:43,480
下面这个图呢就是Pytorch

276
00:11:43,480 --> 00:11:46,840
去实现FSDP的一个详细的流程图

277
00:11:46,840 --> 00:11:47,500
可以看到

278
00:11:47,500 --> 00:11:50,420
首先会把数据进行并行切分

279
00:11:50,420 --> 00:11:52,760
接着呢去进行一个前向的计算

280
00:11:52,760 --> 00:11:53,930
反向的计算

281
00:11:53,930 --> 00:11:56,260
然后去进行reduce get

282
00:11:56,260 --> 00:11:59,320
接着把一些不需要用到的权重参数

283
00:11:59,320 --> 00:12:02,240
去offload到CPU里面

284
00:12:02,240 --> 00:12:04,040
等需要用到的时候

285
00:12:04,040 --> 00:12:07,870
再把权重参数load到NPU里面

286
00:12:07,870 --> 00:12:09,940
接着再做一个All-Gather的操作

287
00:12:09,940 --> 00:12:11,240
如此往复

288
00:12:11,240 --> 00:12:13,910
中间通过All-Gather跟Reduce-Scatter

289
00:12:13,910 --> 00:12:16,720
对梯度进行累积同步更新

290
00:12:16,720 --> 00:12:19,240
讲完DP、DDP、FSDP

291
00:12:19,240 --> 00:12:22,720
3种Pytorch的不同的数据并行的操作

292
00:12:22,720 --> 00:12:24,770
下面来简单的去看看

293
00:12:24,770 --> 00:12:27,660
Pytorch的具体的实现的架构和逻辑

294
00:12:27,660 --> 00:12:29,220
在最简单的DP

295
00:12:29,220 --> 00:12:31,770
就是对训练的数据进行并行

296
00:12:31,770 --> 00:12:34,980
他用的是多进程的方式去实现的

297
00:12:34,980 --> 00:12:37,710
在DDP分布式数据并行里面的

298
00:12:37,710 --> 00:12:40,060
具体是使用多线程的方式

299
00:12:40,060 --> 00:12:41,500
这时候在集群里面呢

300
00:12:41,500 --> 00:12:44,060
就涉及到多卡和跨机器进行通讯

301
00:12:44,060 --> 00:12:46,280
这时候可能会用MPI、NCCL

302
00:12:46,280 --> 00:12:48,410
HCCL或者GLOO

303
00:12:48,410 --> 00:12:52,540
其中一种的方式对数据进行集合通讯

304
00:12:52,540 --> 00:12:55,300
最后就是FSDP

305
00:12:55,300 --> 00:12:57,220
实际上是使用的RPC

306
00:12:57,220 --> 00:12:59,800
远程过程调用的协议进行计算的

307
00:12:59,800 --> 00:13:03,220
而Pytorch里面呢就提供了RPC call，RPC fc

308
00:13:03,220 --> 00:13:05,080
还有分布式Autograd

309
00:13:05,080 --> 00:13:07,520
还有分布式优化器的等API

310
00:13:07,520 --> 00:13:11,790
去实现FSDP，好了

311
00:13:11,790 --> 00:13:12,900
在这一节里面呢

312
00:13:12,900 --> 00:13:15,120
讲了数据并行的三种方式

313
00:13:15,120 --> 00:13:16,000
一种是DP

314
00:13:16,000 --> 00:13:16,900
一种是DDP

315
00:13:16,900 --> 00:13:18,700
另外一种是FSDP

316
00:13:18,700 --> 00:13:20,980
不管是哪种数据并行的方式

317
00:13:20,980 --> 00:13:22,720
希望对训练的数据

318
00:13:22,720 --> 00:13:25,080
对网络模型的梯度权重参数

319
00:13:25,080 --> 00:13:26,490
还有优化器的状态

320
00:13:26,490 --> 00:13:30,720
都作为网络模型的数据进行并行操作

321
00:13:30,720 --> 00:13:33,780
而DP呢,跟DDP和FSDP呢

322
00:13:33,780 --> 00:13:37,240
分别使用了不同的数据并行方式去实现

323
00:13:38,500 --> 00:13:39,340
谢谢各位

324
00:13:39,340 --> 00:13:40,600
拜了个拜

325
00:13:40,600 --> 00:13:41,680
卷的不行了

326
00:13:41,680 --> 00:13:42,490
卷的不行了

327
00:13:42,490 --> 00:13:44,180
记得一键三连加关注哦

328
00:13:44,180 --> 00:13:47,600
所有的内容都会开源在下面这条链接里面

329
00:13:47,600 --> 00:13:48,620
拜了个拜