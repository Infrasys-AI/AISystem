1
00:00:00,140 --> 00:00:04,000
字幕生成: 粟君杰 字幕校对: 粟君杰

2
00:00:04,670 --> 00:00:05,300
hello

3
00:00:05,300 --> 00:00:05,780
大家好

4
00:00:05,780 --> 00:00:07,040
我是ZOMI

5
00:00:07,040 --> 00:00:09,980
今天来到大模型与分布式训练里面的

6
00:00:09,980 --> 00:00:12,740
张量并行这个内容里面

7
00:00:12,740 --> 00:00:14,540
那之前隔了一段时间呢

8
00:00:14,540 --> 00:00:16,340
是因为公司到年底了

9
00:00:16,340 --> 00:00:20,490
各种汇报对齐开会，占了非常多的时间

10
00:00:20,490 --> 00:00:24,540
明显更新的速率要比之前的慢了很多

11
00:00:24,540 --> 00:00:26,340
但模型要真正训练起来

12
00:00:26,340 --> 00:00:28,860
其实不仅仅需要非常多的money

13
00:00:28,860 --> 00:00:30,280
还有服务器集群

14
00:00:30,280 --> 00:00:33,160
更多的是需要分布式并行

15
00:00:33,160 --> 00:00:34,960
或者分布式训练的能力

16
00:00:34,960 --> 00:00:37,900
分布式训练里面呢，又分为模型并行

17
00:00:37,900 --> 00:00:40,900
而模型并行里面有两个非常重要的

18
00:00:40,900 --> 00:00:42,980
就是对模型进行切分

19
00:00:42,980 --> 00:00:46,380
今天主要来聊聊张量并行的这个概念

20
00:00:46,380 --> 00:00:49,590
首先会从一个张量并行的一个原理来讲

21
00:00:49,590 --> 00:00:51,900
然后用三个算子去讲讲

22
00:00:51,900 --> 00:00:53,940
张量是怎么做并行的

23
00:00:53,940 --> 00:00:57,240
最后呢，去讲讲张量重排在MindSpore里面

24
00:00:57,240 --> 00:00:58,800
非常重要的一个概念

25
00:00:58,800 --> 00:01:00,240
在上一节里面呢

26
00:01:00,240 --> 00:01:01,860
讲了数据并行

27
00:01:01,860 --> 00:01:03,480
数据并行其实分为DP

28
00:01:03,480 --> 00:01:07,480
DDP和FSDP 3种不同的并行模式

29
00:01:07,480 --> 00:01:10,270
那之前呢，留了一个遗留问题

30
00:01:10,270 --> 00:01:12,700
假设我输入的图片的通道有非常多

31
00:01:12,700 --> 00:01:14,720
我输入的图片呢也是非常大

32
00:01:14,720 --> 00:01:17,750
那这时候呢，训练的数据这么大的情况下

33
00:01:17,750 --> 00:01:21,000
是属于数据并行吗

34
00:01:21,000 --> 00:01:24,060
诶这个问题呢，上一节没有回答哦

35
00:01:24,060 --> 00:01:26,000
这里面真正来回答一下

36
00:01:26,000 --> 00:01:28,190
不管是训练书的数据呢

37
00:01:28,190 --> 00:01:31,220
还是神经网络里面去执行的数据

38
00:01:31,220 --> 00:01:33,039
统一都称为张量

39
00:01:33,039 --> 00:01:35,859
所以对这个这么大的数据进行切分呢

40
00:01:35,859 --> 00:01:38,180
叫做张量并行

41
00:01:38,180 --> 00:01:40,880
下面来看看张量并行和流水线并行的

42
00:01:40,880 --> 00:01:41,780
不同之处啊

43
00:01:41,780 --> 00:01:43,580
第一个就是流水线并行

44
00:01:43,580 --> 00:01:46,680
可以看到左边的这个图呢，就是流水线并行

45
00:01:46,680 --> 00:01:49,620
流水线并行是按照模型的layer进行切

46
00:01:49,620 --> 00:01:50,880
分到不同的机器的

47
00:01:50,880 --> 00:01:53,020
我第一层切换到第一台机器

48
00:01:53,020 --> 00:01:55,000
第二层切换到第二台机器

49
00:01:55,000 --> 00:01:56,620
那这种层间的并行呢

50
00:01:56,620 --> 00:01:58,890
叫做流水线并行

51
00:01:58,890 --> 00:02:01,999
下一个内容呢，会来介绍的张量并行呢

52
00:02:01,999 --> 00:02:03,379
就像右边的这个图

53
00:02:03,379 --> 00:02:06,229
将计算图的层内的不同的参数

54
00:02:06,229 --> 00:02:07,660
切换到不同的机器

55
00:02:07,660 --> 00:02:09,130
假设是同一层的

56
00:02:09,130 --> 00:02:12,180
我把这一层的参数或者这一层的张量

57
00:02:12,180 --> 00:02:15,270
切分成Device 1和Device 2去执行

58
00:02:15,270 --> 00:02:16,740
那这种层内的并行

59
00:02:16,740 --> 00:02:18,700
叫做张量并行

60
00:02:18,700 --> 00:02:19,840
张量并行呢

61
00:02:19,840 --> 00:02:22,320
实际上会引起两个重要的思考

62
00:02:22,320 --> 00:02:24,360
如何切分我的网络模型

63
00:02:24,360 --> 00:02:26,160
我是从这个位置进行切分呢

64
00:02:26,160 --> 00:02:28,840
还是这个位置对半切分呢

65
00:02:28,840 --> 00:02:30,760
那第二个值得思考的就是

66
00:02:30,760 --> 00:02:34,360
怎么样去保证我切分完之后的正确性呢

67
00:02:34,360 --> 00:02:35,320
Device 1

68
00:02:35,320 --> 00:02:36,340
第一层的神经元

69
00:02:36,340 --> 00:02:37,540
其实是跟Device 2

70
00:02:37,540 --> 00:02:40,260
最后一层的神经元有相关联联系的

71
00:02:40,260 --> 00:02:43,060
如何保证切分之后的正确性呢

72
00:02:43,060 --> 00:02:45,760
这两个问题都是非常值得大家去思考的

73
00:02:48,700 --> 00:02:50,680
Oh My God~

74
00:02:52,880 --> 00:02:54,400
讲完背景知识之后呢

75
00:02:54,400 --> 00:02:57,160
正式的来去了解一下数学原理

76
00:02:57,160 --> 00:02:58,840
数学原理其实很简单啊

77
00:02:58,840 --> 00:03:01,980
张量的切分方式呢主要分为两种

78
00:03:01,980 --> 00:03:04,050
第一种呢，就是行的切分

79
00:03:04,050 --> 00:03:06,630
假设现在有一个2x2的矩阵

80
00:03:06,630 --> 00:03:07,800
按行的切分呢

81
00:03:07,800 --> 00:03:10,280
很明显就是直接按行切分

82
00:03:10,280 --> 00:03:13,310
那第二种切分方式呢就按列来切分

83
00:03:13,310 --> 00:03:16,100
那第三种呢其实它不属于一种切分方式啊

84
00:03:16,100 --> 00:03:17,620
它更多是一个复制

85
00:03:17,620 --> 00:03:21,010
就把一个原始的矩阵复制到不同的机器里面

86
00:03:21,010 --> 00:03:22,060
一句话来说

87
00:03:22,060 --> 00:03:23,020
数学原理很简单

88
00:03:23,020 --> 00:03:25,200
但是组合起来就很复杂

89
00:03:25,200 --> 00:03:26,880
X乘以A等于Y

90
00:03:26,880 --> 00:03:27,540
那X呢

91
00:03:27,540 --> 00:03:30,360
假设是网络模型的输入或者激活

92
00:03:30,360 --> 00:03:31,560
A就是权重

93
00:03:31,560 --> 00:03:32,900
Y就是输出

94
00:03:32,900 --> 00:03:36,500
把A就是权重呢，按列来进行切分

95
00:03:36,500 --> 00:03:38,160
那我就切分了乘以两一个

96
00:03:38,160 --> 00:03:38,820
一个A1 

97
00:03:38,820 --> 00:03:39,600
一个A2 

98
00:03:39,600 --> 00:03:40,320
X乘以A1 

99
00:03:40,320 --> 00:03:42,060
A2呢，就等于Y

100
00:03:42,060 --> 00:03:44,700
那第二个呢，A按行的切分

101
00:03:44,700 --> 00:03:45,880
我按行来切分

102
00:03:45,880 --> 00:03:49,720
这个时候我X就必须按照列进行切分

103
00:03:49,720 --> 00:03:51,010
1x2的矩阵

104
00:03:51,010 --> 00:03:53,050
再乘以2x1的矩阵

105
00:03:53,050 --> 00:03:55,880
最后才能形成一个1x1的矩阵

106
00:03:55,880 --> 00:03:59,930
这个时候我的数呢，就被迫按照列来进行切分

107
00:03:59,930 --> 00:04:01,580
按照不同的切分方式

108
00:04:01,580 --> 00:04:04,360
会影响到我前后的输入输出

109
00:04:04,360 --> 00:04:06,820
下面呢正式的去一个MatMul

110
00:04:06,820 --> 00:04:10,870
就是矩阵乘或者GMMLiner这种矩阵乘的算子

111
00:04:10,870 --> 00:04:12,340
并行作为一个例子

112
00:04:12,340 --> 00:04:15,910
那同样的公式X乘以A等于Y X作为输入

113
00:04:15,910 --> 00:04:17,740
A作为算子的权重

114
00:04:17,740 --> 00:04:20,440
现在呢，我把A按照列进行切分

115
00:04:20,440 --> 00:04:21,720
就竖着来切

116
00:04:21,720 --> 00:04:24,640
这时候呢，我就分开一个A1和A2

117
00:04:24,640 --> 00:04:28,570
X乘以A1 A2 等于Y1 Y2 两个数

118
00:04:28,570 --> 00:04:30,640
最后两个数我要把它拼起来

119
00:04:30,640 --> 00:04:34,440
变成一个矩阵Y我需要通过一个All-Gather的方式

120
00:04:34,440 --> 00:04:36,420
因为Y1在Device 1

121
00:04:36,420 --> 00:04:37,740
Y2 在Device 2

122
00:04:37,740 --> 00:04:40,180
我把它拼起来的时候就需要一个通讯

123
00:04:40,180 --> 00:04:43,020
所以这里面呢，用了All-Gather的通讯方式

124
00:04:43,020 --> 00:04:46,770
第二种方式呢，就是我按照行的方式进行切分

125
00:04:46,770 --> 00:04:49,800
那我的X呢，就必须按照列进行切分

126
00:04:49,800 --> 00:04:51,300
所以看看下面这个图

127
00:04:53,480 --> 00:04:56,300
这时候呢，我的X就需要进行通讯

128
00:04:56,300 --> 00:04:57,140
放在两半

129
00:04:57,140 --> 00:04:58,340
放在不同的机器

130
00:04:58,340 --> 00:05:00,660
然后A呢，按照正常的进行切分

131
00:05:00,660 --> 00:05:04,200
X1乘以A1等于Y1，X2乘以A2 

132
00:05:04,200 --> 00:05:05,760
等于我的Y2

133
00:05:05,760 --> 00:05:07,380
得到Y1，Y2之后呢

134
00:05:07,380 --> 00:05:09,560
现在还是在两台不同的机器

135
00:05:09,560 --> 00:05:12,660
最后要执行一个Reduce的通讯

136
00:05:12,660 --> 00:05:14,580
才能执行这个加号的操作

137
00:05:16,680 --> 00:05:17,940
有了MatMul的理解呢

138
00:05:17,940 --> 00:05:20,160
来看看Transformer的MLP

139
00:05:20,160 --> 00:05:24,000
其实Transformer的MLP呢，只是多了一个激活在这里面

140
00:05:24,000 --> 00:05:26,220
这里面呢，还是按照刚才的方式

141
00:05:26,220 --> 00:05:28,280
第一种就是按行进行切分

142
00:05:28,280 --> 00:05:29,480
按行的切分的方式

143
00:05:29,480 --> 00:05:33,060
意味着我的X就必须要被迫着按照列进行切分

144
00:05:33,060 --> 00:05:35,490
但是后面有一个激活函数

145
00:05:35,490 --> 00:05:37,080
那在执行激活函数的时候呢

146
00:05:37,080 --> 00:05:39,220
我需要进行一个加的操作

147
00:05:39,240 --> 00:05:41,860
这意味着我的Y1、Y2 

148
00:05:41,860 --> 00:05:45,740
就必须要进行一个All-Reduce Sum的通讯

149
00:05:45,740 --> 00:05:47,360
所以从右边的这个图呢

150
00:05:47,360 --> 00:05:49,160
这里面通讯就有两次了

151
00:05:49,160 --> 00:05:52,640
第一次就是在Split这个地方把X分成两个

152
00:05:52,640 --> 00:05:55,580
第二个呢，就是在执行GeLU之前呢

153
00:05:55,580 --> 00:05:57,200
再做一个聚合的通信

154
00:05:57,200 --> 00:06:00,520
第二种方式呢，就是A按照列的方式进行切分

155
00:06:00,520 --> 00:06:02,770
我把A切换成A1 A2 

156
00:06:02,770 --> 00:06:05,440
这个时候我的GeLU就可以独立起来

157
00:06:05,440 --> 00:06:06,680
得到Y1、Y2 

158
00:06:06,680 --> 00:06:08,840
最后我再做一个All-Gather的方式

159
00:06:08,840 --> 00:06:11,090
对Y1、Y2进行拼接起来

160
00:06:11,090 --> 00:06:13,400
但是，又但是了

161
00:06:13,400 --> 00:06:15,710
Transformer里面的MLP还没有这么简单

162
00:06:15,710 --> 00:06:18,380
它后面还会接一个Dropout

163
00:06:18,380 --> 00:06:20,480
这个时候我的Y1，Y2出来之后呢

164
00:06:20,480 --> 00:06:23,650
还需要乘以另外一个权重

165
00:06:23,650 --> 00:06:25,570
就是B得到Z1、Z2 

166
00:06:25,570 --> 00:06:26,800
然后再合并

167
00:06:26,800 --> 00:06:30,960
那这个时候我之前的Y1、Y2呢，已经是两个了

168
00:06:30,960 --> 00:06:34,170
我最后我的B呢，只要按照行切分就可以了

169
00:06:34,170 --> 00:06:35,820
我A按照列来切分

170
00:06:35,820 --> 00:06:38,140
我的B按照行的方式来切分

171
00:06:38,140 --> 00:06:39,700
最后执行MLP呢

172
00:06:39,700 --> 00:06:42,970
只是在开始跟结尾的时候做一个通讯

173
00:06:46,040 --> 00:06:47,780
第一个呢，就是对X进行广播或者Copy，那第二个就是All-Reduce

174
00:06:47,780 --> 00:06:48,980
把它聚合起来

175
00:06:48,980 --> 00:06:50,240
然后做一个Dropout

176
00:06:50,240 --> 00:06:51,560
在集群里面

177
00:06:51,560 --> 00:06:54,560
通讯的成本会比计算的成本要高很多

178
00:06:54,560 --> 00:06:56,170
因为通讯的时间很慢

179
00:06:56,170 --> 00:06:58,540
计算现在已经到了5nm的

180
00:06:58,540 --> 00:07:01,060
晶体管的计算速率是非常高的

181
00:07:01,060 --> 00:07:03,760
所以在网络模型或者计算图里面

182
00:07:03,760 --> 00:07:05,440
对张量进行切分

183
00:07:05,440 --> 00:07:06,900
张量并行的时候

184
00:07:06,900 --> 00:07:10,460
要考虑的就是我的整个网络模型怎么切分

185
00:07:10,460 --> 00:07:13,490
才能够让通讯时间更短

186
00:07:13,490 --> 00:07:14,300
更少

187
00:07:14,300 --> 00:07:18,830
使得整网的训练效率更高呃

188
00:07:18,830 --> 00:07:22,280
那在Transformer里面的第二个重要的就是Self-Attention

189
00:07:22,280 --> 00:07:24,470
来看看Self-Attention的具体执行

190
00:07:24,470 --> 00:07:26,480
Self-Attention其实比较简单

191
00:07:26,480 --> 00:07:28,970
主要是由Q、K、V进行一个矩阵乘

192
00:07:28,970 --> 00:07:30,880
然后加个Softmax，Dropout

193
00:07:30,880 --> 00:07:31,960
得到我的Y1 

194
00:07:31,960 --> 00:07:34,570
最后呢再执行一个Dropout得到我的Z

195
00:07:34,570 --> 00:07:37,930
那这个方式呢，刚才的MLP其实类似的

196
00:07:37,930 --> 00:07:42,070
同样我的Q、K、V呢是按列的方式进行切分

197
00:07:42,070 --> 00:07:44,320
我的B呢，按行进行切分

198
00:07:44,320 --> 00:07:46,760
使得我只要一开始做一个通讯

199
00:07:46,760 --> 00:07:48,900
结尾的时候再做一次通讯

200
00:07:48,900 --> 00:07:51,120
就执行完我一个Self-Attention的

201
00:07:51,120 --> 00:07:51,600
成了

202
00:07:54,840 --> 00:07:56,580
Transformer一开始推出的时候

203
00:07:56,580 --> 00:07:58,440
主是处理NLP的

204
00:07:58,440 --> 00:08:01,300
而NLP的第一个数呢，就是我的Embedding层

205
00:08:01,300 --> 00:08:03,880
一般用来压缩我的vocabulary

206
00:08:03,880 --> 00:08:06,600
或者作为vocabulary-size的第一场输入

207
00:08:06,600 --> 00:08:09,720
在大模型GPT2里面的vocabulary-size

208
00:08:09,720 --> 00:08:11,100
就已经有5万多个了

209
00:08:11,100 --> 00:08:12,520
再加上hidden-size

210
00:08:12,520 --> 00:08:15,920
其实第一层的网络模型就已经非常的巨大了

211
00:08:15,920 --> 00:08:19,850
那这个时候想把它切换到不同的机器里面

212
00:08:19,850 --> 00:08:22,740
才能够塞得多更多的词汇表

213
00:08:22,740 --> 00:08:24,720
那一般的推荐的方式呢

214
00:08:24,720 --> 00:08:26,520
就是按列的方式进行切分

215
00:08:26,520 --> 00:08:29,000
就是按照词表的方式进行切分

216
00:08:29,000 --> 00:08:32,060
前2万个单词呢，我就放在Device 1

217
00:08:32,060 --> 00:08:35,720
前后后两个单词呢我就放在Device 2里面

218
00:08:35,720 --> 00:08:37,160
通过列的切分方式

219
00:08:37,160 --> 00:08:38,510
然后得到Y1，Y2 

220
00:08:38,510 --> 00:08:40,400
最后执行一个All-Gather的通讯

221
00:08:40,400 --> 00:08:41,300
得到Y

222
00:08:41,300 --> 00:08:42,560
再输给Transformer

223
00:08:42,560 --> 00:08:44,359
网络模型的结构层里面

224
00:08:45,059 --> 00:08:47,839
但是Embedding

225
00:08:47,839 --> 00:08:52,260
它不仅仅是用在LLM大规模语言模型里面哦

226
00:08:52,260 --> 00:08:54,960
他还用在推荐模型里面

227
00:08:54,960 --> 00:08:56,520
所以推荐模型里面呢

228
00:08:56,520 --> 00:08:58,620
又有了一个Embedding的切换方式

229
00:08:58,620 --> 00:09:00,780
那下面有两个例子

230
00:09:00,780 --> 00:09:03,420
在推荐领域呢，有两种切换方式

231
00:09:03,420 --> 00:09:05,700
一种是Table-wise的切换方式

232
00:09:05,700 --> 00:09:08,240
一种是Column-wise的切换方式

233
00:09:08,240 --> 00:09:10,700
假设在推荐网络模型里面呢

234
00:09:10,700 --> 00:09:13,120
有很多物体的输入特征

235
00:09:13,120 --> 00:09:16,780
Table-wise呢，就是按照物体的特征进行切分的

236
00:09:16,780 --> 00:09:20,900
我可能把特征4 2 0，切换到GPU 0里面

237
00:09:20,900 --> 00:09:23,720
我把特征3 1切换到Device 2里面

238
00:09:23,720 --> 00:09:26,040
那这是一种Table-wise的切换方式

239
00:09:26,040 --> 00:09:29,220
第二种Column-wise是最通用最常用的

240
00:09:29,220 --> 00:09:31,950
会在混合并行里面详细介绍的

241
00:09:31,950 --> 00:09:33,240
Column-wise的这种方式呢

242
00:09:33,240 --> 00:09:36,729
就是我把所有的特征融合到一起

243
00:09:36,729 --> 00:09:40,480
然后把特征的Embedding表按照列的方式进行切换

244
00:09:40,480 --> 00:09:42,820
可以看到一个物体的Embedding特征呢

245
00:09:42,820 --> 00:09:45,440
我可以放在GPU 0，GPU 1里面

246
00:09:45,440 --> 00:09:48,440
这种就是按照Column-wise进行一个切分的

247
00:09:48,440 --> 00:09:51,860
接下来这个内容呢可能会稍微复杂难的一点

248
00:09:51,860 --> 00:09:53,760
就是cross entropy lost

249
00:09:53,760 --> 00:09:56,280
就是损失函数的并行

250
00:09:56,280 --> 00:10:00,120
对损失函数进行并行呢，有两种场景

251
00:10:00,120 --> 00:10:01,980
第一种呢，就是在大词汇表

252
00:10:01,980 --> 00:10:03,360
或者语言模型里面呢

253
00:10:03,360 --> 00:10:05,580
我的logits的规模是非常大的

254
00:10:05,580 --> 00:10:06,720
其实刚才讲的

255
00:10:06,720 --> 00:10:10,210
我的vocabulary-size是非常多的

256
00:10:10,210 --> 00:10:11,500
我在计算的时候

257
00:10:11,500 --> 00:10:14,080
同样需要把这个vocabulary-size呢

258
00:10:14,080 --> 00:10:15,360
传给loss函数

259
00:10:15,360 --> 00:10:16,860
label也是非常多的

260
00:10:16,860 --> 00:10:18,780
因为label对应词表嘛

261
00:10:18,780 --> 00:10:20,100
所以要考虑到

262
00:10:20,100 --> 00:10:23,840
把整个词表拆分到不同的记忆上面

263
00:10:24,720 --> 00:10:27,680
第二种场景呢，就是我的分类场景

264
00:10:27,680 --> 00:10:30,200
一些极端的高难度的挑战任务里面

265
00:10:30,200 --> 00:10:33,470
可能对图像的分类就有上万种

266
00:10:33,470 --> 00:10:34,520
上10万种

267
00:10:34,520 --> 00:10:36,320
这个时候如果类别非常的大

268
00:10:36,320 --> 00:10:38,580
也会导致单卡或者

269
00:10:38,580 --> 00:10:40,860
单芯片里面呢，没法储存的计算

270
00:10:40,860 --> 00:10:42,510
logit这个矩阵

271
00:10:42,510 --> 00:10:45,720
所以就必须要按照类别的维度进行切分

272
00:10:45,720 --> 00:10:48,060
不管按照类的方式来进行切分呢

273
00:10:48,060 --> 00:10:50,180
还是按照词表的方式进行切分

274
00:10:50,180 --> 00:10:52,880
都是按照列的切换方式

275
00:10:52,880 --> 00:10:56,220
下面一起来回顾一下交叉熵损失函数

276
00:10:56,220 --> 00:10:57,780
那在二分类的时候呢

277
00:10:57,780 --> 00:11:02,020
每个类别的预测概率是p和1-p

278
00:11:02,020 --> 00:11:03,820
真实的预测值呢是Y1

279
00:11:03,820 --> 00:11:06,160
让另外一个预测不到的就是1-Y1

280
00:11:06,160 --> 00:11:07,620
这是二分类的情况

281
00:11:07,620 --> 00:11:09,960
那把它拓展到多分类情况里面呢

282
00:11:09,960 --> 00:11:11,480
就有两个求和了

283
00:11:11,480 --> 00:11:12,740
对于多分类的方式

284
00:11:12,740 --> 00:11:14,300
假设现有m个类别

285
00:11:14,300 --> 00:11:17,420
Pic呢，就是每一个类别的预测概率

286
00:11:17,420 --> 00:11:19,820
这里面呢，以语言模型作为例子

287
00:11:19,820 --> 00:11:22,380
可以看到这里面有一个vocabulary-size

288
00:11:22,380 --> 00:11:24,780
第一步就是要进行数据的拆分

289
00:11:24,780 --> 00:11:28,460
把输入的数据按vocabulary-size进行拆分

290
00:11:28,460 --> 00:11:30,470
那假设我现在有四台机器

291
00:11:30,470 --> 00:11:34,320
我可能会把数据按照列的方式进行拆分

292
00:11:34,320 --> 00:11:36,360
而label呢就是真值

293
00:11:36,360 --> 00:11:39,000
一开始它可能是一些具体的单词

294
00:11:39,000 --> 00:11:41,780
可首先会对具体的单词

295
00:11:41,780 --> 00:11:43,520
执行一个onehot的向量的操作

296
00:11:43,520 --> 00:11:46,180
然后再把它Scatter到不同的机器上面

297
00:11:46,180 --> 00:11:48,040
就变成右边的这个图

298
00:11:48,040 --> 00:11:50,580
logits就是输入有四个部分

299
00:11:50,580 --> 00:11:53,460
然后label也对应有四个部分

300
00:11:53,460 --> 00:11:56,220
第二步呢，就是最大值同步了

301
00:11:56,220 --> 00:11:57,360
最大值同步呢

302
00:12:02,140 --> 00:12:03,940
中间求最大值的时候

303
00:12:03,940 --> 00:12:05,560
就通过All Reduce(Max)

304
00:12:05,560 --> 00:12:08,680
去保证获取的是一个全局的最大值

305
00:12:08,680 --> 00:12:10,420
而不是某个机器的最大值

306
00:12:10,420 --> 00:12:15,060
那第三步呢就是exp sum和softmax的计算啊

307
00:12:15,060 --> 00:12:17,700
继续来打开看看第三步

308
00:12:17,700 --> 00:12:18,120
第三步

309
00:12:18,120 --> 00:12:20,040
首先我进行一个指数的运算

310
00:12:20,040 --> 00:12:21,900
然后求一个当地local的最大值

311
00:12:21,900 --> 00:12:23,929
然后再求全局的最大值

312
00:12:23,929 --> 00:12:25,820
拿到全局的最大值

313
00:12:25,820 --> 00:12:27,740
然后我再进行一个指数的求和

314
00:12:27,740 --> 00:12:29,630
得到softmax的值了

315
00:12:29,630 --> 00:12:32,700
最后一步呢，就是真正执行lost

316
00:12:32,700 --> 00:12:34,680
会把输入的logits

317
00:12:34,680 --> 00:12:36,480
还有label进行相乘

318
00:12:36,480 --> 00:12:41,010
并且求和得到label所对应的一个位置的值

319
00:12:41,010 --> 00:12:44,900
然后再进行一个All-Reduce Sum全区的同步

320
00:12:44,900 --> 00:12:47,120
然后去计算local softmax

321
00:12:47,120 --> 00:12:49,130
然后加上这个符号的操作

322
00:12:49,130 --> 00:12:53,090
就得到了分布式交叉熵的损失函数值了

323
00:12:53,090 --> 00:12:55,120
看到这一步为止呢

324
00:12:55,120 --> 00:12:56,080
可以看到啊

325
00:12:56,080 --> 00:12:58,090
只是简单的模型并行

326
00:12:58,090 --> 00:13:00,420
会对输入的数据进行并行

327
00:13:00,420 --> 00:13:04,420
还会对输入数据的第一层Embedding层进行并行

328
00:13:04,420 --> 00:13:08,590
中间的Embedding和MLP或者矩阵乘也会做并行

329
00:13:08,590 --> 00:13:12,460
包括我的损失函数也会做张量并行

330
00:13:12,460 --> 00:13:13,600
总的来说

331
00:13:13,600 --> 00:13:16,240
张量并行无处不在

332
00:13:16,400 --> 00:13:19,420
下面呢来看一个额外的知识点

333
00:13:19,420 --> 00:13:21,340
就是随机控制的问题

334
00:13:21,340 --> 00:13:24,920
第一个就是参数初始化的随机性的问题

335
00:13:25,500 --> 00:13:27,920
假设现在只有一块卡的时候呢

336
00:13:27,920 --> 00:13:30,920
可能我只需要设置一个简单的随机种子

337
00:13:30,920 --> 00:13:33,800
然后对Device 1进行一个随机数就可以了

338
00:13:33,800 --> 00:13:36,260
但是我把相同的数据切换成两半

339
00:13:36,260 --> 00:13:37,869
放在不同的机器里面

340
00:13:37,869 --> 00:13:40,449
我两台机器的随机种子都是P的时候

341
00:13:40,449 --> 00:13:42,240
就造成了数学上不等价

342
00:13:42,240 --> 00:13:44,580
失去了真正的随机性了

343
00:13:44,580 --> 00:13:47,760
也就是可能我的E1和E2 里面

344
00:13:47,760 --> 00:13:49,640
初始化的数据都是相同的

345
00:13:49,640 --> 00:13:52,490
但是我刚才只有一台机器的时候

346
00:13:52,490 --> 00:13:56,020
两半部分的随机生成的数据肯定是不同的

347
00:13:56,020 --> 00:13:58,540
于是呢在做多卡切分的时候

348
00:13:58,540 --> 00:14:01,630
就会去使用不同的随机种子

349
00:14:01,630 --> 00:14:04,440
对不同的机器进行初始化

350
00:14:05,140 --> 00:14:07,020
第二个随机控制的问题呢

351
00:14:07,020 --> 00:14:09,440
就是算子计算的一个随机性

352
00:14:09,440 --> 00:14:10,220
可以看到啊

353
00:14:10,220 --> 00:14:13,340
神经网络里面呢不仅仅只有初始化的时候

354
00:14:13,340 --> 00:14:14,690
会引入随机问题

355
00:14:14,690 --> 00:14:15,710
在Dropout

356
00:14:15,710 --> 00:14:17,390
还有TruncatedNormal

357
00:14:17,390 --> 00:14:19,040
StandardNormal

358
00:14:19,040 --> 00:14:20,690
还有StandardLaplace

359
00:14:20,690 --> 00:14:23,540
还有Gamm等不同的情况下

360
00:14:23,540 --> 00:14:26,079
都会引入随机种子

361
00:14:26,079 --> 00:14:30,279
这个时候呢就会造成在网络模型当中会引起的

362
00:14:30,279 --> 00:14:31,670
随机性的问题

363
00:14:31,670 --> 00:14:35,150
假设我把一个Dropout的算子进行张量并行

364
00:14:35,150 --> 00:14:38,480
那这个时候呢我可能会设置不同随机种子

365
00:14:38,480 --> 00:14:42,000
但是如果我把所有的数据都All-Gather或者All-Reduce

366
00:14:42,000 --> 00:14:45,090
这个时候我使用相同一个随机种子就可以了

367
00:14:45,090 --> 00:14:46,380
讲这么多原理呢

368
00:14:46,380 --> 00:14:48,240
实际上不是希望用户

369
00:14:48,240 --> 00:14:50,580
或者开发者自己去设置这些

370
00:14:50,580 --> 00:14:52,920
而是告诉大家这些内容呢

371
00:14:52,920 --> 00:14:54,220
AI框架

372
00:14:54,220 --> 00:14:57,310
AI系统都会帮自动的去处理好

373
00:14:57,310 --> 00:15:01,160
所以大家不用去担心或者纠结在一个八机八卡

374
00:15:01,160 --> 00:15:03,860
64块NPU的集群里面

375
00:15:03,860 --> 00:15:05,300
如果初始化的时候呢

376
00:15:05,300 --> 00:15:08,420
使用同一个随机种子网络模型

377
00:15:08,420 --> 00:15:10,920
最后训练出来的精度是没有

378
00:15:10,920 --> 00:15:11,880
我初始化的时候

379
00:15:11,880 --> 00:15:14,500
使用不同的随机种子的精度要高的

380
00:15:14,500 --> 00:15:15,760
从这个图里面呢

381
00:15:15,760 --> 00:15:18,460
可以看出随机性控制对精度

382
00:15:18,460 --> 00:15:21,670
还有loss收敛还是非常具有影响力的

383
00:15:21,670 --> 00:15:23,380
今天的内容呢稍微多了一点

384
00:15:23,380 --> 00:15:24,960
来总结一下模型

385
00:15:24,960 --> 00:15:27,270
并行呢，分为张量并行和流水线并行

386
00:15:27,270 --> 00:15:29,760
而张量并行呢就是层内并行

387
00:15:29,760 --> 00:15:33,220
流水线并行呢是一个层间并行的概念

388
00:15:33,220 --> 00:15:34,900
那第二就是张量并行

389
00:15:34,900 --> 00:15:36,910
主要是对数据进行切分

390
00:15:36,910 --> 00:15:40,440
切分的方式呢有按行按列的方式进行切分

391
00:15:40,440 --> 00:15:44,560
另外最常见的张量并行就是MatMul的张量并行

392
00:15:44,560 --> 00:15:47,320
通过MatMul这个张量并行的算子原理

393
00:15:47,320 --> 00:15:50,410
很快的去把它扩展到了Embedding、MLP

394
00:15:50,410 --> 00:15:53,650
还有Transformer等不同的算子的并行

395
00:15:53,650 --> 00:15:57,600
可能还会涉及到损失函数的并行

396
00:15:58,120 --> 00:16:00,690
最后一个就是张量并行的时候

397
00:16:00,690 --> 00:16:03,580
要注意随机性的问题

398
00:16:03,580 --> 00:16:06,520
开发AI框架或者AI系统的时候呢

399
00:16:06,520 --> 00:16:09,500
要注意随机性种子的设置

400
00:16:09,500 --> 00:16:10,460
卷的不行了

401
00:16:10,460 --> 00:16:11,330
卷的不行了

402
00:16:11,330 --> 00:16:12,980
记得一键三连加关注哦

403
00:16:12,980 --> 00:16:14,480
所有的内容都会开源

404
00:16:14,480 --> 00:16:17,420
在下面这条链接里面拜了个拜