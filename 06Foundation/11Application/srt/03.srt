1
00:00:00,000 --> 00:00:05,750
字幕生成：mkwei 字幕校对：qiaokai

2
00:00:05,750 --> 00:00:07,375
Hello 大家好我是 ZOMI

3
00:00:07,375 --> 00:00:11,975
来到了 ChartGPT 狂飙原理第三个内容

4
00:00:11,975 --> 00:00:12,000
那在第三个内容里面呢

5
00:00:12,000 --> 00:00:14,541
那在第三个内容里面呢

6
00:00:14,541 --> 00:00:18,000
去看看 InstructGPT 它到底有什么不一样

7
00:00:18,000 --> 00:00:20,596
它到底是怎么的把之前的 PPO

8
00:00:20,596 --> 00:00:21,950
还有强化学习

9
00:00:21,950 --> 00:00:24,000
还有之前的 GPT 融合在一起

10
00:00:24,000 --> 00:00:25,278
那这一节呢

11
00:00:25,278 --> 00:00:27,000
才是真正的内容也是非常深入的内容

12
00:00:27,000 --> 00:00:30,270
如果对前面强化学习和 GPT 不太了解的同学呢

13
00:00:30,270 --> 00:00:33,000
也可以翻看之前的相关的视频

14
00:00:33,000 --> 00:00:34,302
那今天呢首先

15
00:00:34,302 --> 00:00:38,000
简单的去看一看论文的一个简单的原理

16
00:00:38,000 --> 00:00:41,000
因为其实论文它不是讲的非常非常的深入

17
00:00:41,000 --> 00:00:44,000
所以会更加深入的去看看它具体的原理

18
00:00:44,000 --> 00:00:47,000
因为 ChartGPT 呢它并没有开放论文

19
00:00:47,000 --> 00:00:48,933
但是呢它说明了它的前身

20
00:00:48,933 --> 00:00:48,958
是 InstructGPT

21
00:00:48,958 --> 00:00:49,975
是 InstructGPT

22
00:00:50,000 --> 00:00:51,807
而 InstructGPT 呢

23
00:00:51,807 --> 00:00:56,000
是基于 RLHF 微调的 GPT-3

24
00:00:56,000 --> 00:00:59,000
这里面就很有意思了分开两个技术点

25
00:00:59,000 --> 00:01:01,919
那 InstructGPT 它整体的训练流程呢

26
00:01:01,919 --> 00:01:03,000
就是下面这个图

27
00:01:03,000 --> 00:01:07,000
它整篇文章呢基本上只有两个公式一个图

28
00:01:07,000 --> 00:01:09,000
然后呢有 68 页这么多

29
00:01:09,000 --> 00:01:12,000
感兴趣的同学呢也可以看一下这篇论文

30
00:01:12,000 --> 00:01:15,000
那这里面呢就结合了监督学习和强化学习

31
00:01:15,000 --> 00:01:17,215
首先呢是用监督学习

32
00:01:17,215 --> 00:01:20,000
去给 GPT-3 进行一个微调

33
00:01:20,000 --> 00:01:22,655
对应的就是下面这个图左边

34
00:01:22,655 --> 00:01:24,000
最左边这个位置

35
00:01:24,000 --> 00:01:25,663
然后用强化学习 PPO 算法

36
00:01:25,663 --> 00:01:29,000
呢来更新微调过后的 GPT-3 的参数

37
00:01:29,000 --> 00:01:31,000
那这个模型呢叫做 SFT

38
00:01:32,000 --> 00:01:34,687
但是呢有一个很大的问题就是

39
00:01:34,687 --> 00:01:37,000
这三个模型到底是怎么配合的

40
00:01:37,000 --> 00:01:39,925
里面又有个 RM 模型有个 SFT 模型有个 PPO 模型

41
00:01:39,925 --> 00:01:40,000
这里面也有个 RM 模型这两个是同一个模型？

42
00:01:40,000 --> 00:01:42,925
这里面也有个 RM 模型这两个是同一个模型？

43
00:01:43,000 --> 00:01:46,000
这两个模型之间又有什么关系呢

44
00:01:46,000 --> 00:01:48,000
往下继续打开

45
00:01:48,000 --> 00:01:51,575
首先呢分开三个阶段

46
00:01:52,000 --> 00:01:56,000
第一个阶段呢就是利用人类标注的一个数据

47
00:01:56,000 --> 00:01:59,000
对 GPT 进行监督的训练

48
00:01:59,000 --> 00:02:02,168
那现在看一看右边的这个图

49
00:02:02,168 --> 00:02:04,000
里面呢就分开三个步骤

50
00:02:04,000 --> 00:02:07,000
第一步呢就是先设计好一个 Prompt DataSet

51
00:02:07,000 --> 00:02:10,000
里面呢就有大量提示的样本

52
00:02:10,000 --> 00:02:13,000
给出各种各样任务的描述

53
00:02:14,025 --> 00:02:16,159
例如对一个六岁的小孩解析

54
00:02:16,159 --> 00:02:18,000
月亮登陆或者登陆月球

55
00:02:18,900 --> 00:02:21,471
第二步呢就是标注的团队会这个

56
00:02:21,471 --> 00:02:23,000
Prompt 的 DataSet 进行一个标注

57
00:02:23,000 --> 00:02:25,750
也就是人工的回答

58
00:02:25,825 --> 00:02:28,000
那第三个呢就是用标注好的数据

59
00:02:28,000 --> 00:02:30,000
上面标注好的 Prompt 的数据

60
00:02:30,000 --> 00:02:34,000
微调 GPT3 这个网络模型

61
00:02:34,000 --> 00:02:37,000
微调后的网络模型呢叫做 SFT

62
00:02:37,000 --> 00:02:40,000
SFT 呢其实就是 supervision fine-tuning

63
00:02:40,000 --> 00:02:42,000
有监督的微调

64
00:02:42,000 --> 00:02:44,127
让这个 SFT 的模型

65
00:02:44,127 --> 00:02:47,000
具备最基本的文本生成的能力

66
00:02:47,825 --> 00:02:50,625
接下来看看第二个阶段

67
00:02:51,025 --> 00:02:55,000
第二个阶段呢就是通过 Reinforcement Human Feedback 的这个思路呢

68
00:02:55,000 --> 00:02:58,000
来去奖励来去训练 Reward Model

69
00:02:58,000 --> 00:03:02,000
现在呢还是看一看右边的这个图

70
00:03:02,000 --> 00:03:05,000
右边这个图呢分开三个步骤

71
00:03:05,550 --> 00:03:09,000
首先第一步呢就是用刚才训练好的 SFT 网络模型呢

72
00:03:09,000 --> 00:03:12,000
去回答这里面提出的 Prompt 的 DataSet 的问题

73
00:03:12,000 --> 00:03:16,975
然后呢收集四个输出 1234ABCD 四个输出

74
00:03:16,975 --> 00:03:17,000
接着人工对刚才 SFT 模型生成的四个回答

75
00:03:17,000 --> 00:03:21,975
接着人工对刚才 SFT 模型生成的四个回答

76
00:03:22,000 --> 00:03:25,000
进行好坏的判断并进行一个标注

77
00:03:25,000 --> 00:03:29,000
还有排列例如 D 的回答是最正确的 CAB

78
00:03:29,000 --> 00:03:31,000
其实都没有 D 的回答好

79
00:03:31,000 --> 00:03:33,857
然后呢把这个排序的结果

80
00:03:33,857 --> 00:03:37,850
用来训练后面最后的奖励模型

81
00:03:37,850 --> 00:03:39,725
然后去学习排序的结果

82
00:03:39,725 --> 00:03:41,750
从而理解人类的偏好

83
00:03:41,850 --> 00:03:44,000
那这个呢就是 RLHF 里面呢

84
00:03:44,000 --> 00:03:46,000
之前讲到引入了人类的反馈

85
00:03:46,000 --> 00:03:50,000
然后去训练网络模型去训练 Agent

86
00:03:50,000 --> 00:03:54,000
这个 RL 呢 Reward Model 就是对应的 Agent 呢

87
00:03:54,000 --> 00:03:58,000
而上面这个数据集呢就是对应的 Environment

88
00:03:58,000 --> 00:04:01,000
而引入了人类的一个反馈一个评价

89
00:04:01,000 --> 00:04:04,000
这就是 RLHF 上一节里面讲过

90
00:04:04,000 --> 00:04:07,450
这里面呢用真实例子给大家再串一串

91
00:04:09,000 --> 00:04:11,000
接下来来到了第三个阶段

92
00:04:11,000 --> 00:04:13,000
第三个阶段呢是文中比较含糊的

93
00:04:13,000 --> 00:04:15,000
或者没讲太明白

94
00:04:15,000 --> 00:04:17,000
或者很多人没理解的太明白的

95
00:04:17,000 --> 00:04:19,000
但是呢也是非常重要的一个

96
00:04:19,000 --> 00:04:21,537
首先呢主要是通过训练好的 Reward Model

97
00:04:21,537 --> 00:04:23,000
来去预测结果

98
00:04:23,000 --> 00:04:24,481
然后呢通过 PPO 算法

99
00:04:24,481 --> 00:04:27,000
来优化 SFT 的网络模型的策略

100
00:04:27,000 --> 00:04:28,975
所以这里面呢就出现了两个模型

101
00:04:28,975 --> 00:04:29,000
一个是 PPO 一个是 SFT 来更新策略

102
00:04:29,000 --> 00:04:32,975
一个是 PPO 一个是 SFT 来更新策略

103
00:04:33,000 --> 00:04:35,000
之前讲强化学习那一章节里面呢

104
00:04:35,000 --> 00:04:38,000
去讲了策略是怎么更新的如何更新

105
00:04:38,000 --> 00:04:40,000
那这里面呢还有一个 Reward 呢

106
00:04:40,000 --> 00:04:42,000
作为一个 Critics

107
00:04:42,000 --> 00:04:43,000
因为之前讲了

108
00:04:43,000 --> 00:04:45,000
这两个呢是属于 actor

109
00:04:45,000 --> 00:04:47,000
这个 RM 呢是属于 Critics

110
00:04:47,000 --> 00:04:49,975
所以有一个 C 网络有个 A 网络

111
00:04:49,975 --> 00:04:50,000
A 网络呢还有一个 A'网络

112
00:04:50,000 --> 00:04:51,975
A 网络呢还有一个 A'网络

113
00:04:52,000 --> 00:04:54,000
用来同步的更新的策略

114
00:04:54,000 --> 00:04:56,000
让的策略收敛的更快

115
00:04:56,000 --> 00:04:58,000
而且没有那么抖动

116
00:04:58,000 --> 00:04:59,000
而且差异没那么大

117
00:04:59,000 --> 00:05:02,000
下面回到右边的这个图来看一看

118
00:05:02,000 --> 00:05:06,000
首先第一步呢是先准备好 Prompt 的问题

119
00:05:06,000 --> 00:05:09,000
然后呢让 SFT 就是 PPO 模型呢

120
00:05:09,000 --> 00:05:10,000
去回答这个问题

121
00:05:10,000 --> 00:05:12,000
然后得到一个策略的输出

122
00:05:12,000 --> 00:05:14,000
得到这个策略的输出呢

123
00:05:14,000 --> 00:05:17,000
首先不要人工的去评价好坏

124
00:05:17,000 --> 00:05:19,000
而是要阶段 2 的 Reward Model

125
00:05:19,000 --> 00:05:21,000
奖励函数或者奖励模型呢

126
00:05:21,000 --> 00:05:24,000
去给 SFT 呢生成的一个回答

127
00:05:24,000 --> 00:05:25,249
或者一个文本呢

128
00:05:25,249 --> 00:05:26,950
进行一个评价的打分

129
00:05:26,950 --> 00:05:27,000
就是这个 r_k 

130
00:05:27,000 --> 00:05:28,950
就是这个 r_k 

131
00:05:29,000 --> 00:05:32,000
然后呢使用 PPO 的这个算法流程呢

132
00:05:32,050 --> 00:05:36,725
对 SFT 网络模型进行反馈更新

133
00:05:38,000 --> 00:05:39,975
哎基本上到现在为止呢

134
00:05:40,050 --> 00:05:42,800
已经简单的讲完 Instruct GPT

135
00:05:42,875 --> 00:05:46,000
里面这篇文章所涉及到的一些模块了

136
00:05:46,000 --> 00:05:48,000
step1 step2 step3

137
00:05:48,000 --> 00:05:52,000
首先去训练一个 SFT 一个 GPT 网络模型

138
00:05:52,000 --> 00:05:55,000
接着呢去用 GPT 网络模型

139
00:05:55,000 --> 00:05:58,000
生成相关的一些数据文本

140
00:05:58,000 --> 00:06:00,000
然后呢给 Reward Model 去学习

141
00:06:00,000 --> 00:06:02,000
引入了人类的反馈

142
00:06:02,000 --> 00:06:05,000
接着呢在第三步利用了 PPO 的算法

143
00:06:05,000 --> 00:06:08,000
去对整个过程进行学习

144
00:06:08,000 --> 00:06:10,000
但是可能这么讲了

145
00:06:10,000 --> 00:06:12,000
还是没讲太明白

146
00:06:12,000 --> 00:06:14,000
于是呢引入最后一个内容

147
00:06:14,000 --> 00:06:17,000
Instruct GPT 的一个原理的深度剖析

148
00:06:17,000 --> 00:06:19,000
那后面的章节呢不会太多

149
00:06:19,000 --> 00:06:20,000
只有两页 PPT

150
00:06:20,000 --> 00:06:22,000
再往下看一看

151
00:06:22,000 --> 00:06:24,000
实际上呢在阶段 2 啊

152
00:06:24,000 --> 00:06:26,000
主要是训练 Reward Model 嘛

153
00:06:26,000 --> 00:06:28,000
就是奖励函数

154
00:06:28,000 --> 00:06:29,000
那 Reward Model 的核心呢

155
00:06:29,000 --> 00:06:33,000
就是人类对 SFT 的网络模型生成的

156
00:06:33,000 --> 00:06:36,000
多个答案进行排序

157
00:06:36,000 --> 00:06:38,000
那这里面的答案呢

158
00:06:38,000 --> 00:06:41,000
就是通过 SFT 网络模型进行生成的

159
00:06:41,000 --> 00:06:43,000
然后再来训练 Reward Model

160
00:06:43,000 --> 00:06:45,000
整套流程是这样的

161
00:06:45,000 --> 00:06:47,000
那这里面呢可以看到

162
00:06:47,000 --> 00:06:49,000
假设现在已经有四个语句

163
00:06:49,000 --> 00:06:51,000
两两组和一组

164
00:06:51,000 --> 00:06:52,000
两两组一组

165
00:06:52,000 --> 00:06:53,000
A 跟 B 进行组合

166
00:06:53,000 --> 00:06:54,000
A 跟 C 进行组合

167
00:06:54,000 --> 00:06:56,000
A 跟 D 进行组合

168
00:06:56,000 --> 00:06:57,000
C 跟 D 组合

169
00:06:57,000 --> 00:06:58,000
B 跟 D 组合

170
00:06:58,000 --> 00:07:01,000
于是呢就有 C42 个组合

171
00:07:01,000 --> 00:07:02,000
这里面的 K 呢

172
00:07:02,000 --> 00:07:06,000
就是对应于 SFT 生成的答案

173
00:07:06,000 --> 00:07:08,000
于是呢就变成 CK2 这种方式

174
00:07:08,000 --> 00:07:11,000
一共呢就有 CK2 个损失函数

175
00:07:11,000 --> 00:07:12,000
所以可以看到呢

176
00:07:12,000 --> 00:07:13,000
有了这个之后呢

177
00:07:13,000 --> 00:07:16,000
就可以通过 Reward Model

178
00:07:16,000 --> 00:07:18,000
去算损失函数

179
00:07:18,000 --> 00:07:20,000
两两进行组合

180
00:07:20,000 --> 00:07:23,000
最后呢再去加权平均值

181
00:07:23,000 --> 00:07:25,000
公式里面的 r_theta 呢

182
00:07:25,000 --> 00:07:27,000
theta 呢就是 Reward Model

183
00:07:27,000 --> 00:07:29,000
对应的就是 RM 模型

184
00:07:29,000 --> 00:07:32,000
RM 模型进行更新的策略

185
00:07:32,000 --> 00:07:33,000
x 呢就是 Prompt

186
00:07:33,000 --> 00:07:36,000
y 呢就是 SFT 预测的结果

187
00:07:36,000 --> 00:07:40,000
从而去求出整个的损失函数

188
00:07:40,000 --> 00:07:42,000
接下来看一下

189
00:07:42,000 --> 00:07:44,000
第二个阶段的一个整体的图

190
00:07:44,000 --> 00:07:47,000
阶段二呢主要是训练 Reward Model 嘛

191
00:07:47,000 --> 00:07:48,000
那 Reward Model 呢

192
00:07:48,000 --> 00:07:49,000
主要是学会了

193
00:07:49,000 --> 00:07:51,000
给刚才 D 的这一类语句呢

194
00:07:51,000 --> 00:07:52,000
进行打分

195
00:07:52,000 --> 00:07:53,000
给 ABC 这类语句呢

196
00:07:53,000 --> 00:07:54,000
打低分

197
00:07:54,000 --> 00:07:56,000
从而模仿人类的偏好

198
00:07:56,000 --> 00:07:58,000
就引入了 reinforcement human feedback 

199
00:07:58,000 --> 00:08:00,000
这种方式由人类来充当 reward

200
00:08:00,000 --> 00:08:02,000
人类来充当反馈的机制

201
00:08:02,000 --> 00:08:03,000
充当环境

202
00:08:03,000 --> 00:08:04,000
下面这个图呢

203
00:08:04,000 --> 00:08:05,000
首先输入呢

204
00:08:05,050 --> 00:08:06,000
就是 prompt 的 DataSet

205
00:08:06,000 --> 00:08:06,050
输到给一个 SFT 的网络模型

206
00:08:06,050 --> 00:08:09,000
输到给一个 SFT 的网络模型

207
00:08:09,000 --> 00:08:11,000
SFT 的网络模型呢

208
00:08:11,000 --> 00:08:13,425
就会生成几个答案

209
00:08:13,625 --> 00:08:15,000
然后呢人类就进行打分

210
00:08:15,000 --> 00:08:16,000
打完分之后呢

211
00:08:16,000 --> 00:08:18,000
就把这些相关的数据呢

212
00:08:18,000 --> 00:08:21,000
去给 Reward Model 进行学习

213
00:08:21,000 --> 00:08:23,000
整体流程呢就是这么简单

214
00:08:23,000 --> 00:08:24,000
其实并不难

215
00:08:24,000 --> 00:08:26,000
但是呢大家要注意一下

216
00:08:26,000 --> 00:08:28,000
这里面呢就已经涉及到了

217
00:08:28,000 --> 00:08:30,000
整个强化学习的概念呢

218
00:08:30,000 --> 00:08:33,000
虽然呢它只是一个简单的概念

219
00:08:33,000 --> 00:08:35,000
但是呢接下来将会引入了

220
00:08:35,000 --> 00:08:37,000
更大的一个强化学习的内容

221
00:08:37,000 --> 00:08:38,000
就是 PPO 算法

222
00:08:38,000 --> 00:08:41,000
那现在马上就进入到了

223
00:08:41,000 --> 00:08:43,000
阶段三使用 PPO 算法呢

224
00:08:43,000 --> 00:08:45,950
去优化策略模型 policy

225
00:08:45,950 --> 00:08:46,000
Gradient policy Gradient model

226
00:08:46,000 --> 00:08:48,875
Gradient policy Gradient model

227
00:08:49,000 --> 00:08:51,000
可以看到在文中呢

228
00:08:51,000 --> 00:08:52,000
这条公式呢

229
00:08:52,000 --> 00:08:54,000
就是文中出现为数不多

230
00:08:54,000 --> 00:08:56,000
两套公式的其中第一条

231
00:08:56,000 --> 00:08:57,975
这个就是训练的目标函数

232
00:08:57,975 --> 00:08:58,000
E 就是 expectation

233
00:08:58,000 --> 00:08:58,975
E 就是 expectation

234
00:08:59,000 --> 00:09:01,000
一个目标

235
00:09:01,000 --> 00:09:02,000
可以看到呢

236
00:09:02,000 --> 00:09:04,000
Rθ就是刚才讲的

237
00:09:04,000 --> 00:09:07,000
它是一个对应的 RM 的模型

238
00:09:07,000 --> 00:09:08,000
而后面的这个呢

239
00:09:08,000 --> 00:09:10,000
就是求解 KL 散度

240
00:09:10,000 --> 00:09:12,000
KL 散度的简单的一个公式

241
00:09:12,000 --> 00:09:13,000
去对比 RL 模型

242
00:09:13,000 --> 00:09:19,000
还有 SFT 两个模型之间的一个差异

243
00:09:19,000 --> 00:09:20,000
这两个模型呢

244
00:09:20,000 --> 00:09:24,000
其实都是 GPT-3 的一个大规模的模型

245
00:09:24,000 --> 00:09:26,000
就呢就给出了一个偏正的修正项

246
00:09:26,000 --> 00:09:29,000
为了就是约束前面的一个值

247
00:09:29,000 --> 00:09:31,000
那下面呢看一看

248
00:09:31,000 --> 00:09:32,000
整体的流程

249
00:09:32,000 --> 00:09:34,000
整体呢它是一个因为强化学习的嘛

250
00:09:34,000 --> 00:09:36,000
它是一个迭代式的奖励的过程

251
00:09:36,000 --> 00:09:38,000
迭代式的通过奖励模型

252
00:09:38,000 --> 00:09:40,000
还有策略 SFT 的模型

253
00:09:40,000 --> 00:09:41,000
让奖励模型 RM 呢

254
00:09:41,000 --> 00:09:43,000
对 PPO 的模型输出的质量

255
00:09:43,000 --> 00:09:46,000
进行一个精准的刻画和修正引导

256
00:09:46,000 --> 00:09:48,000
使得输出的文本呢

257
00:09:48,000 --> 00:09:51,000
越来越符合人类的认知

258
00:09:51,000 --> 00:09:53,000
那这整个呢就是强化学习的

259
00:09:53,000 --> 00:09:54,000
学习的方式

260
00:09:54,000 --> 00:09:55,000
现在来看一下

261
00:09:55,000 --> 00:09:57,000
首先输入的是一个 Prompt 的 DataSet

262
00:09:57,000 --> 00:09:59,000
这里面呢就有两个分支

263
00:09:59,000 --> 00:10:00,000
第一个分支呢

264
00:10:00,000 --> 00:10:02,000
就去把网络模型处理化的

265
00:10:02,000 --> 00:10:04,000
就是 SFT 的网络模型

266
00:10:04,000 --> 00:10:05,000
进行一个计算

267
00:10:05,000 --> 00:10:07,000
得到一个 basic

268
00:10:07,000 --> 00:10:09,000
就是 basic line 

269
00:10:09,000 --> 00:10:10,000
然后呢接下来

270
00:10:10,000 --> 00:10:12,000
用一个强化学习

271
00:10:12,000 --> 00:10:14,000
一个 RL model policy

272
00:10:14,000 --> 00:10:16,000
然后去生成另外一个内容

273
00:10:16,000 --> 00:10:18,000
就是 PPO 模型

274
00:10:18,000 --> 00:10:21,000
接着呢去计算 KL 散度

275
00:10:21,000 --> 00:10:22,000
送完 KL 散度之后呢

276
00:10:22,000 --> 00:10:24,000
Rθ就是给 Reward Model

277
00:10:24,000 --> 00:10:25,000
然后去计算

278
00:10:25,000 --> 00:10:27,000
给完 Reward Model 计算呢

279
00:10:27,000 --> 00:10:28,000
就返回回来

280
00:10:28,000 --> 00:10:30,000
去更新θ

281
00:10:30,000 --> 00:10:32,000
这里面θ的更新公式呢

282
00:10:32,000 --> 00:10:34,000
就跟传统的深度学习的更新公式

283
00:10:34,000 --> 00:10:35,000
差不多

284
00:10:35,000 --> 00:10:37,000
去更新这里面的

285
00:10:37,050 --> 00:10:39,000
renforcement learning 里面的 policy

286
00:10:39,000 --> 00:10:39,050
就是 PPO 的网络模型

287
00:10:39,050 --> 00:10:41,000
就是 PPO 的网络模型

288
00:10:41,000 --> 00:10:43,000
通过这种方式呢

289
00:10:43,000 --> 00:10:46,000
不断的去更新 PPO 的网络模型

290
00:10:46,000 --> 00:10:47,000
而这个呢

291
00:10:47,000 --> 00:10:49,000
其实就是一开始训练好的

292
00:10:49,000 --> 00:10:50,000
SFT 的网络模型

293
00:10:50,000 --> 00:10:53,000
那最后用的就是 PPO 的模型

294
00:10:53,000 --> 00:10:55,000
通过 PPO 的模型呢

295
00:10:55,000 --> 00:10:57,000
使得他更加的智能

296
00:10:57,000 --> 00:10:59,000
更加的引入了人工的反馈

297
00:10:59,000 --> 00:11:02,000
从而完成了整体的学习

298
00:11:02,000 --> 00:11:03,000
那现在呢

299
00:11:03,000 --> 00:11:05,000
基本上已经讲完

300
00:11:05,000 --> 00:11:08,000
整个 Instruct GPT 的一个基本的原理

301
00:11:08,000 --> 00:11:10,000
下面呢我想引起一个思考

302
00:11:10,000 --> 00:11:11,000
提出一些疑问

303
00:11:11,000 --> 00:11:13,000
也希望大家能够参与进来

304
00:11:13,000 --> 00:11:14,000
首先呢

305
00:11:14,000 --> 00:11:16,000
这里面的问题分开三个

306
00:11:16,000 --> 00:11:17,000
第一个呢

307
00:11:17,000 --> 00:11:18,000
就是大模型

308
00:11:18,000 --> 00:11:20,000
其实已经通过了计算的方式

309
00:11:20,000 --> 00:11:21,000
去模拟人类的思考

310
00:11:21,000 --> 00:11:23,000
类似于 ChatGPT 这样的

311
00:11:23,000 --> 00:11:25,000
reinforcement human feedback 的技术

312
00:11:25,000 --> 00:11:27,000
能否给整个世界带来新的

313
00:11:27,000 --> 00:11:29,000
技术的产业革命呢

314
00:11:29,000 --> 00:11:31,000
大家去搜 b 站或者 youtube 上面

315
00:11:31,000 --> 00:11:33,000
ChatGPT 的东西呢

316
00:11:33,000 --> 00:11:35,000
确实已经有非常多的博主

317
00:11:35,000 --> 00:11:37,000
包括已经不是搞技术的博主呢

318
00:11:37,000 --> 00:11:39,000
都在研究这相关的技术

319
00:11:39,000 --> 00:11:40,000
那第二个呢

320
00:11:40,000 --> 00:11:42,000
就是我比较关心的

321
00:11:42,000 --> 00:11:43,000
像 ChatGPT 呢

322
00:11:43,000 --> 00:11:45,000
它使用了为这个框架

323
00:11:45,000 --> 00:11:47,000
作为细粒度的并行的计算

324
00:11:47,000 --> 00:11:48,000
还有异构的计算

325
00:11:48,000 --> 00:11:50,000
很好的去管理了分配强化学习

326
00:11:50,000 --> 00:11:52,000
深度学习的任务

327
00:11:52,000 --> 00:11:54,000
而且方便的利用强化学习

328
00:11:54,000 --> 00:11:57,000
对环境和计算任务进行控制

329
00:11:57,000 --> 00:11:58,000
那这个时候呢

330
00:11:58,000 --> 00:11:59,000
就对整个 AI 框架的

331
00:11:59,000 --> 00:12:01,000
分布式的能力的边界

332
00:12:01,000 --> 00:12:04,000
带来哪些新的挑战和新的冲击呢

333
00:12:04,000 --> 00:12:06,000
因为需要对环境

334
00:12:06,000 --> 00:12:07,000
还有对任务

335
00:12:07,000 --> 00:12:09,000
对深度学习的模型

336
00:12:09,000 --> 00:12:11,000
不断的进行交互

337
00:12:11,000 --> 00:12:12,000
那这个时候呢

338
00:12:12,000 --> 00:12:15,000
分布式的能力的边界

339
00:12:15,000 --> 00:12:17,000
到底会怎么样的进一步拓充

340
00:12:17,000 --> 00:12:18,000
这也是希望大家

341
00:12:18,000 --> 00:12:20,000
能够进一步的思考的

342
00:12:20,000 --> 00:12:21,000
也是我之前分享的

343
00:12:21,000 --> 00:12:22,000
很多相关的视频

344
00:12:22,000 --> 00:12:24,000
AI 系统里面

345
00:12:24,000 --> 00:12:26,000
需要认真去思考的

346
00:12:26,000 --> 00:12:27,000
这也是对昇腾

347
00:12:27,000 --> 00:12:29,000
带来的一个很大的挑战

348
00:12:29,000 --> 00:12:30,000
那第三个呢

349
00:12:30,000 --> 00:12:31,000
就是 ChatGPT 这种呢

350
00:12:31,000 --> 00:12:34,000
非常注重数据的交互

351
00:12:34,000 --> 00:12:36,000
对就是数据的交互

352
00:12:36,000 --> 00:12:37,000
对环境

353
00:12:37,000 --> 00:12:38,000
不断的进行交互

354
00:12:38,000 --> 00:12:39,000
得到新的数据

355
00:12:39,000 --> 00:12:40,000
这个时候呢

356
00:12:40,000 --> 00:12:41,000
纯算一体的技术

357
00:12:41,000 --> 00:12:44,000
会不会因为 ChatGPT 等应用

358
00:12:44,000 --> 00:12:46,000
出现新的专用的芯片

359
00:12:46,000 --> 00:12:48,000
和新的 AI 的架构呢

360
00:12:48,000 --> 00:12:51,000
这是我比较关心的下一个问题

361
00:12:51,000 --> 00:12:53,000
也希望大家一起去思考

362
00:12:55,000 --> 00:12:56,000
好了看到这

363
00:12:56,000 --> 00:12:57,000
希望大家一键三连

364
00:12:57,000 --> 00:12:58,000
多多支持我

365
00:12:59,000 --> 00:13:00,000
一开始呢

366
00:13:00,000 --> 00:13:01,000
讲了 BERT 这种

367
00:13:01,000 --> 00:13:02,000
双向的语料模型呢

368
00:13:02,000 --> 00:13:03,000
跟 GPT 这种

369
00:13:03,000 --> 00:13:05,000
单向的语料模型的区别

370
00:13:05,000 --> 00:13:06,000
然后呢去分别介绍

371
00:13:06,000 --> 00:13:08,000
GPT 家族整个系列

372
00:13:08,000 --> 00:13:09,000
接着呢

373
00:13:09,000 --> 00:13:10,000
去看看

374
00:13:10,000 --> 00:13:11,000
强化学习

375
00:13:11,000 --> 00:13:13,000
怎么加入人类的反馈

376
00:13:13,000 --> 00:13:14,000
然后呢

377
00:13:14,000 --> 00:13:15,000
了解了一下

378
00:13:15,000 --> 00:13:16,000
PG policy Gradient 

379
00:13:16,000 --> 00:13:17,000
还有 PPO 这个算法

380
00:13:17,000 --> 00:13:19,000
到底是怎么去运作的

381
00:13:19,000 --> 00:13:20,000
最后呢

382
00:13:20,000 --> 00:13:21,000
在第四个内容里面

383
00:13:21,000 --> 00:13:22,000
Instruct GPT

384
00:13:22,000 --> 00:13:24,000
深入的去把 GPT 系列

385
00:13:24,000 --> 00:13:26,000
还有 PPO 算法引进来

386
00:13:26,000 --> 00:13:27,000
当然了

387
00:13:27,000 --> 00:13:28,000
RLHF 的算法

388
00:13:28,000 --> 00:13:29,000
也是用来去训练

389
00:13:29,000 --> 00:13:31,000
Reward Model 的

390
00:13:31,000 --> 00:13:32,000
而 PPO 算法呢

391
00:13:32,000 --> 00:13:33,000
就嵌到在 Instruct GPT

392
00:13:33,000 --> 00:13:34,000
第三步

393
00:13:34,000 --> 00:13:35,000
这是第二步

394
00:13:35,000 --> 00:13:36,000
这是第一步

395
00:13:36,000 --> 00:13:37,000
一二三

396
00:13:37,000 --> 00:13:38,000
合起来就变成

397
00:13:38,000 --> 00:13:39,000
整个 GPT 的原理了

398
00:13:39,950 --> 00:13:42,279
好今天的内容就到这里为止

399
00:13:42,521 --> 00:13:46,071
ChatGPT 这个内容也到这里为止

