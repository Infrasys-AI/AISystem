1
00:00:00,000 --> 00:00:05,350
字幕生成：mkwei 字幕校对：qiaokai

2
00:00:05,350 --> 00:00:06,960
Hello 大家好

3
00:00:06,960 --> 00:00:08,880
我是那个月亮睡了

4
00:00:08,880 --> 00:00:09,760
我不睡

5
00:00:09,760 --> 00:00:12,800
我是秃头小宝贝的 ZOMI

6
00:00:12,800 --> 00:00:14,590
今天来到

7
00:00:14,590 --> 00:00:18,240
ChatGPT 狂飙原理普析的第二个内容

8
00:00:18,280 --> 00:00:19,000
在上一节

9
00:00:19,160 --> 00:00:22,240
其实已经讲了 GPT 整个系列

10
00:00:22,240 --> 00:00:25,240
包括之前的 123 系列里面的原理

11
00:00:25,240 --> 00:00:27,760
现在来到第二个内容

12
00:00:27,760 --> 00:00:29,960
主要是关心一下强化学习

13
00:00:29,960 --> 00:00:31,080
加入人类反馈

14
00:00:31,080 --> 00:00:32,160
就是 Reinforcement

15
00:00:32,160 --> 00:00:34,720
Human Feedback 这个模式

16
00:00:35,200 --> 00:00:38,920
最后一个内容才是 InstructGPT

17
00:00:38,920 --> 00:00:41,080
如果大家想直接去了解

18
00:00:41,080 --> 00:00:41,760
InstructGPT

19
00:00:41,760 --> 00:00:43,200
已经对强化学习

20
00:00:43,200 --> 00:00:45,120
还有 GPT 非常了解了

21
00:00:45,240 --> 00:00:47,240
你直接去跳到最后看就行了

22
00:00:47,240 --> 00:00:49,360
现在快速的回顾一下

23
00:00:49,360 --> 00:00:50,120
强化学习

24
00:00:50,120 --> 00:00:51,120
引入人类反馈

25
00:00:51,120 --> 00:00:53,240
还有 RLHF 这个模式

26
00:00:53,400 --> 00:00:55,040
现在来简单的去看一下

27
00:00:55,040 --> 00:00:56,560
强化学习到底是什么

28
00:00:56,560 --> 00:00:58,640
首先直接看下面这个图

29
00:00:58,640 --> 00:00:59,400
监督学习

30
00:00:59,400 --> 00:01:00,120
无监督学习

31
00:01:00,120 --> 00:01:00,840
强化学习

32
00:01:00,840 --> 00:01:03,640
是基金学习里面的三个范式

33
00:01:03,880 --> 00:01:05,400
现在经常用到的

34
00:01:05,400 --> 00:01:06,040
神经网络

35
00:01:06,040 --> 00:01:06,920
深度强化学习

36
00:01:07,160 --> 00:01:08,880
主要是在这个位置

37
00:01:08,880 --> 00:01:11,120
而后来出现了深度强化学习

38
00:01:11,120 --> 00:01:12,720
就把强化学习技术

39
00:01:12,720 --> 00:01:14,120
进行了一个交叉

40
00:01:14,120 --> 00:01:15,400
融合进来

41
00:01:15,520 --> 00:01:17,680
无论经常讲的 XXX 学习

42
00:01:17,680 --> 00:01:18,560
自监督学习

43
00:01:18,560 --> 00:01:19,480
还有无监督学习

44
00:01:19,480 --> 00:01:20,640
各种学习方法

45
00:01:20,640 --> 00:01:23,360
基本都囊括在这三个方法论

46
00:01:23,360 --> 00:01:25,440
或者这三个范式里面

47
00:01:25,480 --> 00:01:27,280
现在再来看看

48
00:01:27,280 --> 00:01:30,360
强化学习里面有什么东西

49
00:01:30,480 --> 00:01:33,360
强化学习主要有两个交互的对象

50
00:01:33,360 --> 00:01:34,800
第一个就是 Agent

51
00:01:34,800 --> 00:01:36,160
智能体

52
00:01:36,160 --> 00:01:37,560
第二个就是 Environment

53
00:01:37,560 --> 00:01:39,200
环境

54
00:01:39,400 --> 00:01:41,040
首先会从智能体

55
00:01:41,040 --> 00:01:41,640
开始出发

56
00:01:41,640 --> 00:01:43,120
智能体执行一个动作

57
00:01:43,120 --> 00:01:44,720
在环境当中执行一个动作

58
00:01:44,720 --> 00:01:47,320
那环境就会反馈一个新的状态

59
00:01:47,320 --> 00:01:48,920
和新的一个 Reward

60
00:01:48,920 --> 00:01:50,840
就是反馈奖励

61
00:01:50,840 --> 00:01:52,160
给 Agent

62
00:01:52,160 --> 00:01:53,200
根据当前的状态

63
00:01:53,200 --> 00:01:53,800
新的状态

64
00:01:53,800 --> 00:01:54,920
还有新的一个奖励

65
00:01:55,200 --> 00:01:56,800
再去选择新的动作

66
00:01:57,000 --> 00:01:58,600
实际上虽然看上去

67
00:01:58,600 --> 00:02:00,120
这么简单的一个交互

68
00:02:00,120 --> 00:02:01,120
怎么去求解

69
00:02:01,120 --> 00:02:02,840
怎么变成数学范式

70
00:02:03,360 --> 00:02:06,120
这个问题就非常的有意思了

71
00:02:07,720 --> 00:02:09,160
在强化学习里面

72
00:02:09,440 --> 00:02:12,320
主要是把刚才的一个交互的方式

73
00:02:12,320 --> 00:02:14,960
变成了一个马尔可夫链

74
00:02:15,000 --> 00:02:16,240
通过马尔可夫链

75
00:02:16,440 --> 00:02:18,000
就对强化学这个范式

76
00:02:18,200 --> 00:02:19,960
有了一个数学的表示

77
00:02:20,000 --> 00:02:21,760
最后就是怎么去求解

78
00:02:21,760 --> 00:02:23,800
马尔可夫链的最优质

79
00:02:24,000 --> 00:02:25,160
这个就完完全全

80
00:02:25,160 --> 00:02:27,000
变成数学等价的公式

81
00:02:27,000 --> 00:02:29,760
就可以求强化学习了

82
00:02:30,040 --> 00:02:32,040
下面继续往下看一下

83
00:02:32,040 --> 00:02:35,600
强化学习有哪些基本的概念

84
00:02:35,760 --> 00:02:37,480
其实有一些没有相关的概念

85
00:02:37,600 --> 00:02:38,880
我已经把提出了

86
00:02:38,880 --> 00:02:39,880
这些基本概念

87
00:02:40,040 --> 00:02:41,080
实际上到后面

88
00:02:41,240 --> 00:02:42,960
还是跟 ChartGPT

89
00:02:42,960 --> 00:02:45,120
还有 InstructGPT 相关的

90
00:02:45,120 --> 00:02:47,640
首先要了解一下策略

91
00:02:48,240 --> 00:02:49,280
策略主要是指

92
00:02:49,280 --> 00:02:50,760
智能体在特定的时间内

93
00:02:50,760 --> 00:02:52,800
选择的行为的方式

94
00:02:52,920 --> 00:02:53,880
简单的理解

95
00:02:54,000 --> 00:02:55,320
就是智能体

96
00:02:55,360 --> 00:02:56,760
它在某个时间段

97
00:02:56,760 --> 00:02:58,000
根据什么策略

98
00:02:58,000 --> 00:03:00,080
去执行它的动作

99
00:03:00,320 --> 00:03:01,280
奖励函数

100
00:03:01,440 --> 00:03:03,280
就是我执行完这个策略之后

101
00:03:03,640 --> 00:03:05,680
环境就会给我一个反馈

102
00:03:05,680 --> 00:03:06,200
告诉我

103
00:03:06,200 --> 00:03:07,160
我这一步做得好

104
00:03:07,160 --> 00:03:07,640
还是不好

105
00:03:07,640 --> 00:03:08,440
例如下棋

106
00:03:08,440 --> 00:03:10,560
我下这个位置到底好还是不好

107
00:03:10,560 --> 00:03:12,600
它会有一个奖励函数

108
00:03:12,960 --> 00:03:14,200
可能我下的比较好

109
00:03:14,200 --> 00:03:16,040
它就会给我做一个正的奖励

110
00:03:16,040 --> 00:03:17,200
如果我下的不好

111
00:03:17,200 --> 00:03:18,240
或者我要输棋了

112
00:03:18,240 --> 00:03:19,840
它就给我一个负的奖励

113
00:03:19,840 --> 00:03:20,360
这个时候

114
00:03:20,480 --> 00:03:22,160
就可以通过求解马尔可夫链

115
00:03:22,160 --> 00:03:24,080
或者求解马尔可夫规则

116
00:03:24,320 --> 00:03:25,840
通过求解马尔可夫过程

117
00:03:25,840 --> 00:03:27,000
来得到最优的解

118
00:03:27,000 --> 00:03:27,960
或者最后的解

119
00:03:28,160 --> 00:03:30,600
下面还有一个价值函数

120
00:03:30,720 --> 00:03:31,480
这个价值函数

121
00:03:31,560 --> 00:03:32,560
也是在后面的

122
00:03:32,560 --> 00:03:34,840
ChatGPT 里面会用到的

123
00:03:35,040 --> 00:03:36,320
从长远的角度看

124
00:03:36,600 --> 00:03:38,480
刚才只是单单一步

125
00:03:38,480 --> 00:03:39,560
好还是不好

126
00:03:39,560 --> 00:03:41,680
价值函数是从长远角度看

127
00:03:41,680 --> 00:03:43,080
这个整体的状态

128
00:03:43,080 --> 00:03:44,240
或整体的策略

129
00:03:44,240 --> 00:03:46,080
到底有没有收益

130
00:03:46,280 --> 00:03:48,240
最后就是环境模型

131
00:03:48,240 --> 00:03:48,760
环境模型

132
00:03:48,760 --> 00:03:49,200
就是

133
00:03:49,200 --> 00:03:50,400
Bandit Environment

134
00:03:50,400 --> 00:03:51,760
就是数据集

135
00:03:51,800 --> 00:03:53,960
就后面的 POMD 数据集

136
00:03:54,240 --> 00:03:56,560
接下来再往下看一看

137
00:03:56,560 --> 00:03:58,360
小新同学来问一问

138
00:03:59,160 --> 00:04:00,480
ZOMI 老师你好

139
00:04:00,480 --> 00:04:02,080
在 Native 强化学习里面

140
00:04:02,520 --> 00:04:04,840
有 Environment 和 Reward Model

141
00:04:04,840 --> 00:04:05,800
那 Reward Model

142
00:04:05,800 --> 00:04:07,200
就是奖励函数

143
00:04:07,880 --> 00:04:08,200
但是

144
00:04:08,200 --> 00:04:09,280
内强化学习里面

145
00:04:09,280 --> 00:04:10,360
没有奖励函数

146
00:04:10,760 --> 00:04:12,160
只有一些专家

147
00:04:12,160 --> 00:04:14,680
或者人类的一个示范和反馈

148
00:04:14,960 --> 00:04:16,560
怎么纳入到

149
00:04:16,560 --> 00:04:18,160
强化学习这个体系里面

150
00:04:18,160 --> 00:04:19,480
或者马尔可夫链里面

151
00:04:20,480 --> 00:04:22,080
小新问的这个问题

152
00:04:22,080 --> 00:04:23,760
或者我类似的这个问题

153
00:04:23,760 --> 00:04:25,320
确实问的挺好的

154
00:04:26,640 --> 00:04:28,920
首先通过人类的标注

155
00:04:28,920 --> 00:04:30,520
去获得这个 Reward Model

156
00:04:30,520 --> 00:04:32,160
就相当于我有了人类的标注

157
00:04:32,160 --> 00:04:33,040
然后我相信

158
00:04:33,040 --> 00:04:35,480
这个人或者专家标出来的数据

159
00:04:35,800 --> 00:04:36,640
是不错的

160
00:04:36,640 --> 00:04:37,440
比较好的

161
00:04:37,440 --> 00:04:39,240
然后反推人类

162
00:04:39,240 --> 00:04:40,280
就是 Human

163
00:04:40,280 --> 00:04:43,080
为什么去设置这样的奖励函数

164
00:04:43,080 --> 00:04:44,480
就我去学习

165
00:04:44,920 --> 00:04:46,240
反向的过程

166
00:04:46,440 --> 00:04:47,800
既然学习完之后

167
00:04:47,880 --> 00:04:49,520
就有了奖励函数了

168
00:04:49,520 --> 00:04:52,520
就可以使用一般的强化学习的方法

169
00:04:52,520 --> 00:04:53,920
去找到最优的策略

170
00:04:53,920 --> 00:04:55,800
或者最优的动作

171
00:04:55,800 --> 00:04:58,840
下面看看下面这个图

172
00:04:59,000 --> 00:04:59,600
下面这个图

173
00:04:59,600 --> 00:05:01,360
简单的再放大一下

174
00:05:01,360 --> 00:05:03,240
它主要有三个模块

175
00:05:03,240 --> 00:05:04,560
首先有 Agent

176
00:05:04,560 --> 00:05:06,360
那 Agent 就是刚才讲到的

177
00:05:06,360 --> 00:05:07,400
一个智能体

178
00:05:07,400 --> 00:05:08,400
那智能体实际上

179
00:05:08,400 --> 00:05:10,360
它是一个强化学习的算法

180
00:05:10,360 --> 00:05:11,960
你直接把它当成强化学习

181
00:05:11,960 --> 00:05:13,080
算法解好了

182
00:05:13,080 --> 00:05:15,440
然后还有一个 Environment

183
00:05:15,440 --> 00:05:17,240
也就是环境

184
00:05:17,240 --> 00:05:19,440
那实际上环境在 ChartGPT 里面

185
00:05:19,480 --> 00:05:20,720
或者 Instruct-GPT 里面

186
00:05:21,000 --> 00:05:23,000
把它当成一个 data set

187
00:05:23,000 --> 00:05:25,440
只是在里面不断的进行采样

188
00:05:25,440 --> 00:05:26,760
获得数据

189
00:05:26,800 --> 00:05:28,760
然后有第三个内容

190
00:05:28,760 --> 00:05:30,360
就是 Reward Predict

191
00:05:30,360 --> 00:05:31,960
人类的反馈

192
00:05:31,960 --> 00:05:33,200
通过人类的反馈

193
00:05:33,320 --> 00:05:35,320
去获得一个预测的奖励

194
00:05:35,320 --> 00:05:37,720
然后去给算法进行学习

195
00:05:37,760 --> 00:05:38,520
学习完之后

196
00:05:38,640 --> 00:05:39,200
我的 Agent

197
00:05:39,200 --> 00:05:40,600
就可以在环境当中

198
00:05:40,600 --> 00:05:42,600
不断的跟环境进行交互

199
00:05:42,640 --> 00:05:43,800
执行一个动作

200
00:05:43,800 --> 00:05:46,320
然后得到当前的状态和观察值

201
00:05:46,320 --> 00:05:47,920
之前讲到的 Environment

202
00:05:47,920 --> 00:05:49,080
返回的一个 Reward

203
00:05:49,360 --> 00:05:50,920
就可以通过 Reward Predict

204
00:05:51,200 --> 00:05:53,760
返回给 RL Algorithm

205
00:05:53,760 --> 00:05:54,880
Agent

206
00:05:54,880 --> 00:05:55,760
通过这种方式

207
00:05:55,840 --> 00:05:57,360
就把人类反馈

208
00:05:57,360 --> 00:06:00,720
加入到强化学习的整个体系里面了

209
00:06:00,720 --> 00:06:03,440
所以说并没有多难

210
00:06:03,440 --> 00:06:05,400
难的难在后面

211
00:06:05,400 --> 00:06:07,200
强化学习的算法

212
00:06:07,200 --> 00:06:08,960
策略梯度 Policy

213
00:06:08,960 --> 00:06:12,000
Gradient 到 PPO 算法

214
00:06:12,800 --> 00:06:16,280
下面在正式进入到算法之前

215
00:06:16,480 --> 00:06:20,400
我还是先讲讲一些简单的概念

216
00:06:20,520 --> 00:06:21,400
强化学习

217
00:06:21,600 --> 00:06:23,600
按照方法学习策略来分

218
00:06:23,800 --> 00:06:26,760
主要分为 Value Base 和 Policy Base

219
00:06:26,760 --> 00:06:28,120
在 ChartGPT 里面

220
00:06:28,240 --> 00:06:29,640
用的是 Policy Base

221
00:06:29,640 --> 00:06:31,200
而两者之间的交集

222
00:06:31,400 --> 00:06:33,800
有一个 Actor 跟 Critic

223
00:06:33,960 --> 00:06:36,360
ChartGPT 更加准确的来说

224
00:06:36,600 --> 00:06:39,240
用的就是 action 跟 Critic

225
00:06:39,280 --> 00:06:42,080
这种方式或者这种求解的方法

226
00:06:42,080 --> 00:06:43,200
我有一个演员

227
00:06:43,200 --> 00:06:44,400
有一个评判家

228
00:06:44,400 --> 00:06:46,240
演员去跳一段舞蹈

229
00:06:46,240 --> 00:06:47,760
评判家去看一下

230
00:06:47,760 --> 00:06:49,160
他跳的好不好

231
00:06:49,160 --> 00:06:51,560
通过这种互相博弈的方式

232
00:06:51,560 --> 00:06:53,040
进行一个学习

233
00:06:54,360 --> 00:06:57,440
下面看一下两种学习方式

234
00:06:57,440 --> 00:06:58,480
一种是 Value Base

235
00:06:58,480 --> 00:07:00,400
一种是 Policy Base

236
00:07:00,400 --> 00:07:01,640
它到底有什么不同

237
00:07:01,800 --> 00:07:03,600
一种是学习一个值

238
00:07:03,640 --> 00:07:06,080
一种是学习一种策略

239
00:07:06,240 --> 00:07:08,320
在数学上其实表示很简单

240
00:07:08,360 --> 00:07:09,680
假设我现在的 Agent

241
00:07:09,800 --> 00:07:11,560
有三个动作

242
00:07:11,560 --> 00:07:13,720
a1 a2 a3 三个动作

243
00:07:13,720 --> 00:07:14,400
那三个动作

244
00:07:14,520 --> 00:07:16,640
我选取 q 就是这个值

245
00:07:16,640 --> 00:07:17,440
三个值

246
00:07:17,440 --> 00:07:19,840
最大的作为本次选择的动作

247
00:07:19,840 --> 00:07:20,960
例如玩游戏的时候

248
00:07:20,960 --> 00:07:22,280
告诉我上下左右

249
00:07:22,280 --> 00:07:24,240
我按上左还是按下键

250
00:07:24,440 --> 00:07:26,240
这个就是 Value Base

251
00:07:26,440 --> 00:07:28,840
下面就是 Policy Base

252
00:07:28,960 --> 00:07:29,600
Policy Base

253
00:07:29,720 --> 00:07:31,800
就会有一个动作的函数 Actor

254
00:07:31,800 --> 00:07:32,640
这个动作函数

255
00:07:32,760 --> 00:07:35,440
会得到一个策略的概率

256
00:07:35,440 --> 00:07:36,600
就当前这个动作

257
00:07:36,600 --> 00:07:37,840
执行某个策略

258
00:07:37,840 --> 00:07:38,920
得到的一个概率

259
00:07:38,920 --> 00:07:40,720
叫 psa

260
00:07:40,760 --> 00:07:42,080
最后根据这个策略

261
00:07:42,280 --> 00:07:44,640
选取对应的动作

262
00:07:46,680 --> 00:07:47,800
说的简单一点

263
00:07:48,280 --> 00:07:50,280
我的智能体决定我上下左右

264
00:07:50,400 --> 00:07:52,200
我直接用 Value Base 就行了

265
00:07:52,240 --> 00:07:54,160
但是在我下棋的时候

266
00:07:54,280 --> 00:07:55,600
如果有机器人辅助

267
00:07:55,600 --> 00:07:56,800
或有作弊机器人的时候

268
00:07:56,800 --> 00:07:57,800
我不喜欢智能体

269
00:07:57,960 --> 00:07:59,240
告诉我是下这一步

270
00:07:59,240 --> 00:08:00,440
还是下那一步

271
00:08:00,440 --> 00:08:02,040
我更希望智能体告诉我

272
00:08:02,040 --> 00:08:04,440
我下这一步赢的概率有多大

273
00:08:04,440 --> 00:08:05,520
我下这一步

274
00:08:05,520 --> 00:08:07,320
会得到什么不一样的结果

275
00:08:07,520 --> 00:08:10,160
这种就是一个 Policy Base 的方式

276
00:08:10,160 --> 00:08:11,360
能够更好的处理

277
00:08:11,360 --> 00:08:12,200
连续的动作

278
00:08:12,200 --> 00:08:13,400
给我一个预测的概率

279
00:08:13,400 --> 00:08:14,240
让我自主

280
00:08:14,240 --> 00:08:15,640
或者让 Agent 再去决定

281
00:08:15,640 --> 00:08:17,080
我应该执行怎么动作

282
00:08:17,080 --> 00:08:18,880
我应该怎么进行探索

283
00:08:18,880 --> 00:08:20,880
这也是 Policy Base 跟 Value Base

284
00:08:20,880 --> 00:08:22,480
最大的区别

285
00:08:23,400 --> 00:08:24,200
下一个话题

286
00:08:24,640 --> 00:08:26,080
谷歌之前很火的

287
00:08:26,080 --> 00:08:28,160
用强化显微镜做下棋

288
00:08:28,160 --> 00:08:30,760
我的 alphaGo 就是用了这种方式

289
00:08:30,960 --> 00:08:33,520
下面真实的来到了

290
00:08:33,960 --> 00:08:35,800
PG Policy Gradient

291
00:08:35,800 --> 00:08:37,240
策略梯度下降

292
00:08:37,240 --> 00:08:39,320
这一个算法里面

293
00:08:40,560 --> 00:08:41,720
首先看一下

294
00:08:41,720 --> 00:08:43,000
下面这一段话

295
00:08:43,000 --> 00:08:45,120
在实际的环境或者实验当中

296
00:08:45,200 --> 00:08:47,840
会让 Actor 跟 Environment 进行交互

297
00:08:47,840 --> 00:08:49,680
产生一系列的数据

298
00:08:49,680 --> 00:08:52,160
就是我 given 一个具体的 Policy

299
00:08:52,160 --> 00:08:54,960
然后产生一系列的 simple 的数据

300
00:08:54,960 --> 00:08:57,280
那 x 代表的是当前的状态

301
00:08:57,280 --> 00:08:59,320
a 是代表执行这个 Policy

302
00:08:59,320 --> 00:09:01,160
得到的动作

303
00:09:01,160 --> 00:09:04,080
而奥鲁套就是当前的一个奖励

304
00:09:04,080 --> 00:09:05,240
通过跟环境的交互

305
00:09:05,400 --> 00:09:07,040
就得到很多 state action 的 Policy

306
00:09:07,040 --> 00:09:08,360
就 state action 对

307
00:09:08,360 --> 00:09:09,680
表示在某个状态下

308
00:09:09,680 --> 00:09:10,640
采取了某个动作

309
00:09:10,640 --> 00:09:12,800
得到什么一个奖励

310
00:09:13,040 --> 00:09:15,080
现在会将这些数据

311
00:09:15,080 --> 00:09:17,040
送入到训练的过程当中

312
00:09:17,040 --> 00:09:18,200
进行计算

313
00:09:18,200 --> 00:09:20,240
更新网络模型

314
00:09:20,240 --> 00:09:22,160
而 sita 就是深度学习里面

315
00:09:22,160 --> 00:09:24,680
某个模型里面的所有的参数

316
00:09:24,680 --> 00:09:25,800
那参数的更新方式

317
00:09:25,960 --> 00:09:26,840
就是这一条

318
00:09:26,840 --> 00:09:28,720
而参数的更新的驱动方式

319
00:09:28,880 --> 00:09:30,320
就是算 RT

320
00:09:30,320 --> 00:09:32,040
下面清空所有的笔记验证

321
00:09:32,160 --> 00:09:34,440
看一下下面这条交互的方式

322
00:09:34,600 --> 00:09:36,840
那首先会有一个特定的策略

323
00:09:36,840 --> 00:09:38,280
在一个特定的策略里面

324
00:09:38,680 --> 00:09:41,240
Agent 和环境进行交互

325
00:09:41,240 --> 00:09:43,640
得到很多 sa 的 pairs 的对

326
00:09:43,880 --> 00:09:45,680
这些都是数据的样本

327
00:09:45,680 --> 00:09:46,840
数据的样本

328
00:09:47,000 --> 00:09:48,640
会丢给神经网络

329
00:09:48,640 --> 00:09:49,560
进行更新

330
00:09:49,560 --> 00:09:50,280
更新完之后

331
00:09:50,440 --> 00:09:51,720
就会 Update Model

332
00:09:51,720 --> 00:09:52,720
Update Model 之后

333
00:09:53,000 --> 00:09:55,240
就重新的给到新的 Policy

334
00:09:55,240 --> 00:09:56,080
去执行

335
00:09:56,080 --> 00:09:57,880
然后去判断什么时候好

336
00:09:57,880 --> 00:09:58,800
什么时候收敛

337
00:09:58,800 --> 00:10:00,760
什么时候达到目标

338
00:10:00,760 --> 00:10:01,400
预期的目标

339
00:10:01,400 --> 00:10:03,880
就可以把迭代的方式停止下来

340
00:10:03,880 --> 00:10:05,440
就学习到了一个

341
00:10:05,440 --> 00:10:07,880
很好的强化学习的策略了

342
00:10:09,440 --> 00:10:11,480
现在正式的进入到

343
00:10:11,480 --> 00:10:13,000
PPO 这个算法了

344
00:10:13,160 --> 00:10:13,880
PPO 算法

345
00:10:14,040 --> 00:10:16,000
就是 ChatGPT 里面

346
00:10:16,000 --> 00:10:17,320
使用到的一个算法

347
00:10:17,320 --> 00:10:19,280
也是 OpenAI 去提出来的

348
00:10:19,280 --> 00:10:20,680
在 17 年的时候

349
00:10:20,920 --> 00:10:22,200
对于 PG 算法来说

350
00:10:22,360 --> 00:10:23,400
其实最大的问题

351
00:10:23,400 --> 00:10:25,320
就是在策略更新之后

352
00:10:25,320 --> 00:10:27,680
还需要使用相同的环境互动

353
00:10:27,680 --> 00:10:28,840
去收集数据

354
00:10:28,840 --> 00:10:30,360
进行下轮的迭代

355
00:10:30,360 --> 00:10:32,280
也就是刚才说到的

356
00:10:32,280 --> 00:10:33,440
我收集完数据

357
00:10:33,440 --> 00:10:34,520
迭代更新

358
00:10:34,520 --> 00:10:35,200
收集数据

359
00:10:35,200 --> 00:10:36,280
迭代更新

360
00:10:36,280 --> 00:10:37,720
这种方式

361
00:10:38,280 --> 00:10:39,280
于是 PPO 算法

362
00:10:39,280 --> 00:10:41,800
就利用了重要性采样这种思想

363
00:10:41,840 --> 00:10:43,680
当智能体步入到当前策略的

364
00:10:43,680 --> 00:10:44,920
一个路径概率的情况下

365
00:10:45,160 --> 00:10:48,440
模拟一个近似的 Q 的分布

366
00:10:48,760 --> 00:10:49,880
这里很有意思

367
00:10:49,880 --> 00:10:51,000
模拟 Q 的分布

368
00:10:51,000 --> 00:10:52,760
只要 P 跟 Q 同分布

369
00:10:52,760 --> 00:10:53,760
差得不太远

370
00:10:53,760 --> 00:10:54,840
通过多轮迭代

371
00:10:55,000 --> 00:10:56,520
就可以快速的收敛

372
00:10:56,520 --> 00:10:58,240
使得智能体策略

373
00:10:58,240 --> 00:11:00,120
学习的更快

374
00:11:00,160 --> 00:11:01,040
更爽

375
00:11:01,280 --> 00:11:02,800
现在正式的看看

376
00:11:02,800 --> 00:11:04,800
PPO 算法里面的一些公式

377
00:11:04,800 --> 00:11:06,000
首先需要讲讲

378
00:11:06,440 --> 00:11:07,680
PPO 是结合了一个

379
00:11:07,680 --> 00:11:09,480
Actor 跟 Critic 的方式

380
00:11:09,480 --> 00:11:10,840
那 Actor 是一个演员

381
00:11:10,840 --> 00:11:11,360
Critic

382
00:11:11,360 --> 00:11:13,120
那就是一个评判家

383
00:11:13,400 --> 00:11:15,040
这里面的一个 Actor 跟 Critics

384
00:11:15,040 --> 00:11:17,880
都是对应不同的网络模型

385
00:11:17,880 --> 00:11:19,320
这是个 A 模型

386
00:11:19,320 --> 00:11:20,840
这个是 C 模型

387
00:11:20,880 --> 00:11:22,200
A 模型跟 C 模型

388
00:11:22,200 --> 00:11:24,800
是在环境当中不断的博弈的

389
00:11:24,800 --> 00:11:27,200
Actor 首先负责跟环境互动

390
00:11:27,200 --> 00:11:27,840
收集样本

391
00:11:27,840 --> 00:11:30,160
等同于刚才的一个 PG

392
00:11:30,160 --> 00:11:32,000
然后用来更新 PPO 的

393
00:11:32,000 --> 00:11:32,600
另外的话

394
00:11:32,600 --> 00:11:33,800
添加了一个 Critics

395
00:11:33,920 --> 00:11:34,760
用于评判

396
00:11:34,760 --> 00:11:35,840
我的 Actor 的动作

397
00:11:35,840 --> 00:11:37,440
到底好还是不好

398
00:11:37,560 --> 00:11:38,600
在实际算法的时候

399
00:11:38,760 --> 00:11:39,320
我的 Actor

400
00:11:39,440 --> 00:11:40,240
就会有一个旧的

401
00:11:40,240 --> 00:11:41,200
还有一个新的

402
00:11:41,200 --> 00:11:42,760
两个通过 KL 散度

403
00:11:42,760 --> 00:11:44,080
进行一个互相的学习

404
00:11:44,080 --> 00:11:46,160
也就是之前讲到的

405
00:11:46,160 --> 00:11:47,360
只要有两个同分布

406
00:11:47,360 --> 00:11:48,080
差得不太远

407
00:11:48,080 --> 00:11:49,440
就可以尽快去模拟

408
00:11:49,440 --> 00:11:50,800
其中的一个策略

409
00:11:51,080 --> 00:11:54,040
而 AT 就是 advantage estimate

410
00:11:54,800 --> 00:11:56,880
对应强化学习概念里面的

411
00:11:56,880 --> 00:11:58,240
价值函数

412
00:11:58,240 --> 00:11:59,920
通过根据不同环境

413
00:11:59,920 --> 00:12:01,240
就是整体的

414
00:12:01,240 --> 00:12:02,240
整一个环境里面

415
00:12:02,240 --> 00:12:03,280
获取的一个 RT

416
00:12:03,280 --> 00:12:04,120
就是奖励

417
00:12:04,120 --> 00:12:06,440
然后去得到最终的

418
00:12:06,440 --> 00:12:07,800
或者预期的奖励

419
00:12:07,800 --> 00:12:09,000
通过计算预期的奖励

420
00:12:09,360 --> 00:12:10,960
就得到最终的

421
00:12:10,960 --> 00:12:12,760
一个整体的期望

422
00:12:12,960 --> 00:12:15,920
现在再看一下具体的算法

423
00:12:15,920 --> 00:12:16,760
现在看一下

424
00:12:16,760 --> 00:12:17,600
Actor 跟 Critics

425
00:12:17,600 --> 00:12:19,200
整个算法是怎么实现的

426
00:12:19,200 --> 00:12:20,920
首先有两轮迭代

427
00:12:21,120 --> 00:12:21,800
第一轮迭代

428
00:12:21,960 --> 00:12:23,840
是在环境当中的一个交互

429
00:12:23,840 --> 00:12:25,680
第二个就是我的 Actor

430
00:12:25,680 --> 00:12:26,880
之间的一个互相学习

431
00:12:26,880 --> 00:12:29,160
或者在里面去运行的

432
00:12:29,200 --> 00:12:29,840
proxy owner

433
00:12:29,840 --> 00:12:30,840
会在环境当中

434
00:12:30,840 --> 00:12:32,480
去不断的执行

435
00:12:32,480 --> 00:12:34,440
然后计算 advantage

436
00:12:34,440 --> 00:12:35,320
estimate

437
00:12:35,320 --> 00:12:36,520
对这个没读错

438
00:12:36,520 --> 00:12:38,520
就是说对应价值的函数

439
00:12:38,520 --> 00:12:40,360
然后迭代完之后就停止了

440
00:12:40,360 --> 00:12:41,600
然后去更新

441
00:12:41,600 --> 00:12:44,240
网络模型的策略

442
00:12:44,440 --> 00:12:45,360
最后更新完之后

443
00:12:45,480 --> 00:12:47,600
就把新的学习到的策略

444
00:12:47,600 --> 00:12:49,560
给到旧的网络模型当中

445
00:12:49,560 --> 00:12:51,640
然后重新的去跟环境

446
00:12:51,640 --> 00:12:52,720
进行交互迭代

447
00:12:52,720 --> 00:12:53,640
通过这种方式

448
00:12:53,840 --> 00:12:55,640
去不断的在环境当中

449
00:12:55,640 --> 00:12:56,800
学习到一个

450
00:12:56,800 --> 00:12:58,720
很好的算法的模式

451
00:12:58,800 --> 00:12:59,640
那好了

452
00:12:59,640 --> 00:13:00,600
今天的内容

453
00:13:00,880 --> 00:13:02,800
就基本上到这里为止了

454
00:13:03,040 --> 00:13:04,800
今天讲了强化学的基本概念

455
00:13:04,800 --> 00:13:05,760
还有 Policy Gradient

456
00:13:05,760 --> 00:13:07,520
还有 PPO 这个算法

457
00:13:07,760 --> 00:13:08,400
听不懂的

458
00:13:08,400 --> 00:13:09,600
其实没有太多关系

459
00:13:09,600 --> 00:13:10,480
你对这些概念

460
00:13:10,600 --> 00:13:12,160
有个大致的了解

461
00:13:12,160 --> 00:13:14,200
或者有些名词上的认知就好了

462
00:13:14,960 --> 00:13:16,480
将会在下一节当中

463
00:13:16,480 --> 00:13:17,360
instruct gdp

464
00:13:17,360 --> 00:13:17,960
ChartGPT

465
00:13:17,960 --> 00:13:18,920
这个原理里面

466
00:13:19,240 --> 00:13:23,000
重新的去把刚才前面讲到的算法

467
00:13:23,000 --> 00:13:24,480
都融合进来

468
00:13:24,480 --> 00:13:27,280
然后再跟大家讲一遍

469
00:13:27,800 --> 00:13:29,200
这里面就是最重要

470
00:13:29,200 --> 00:13:30,400
最核心的内容了

471
00:13:30,400 --> 00:13:31,920
欢迎大家继续留意
