# 数据并行

数据并行就是把原本的单一设备和数据的训练方式扩展为多设备和数据，通过多个设备并行进行计算，再将计算后的结果同步，以达到理想情况下设备数量倍加速的目的。数据并行分为**数据并行**和**分布式数据并行**。和数据并行相比，尽管分布式数据并行更复杂，但是我们在选择数据并行算法时，还是应该优先选择分布式数据并行。简单来说：它更高效，易于优化，且可以很好的与其他并行算法相结合。

**设备**：这里一般情况下代指 NVIDIA GPU。NVIDIA GPU 属于 SPMD（Single Programer Multiple Data），即所有处理单元执行同一段代码。**一般由一个主机线程或进程来控制一块GPU设备**。在Pytorch中，由于Python的GIL限制，一般以多进程为主。

**数据并行**：数据并行是单进程、多线程的，且只能在一台机器上运行。

数据并行算法可以拆分为三个步骤：

- 前向传播：在前向传播过程中，首先将mini-batch数据均分到每个设备中，如果mini-batch设置的太小，将会导致设备内的并行性很差，使训练变慢，在通讯的额外开销下，甚至有可能当会出现速度低于单设备的情况。接下来是分布式初始化，模型和优化器在某一块设备上进行随机初始化，然后将模型，优化器拷贝到每个设备上，模型、优化器完全相同，甚至是使用相同的随机种子。在初始化完成之后，各个设备上根据自己拿到的数据和模型同时进行前向传播。

- 损失计算与反向传播：在前向传播完成后，每块设备各种计算模型的损失，并分别进行反向传播。在得到梯度后，梯度传递到某一块设备上进行累加，并将累加到的梯度同步到每个设备上进行参数更新。这样，就保证了每一块设备上的模型都是一致的，如果模型不一致，使用其他模型的梯度进行参数更新将会带来收敛性的问题。

<img src="C:\Users\aojia\Desktop\分布式并行\DP.png" alt="d" style="zoom:50%;" />

但由于用户通常使用Python作为深度学习的开发语言，并使用基于动态图的框架（如：Pytorch），数据并行通常受到Python的GIL（Global Interpreter Lock）限制，导致设备利用率和训练速度低下。另外一方面，数据并行在累计梯度时将全局的梯度在单一设备上进行累积，使得算法的并行度降低，同时也增加了该设备的负担。

**分布式数据并行**：分布式数据并行的流程与数据并行类似，但引入了多进程，这规避了Python多线程的GIL限制，同时做了很多针对通信的优化。每个进程启动一个单独的主训练脚本副本，也可以扩展到多台网络连接的机器，进一步扩展了分布式的规模和效率。针对通信的优化主要是使用**分桶Ring-AllReduce算法**和**图优化算法**，另外一些与数据并行的区别是：

- 每个设备将会获取到一份完整的mini-batch数据，而不是mini-batch数据的拆分。
- 每个设备的负载是均衡的，不会出现单一设备负载更高的情况。

**分桶Ring-AllReduce算法（bucketed Ring-AllReduce）**：分桶Ring-AllReduce算法是Pytorch分布式数据并行默认采用的分布式通讯算法。他将模型的梯度分组，分别放入不同的桶中，在一个桶中的梯度计算完成后即可进行通讯。这减小了每次进行通讯的计算量，同时也让计算与通讯的重叠更简单（计算下一组参数的梯度时，若本组的梯度都已经计算完毕，可以同时进行本组梯度的通讯，减少了计算完成后对通讯的等待时间）

<img src="C:\Users\aojia\Desktop\分布式并行\Communicate.png" style="zoom:50%;" />

同时，分桶Ring-AllReduce算法使用Ring-AllReduce进行通讯，它每次只发送一部分梯度，分步将梯度传播到每个设备上，通讯的过程就像一个环形一样。使用Ring-AllReduce有助于降低通讯设备的压力。

<img src="C:\Users\aojia\Desktop\分布式并行\Ring-AllReduce-Spread.png" style="zoom:50%;" />	<img src="C:\Users\aojia\Desktop\分布式并行\Ring-AllReduce.png" style="zoom:50%;" />

**计算与通讯的重叠**：一般来说，在分布式训练中每个进程会在完成当前网络反向传播的同时进行梯度更新，以达到隐藏通讯延迟的目的。在一部分梯度计算完成后，就立刻进行通讯，这一般是通过一个钩子函数实现的。在通讯的同时，也会进行梯度的计算，这样的好处是不用在所以计算完成后再进行集合通讯，可以将通讯的过程覆盖到计算的时间内，充分利用设备；也不用在计算完成后等待通讯，提高了设备的使用率。

**同步和异步的梯度平均与参数平均**：在之前的介绍中，都是基于同步梯度平均进行分布式训练，数据并行性通过在优化器步骤之前通信梯度来实现分布式训练，以确保使用完全相同的梯度集更新所有模型副本的参数，因此模型副本可以在迭代中保持一致。这样有很多优点，但是也有一些缺点。同步的梯度平均使得模型一致性和收敛性得到保证，但是不同设备之间需要等待其他设备完成计算，这造成了部分计算资源的浪费。在异步的梯度平均或参数平均中，训练拥有更强的异步性，模型不需要保持同步的一致，因此具有更少的设备等待时间，更高的设备利用率，但是也会为收敛带来一定的问题。

**补充材料**：

1.Facebook AI 对自己的开源框架 Pytorch 数据并行训练的技术报告 *PyTorch Distributed: Experiences on Accelerating Data Parallel Training* 中详细讨论了数据并行的相关方案与框架设计。