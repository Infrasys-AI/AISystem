1
00:00:00,000 --> 00:00:07,775
嗨 大家好 我是 ZOMI

2
00:00:07,775 --> 00:00:10,300
今天来到分布式训练系列里面的

3
00:00:10,300 --> 00:00:11,520
大模型算法结构

4
00:00:11,520 --> 00:00:14,150
那聊到大模型的算法结构

5
00:00:14,150 --> 00:00:15,850
主要是去看看

6
00:00:15,850 --> 00:00:18,400
大模型算法的一个整体的发展

7
00:00:18,400 --> 00:00:20,975
从没有到有，从小到大

8
00:00:20,975 --> 00:00:23,575
然后呢，我想到那个耶来

9
00:00:23,575 --> 00:00:26,375
接着来看看大模型算法的 

10
00:00:26,375 --> 00:00:29,200
最重要的或者最著名的两个结构

11
00:00:29,200 --> 00:00:31,275
第一个呢，就是现在基本上

12
00:00:31,275 --> 00:00:34,240
经常会用到的 Transformer 这个结构

13
00:00:34,240 --> 00:00:35,375
那第二个呢

14
00:00:35,375 --> 00:00:37,700
去看看能够让网络模型 

15
00:00:37,700 --> 00:00:39,520
上万亿规模的 MOE 结构

16
00:00:40,160 --> 00:00:42,625
可以看到网络模型的规模越大

17
00:00:42,625 --> 00:00:44,000
一台机器是放不下

18
00:00:44,000 --> 00:00:45,825
所以面向大模型呢

19
00:00:45,825 --> 00:00:48,240
要进行大规模分布式训练

20
00:00:48,240 --> 00:00:52,080
但是呢，今天主要是聊聊大模型的一些结构

21
00:00:52,160 --> 00:00:54,300
而在后面的分享里面呢,

22
00:00:54,300 --> 00:00:57,775
才去看看怎么把这些模型并行的切分到 

23
00:00:57,775 --> 00:01:00,720
不同的机器上面去做一个训练的加速

24
00:01:00,720 --> 00:01:03,275
从 2011 年到 2022 年

25
00:01:03,275 --> 00:01:06,720
模型的参数量其实是不断的增加的

26
00:01:06,720 --> 00:01:09,400
而在 2016 年到 2017 年的时候

27
00:01:09,400 --> 00:01:11,875
Transformer 的出现呢

28
00:01:11,875 --> 00:01:13,920
就形成了大模型红色的这条线

29
00:01:13,920 --> 00:01:17,520
相当于这大模型里面已经有了一个断层

30
00:01:17,520 --> 00:01:19,760
那大模型对来说意味着什么

31
00:01:19,920 --> 00:01:23,440
大模型又能帮助解决哪些问题呢

32
00:01:23,440 --> 00:01:25,440
看看左边的三个

33
00:01:25,440 --> 00:01:28,480
首先呢，大模型需要大量的数据

34
00:01:28,480 --> 00:01:30,250
既然数据量非常大

35
00:01:30,250 --> 00:01:32,800
就不可能每个数据都进行标注

36
00:01:32,800 --> 00:01:36,225
所以呢，大模型就有了一个很重要的工作

37
00:01:36,225 --> 00:01:39,920
就是引入了自监督学习或者无监督学习的方法

38
00:01:39,920 --> 00:01:41,850
第二个优点就是大模型

39
00:01:41,850 --> 00:01:44,160
可以看到为什么要叫大

40
00:01:44,160 --> 00:01:46,675
因为模型的参数量非常多

41
00:01:46,675 --> 00:01:47,680
参数量多

42
00:01:47,840 --> 00:01:51,840
网络模型的精度就有了进一步的突破

43
00:01:51,840 --> 00:01:56,400
第三个就是大模型可以解决很多下游任务的问题

44
00:01:56,400 --> 00:01:59,750
在 Bert 和 Transformer 出现之后

45
00:01:59,750 --> 00:02:03,525
大模型就提供了预训练或者 Zero-Shot 的这种方式

46
00:02:03,525 --> 00:02:05,120
解决了模型碎片化

47
00:02:05,120 --> 00:02:07,400
我解决十几个 NLP 的任务

48
00:02:07,400 --> 00:02:10,960
可能只需要用一个大模型就可以解决了

49
00:02:10,960 --> 00:02:13,000
不需要开发十个大模型

50
00:02:13,000 --> 00:02:15,120
每个大模型对应一个任务

51
00:02:16,080 --> 00:02:17,550
下面看看

52
00:02:17,550 --> 00:02:19,225
在分布式训练里面

53
00:02:19,225 --> 00:02:21,520
要解决训练耗时的问题

54
00:02:21,520 --> 00:02:25,600
可能会跟训练的规模还有单步的计算量相关 

55
00:02:25,600 --> 00:02:29,200
单步的计算量又跟网络模型相关

56
00:02:29,200 --> 00:02:32,240
这几天我看到一些非常派言听闻的标题

57
00:02:32,240 --> 00:02:36,880
更具将于网络模型和设计和领域相关的网络模型

58
00:02:36,880 --> 00:02:38,425
2022 年 10 月 24 号 

59
00:02:38,425 --> 00:02:40,775
2022 年 10 月 24 号的这些标题

60
00:02:40,775 --> 00:02:42,240
我给大家去念一念

61
00:02:42,720 --> 00:02:47,875
第一个就是谷歌 Flan-T5 诞生了 1800 种语言

62
00:02:47,875 --> 00:02:50,880
 超大规模微调 1800 种语言

63
00:02:50,880 --> 00:02:54,150
第二个就是多语言图像描述

64
00:02:54,150 --> 00:02:58,000
最强评估基准,XM3600 来了

65
00:02:58,000 --> 00:03:01,680
涵盖 36 种语言,36 种语言

66
00:03:02,160 --> 00:03:07,440
第三个就是小扎亲自演示首个闽南语翻译系统

67
00:03:07,440 --> 00:03:10,425
主攻 3000 多种无文字语言

68
00:03:10,425 --> 00:03:12,800
3000 多种无文字语言

69
00:03:12,800 --> 00:03:15,700
现在的大模型都已经这么牛逼了吗

70
00:03:15,700 --> 00:03:18,400
一下子就解决了这么多下游任务

71
00:03:21,360 --> 00:03:23,175
于是在这一节分享里面

72
00:03:23,175 --> 00:03:27,120
我想给大家去聊一聊大模型的整个结构的演进

73
00:03:27,120 --> 00:03:28,800
是相关结构的演进

74
00:03:28,800 --> 00:03:32,750
从一开始的 Transformer 取代了 RNN

75
00:03:32,750 --> 00:03:35,200
让迈进了整个大模型的时代

76
00:03:35,200 --> 00:03:38,275
其实有了 Transformer 之后

77
00:03:38,275 --> 00:03:40,850
模型的规模参数可能还停留在

78
00:03:40,850 --> 00:03:42,880
一个亿或者千万的级别上面

79
00:03:42,880 --> 00:03:45,125
在 2017 年的时候

80
00:03:45,125 --> 00:03:48,400
谷歌针对 Hitton 的 MOE 模型

81
00:03:48,400 --> 00:03:51,200
又提出了稀疏混合专家的结构

82
00:03:51,200 --> 00:03:54,400
让模型量可以进一步的突破了百亿

83
00:03:54,560 --> 00:03:56,975
那像 Bert 网络模型就是

84
00:03:56,975 --> 00:04:00,480
首个稠密的突破十亿规模的 NLP 大模型

85
00:04:00,480 --> 00:04:03,975
所以它对大模型的贡献是非常大的

86
00:04:03,975 --> 00:04:06,250
直到 GPT-3 的出现

87
00:04:06,250 --> 00:04:09,440
一下子刷新了人们对大模型的认知

88
00:04:09,440 --> 00:04:12,650
原来大模型还可以去到千亿规模

89
00:04:12,650 --> 00:04:15,520
千亿规模的参数量是非常大的,

90
00:04:15,520 --> 00:04:18,825
可能一个 GDP-3 的网络模型的权重

91
00:04:18,825 --> 00:04:20,960
就已经快接近一个 G 了

92
00:04:21,360 --> 00:04:22,650
在 2021 年的时候

93
00:04:22,650 --> 00:04:26,000
谷歌又发明了 Switch Transformer,

94
00:04:26,000 --> 00:04:30,640
这个 Switch Transformer 已经突破了首个万亿的大模型 

95
00:04:30,640 --> 00:04:34,175
刚才聊到了 GTP-3 还是千亿

96
00:04:34,175 --> 00:04:37,440
千亿之后又来到了万亿的大模型

97
00:04:37,440 --> 00:04:39,325
这个还是很牛逼的

98
00:04:39,325 --> 00:04:41,360
后来又有了 GLaM

99
00:04:42,240 --> 00:04:45,600
在保持相同万亿规模的参数量的时候,

100
00:04:45,600 --> 00:04:48,640
谷歌推出了 GLaM 网络模型

101
00:04:49,040 --> 00:04:52,800
让大规模语言模型进一步提升它的精度

102
00:04:52,800 --> 00:04:56,480
下面逐个来展开一下大模型的结构

103
00:04:56,480 --> 00:05:00,640
大模型的参数量是怎么一步一步往上走的

104
00:05:05,920 --> 00:05:07,550
Transformer 这篇文章

105
00:05:07,550 --> 00:05:10,400
原文叫做 Attention is all you need

106
00:05:10,400 --> 00:05:13,760
就是你只需要注意力机制就行了

107
00:05:13,760 --> 00:05:18,560
这篇文章更多的是一个示例和公式讲的没那么详细

108
00:05:18,560 --> 00:05:22,480
后面我希望用一个简单的图去给大家讲示的

109
00:05:22,480 --> 00:05:25,600
现在来粗略的浏览一下这篇文章

110
00:05:25,600 --> 00:05:26,600
首先文章里面这个

111
00:05:26,600 --> 00:05:29,760
就代表 Transformer 的网络结构

112
00:05:29,760 --> 00:05:32,880
网络模型从一开始的 Input Embedding

113
00:05:32,880 --> 00:05:34,960
然后再加一个 Position Encoding

114
00:05:34,960 --> 00:05:37,040
然后输给网络模型

115
00:05:37,040 --> 00:05:40,400
输进去的时候第一个会遇到 Multi-head Attent

116
00:05:40,400 --> 00:05:40,800
ion,

117
00:05:40,800 --> 00:05:44,480
就是多头的注意力，接着有一个 Normalization

118
00:05:44,480 --> 00:05:46,240
然后再做一个 FeedForward

119
00:05:46,240 --> 00:05:49,120
然后就传给 Encorder 层

120
00:05:49,120 --> 00:05:51,440
可以理解为左边的是 Encorder

121
00:05:51,440 --> 00:05:54,000
右边的是 Decoder 的这种模式

122
00:05:54,000 --> 00:05:58,720
3.2 节开始就去讲讲 Attention 的机制

123
00:05:58,720 --> 00:06:02,880
这里面就通过 QKV 去实现 Attention 的机制

124
00:06:02,880 --> 00:06:06,560
3.3 就去讲讲 FeedForward 到底是个什么东西

125
00:06:06,560 --> 00:06:11,440
最后可能 3.4,3.5 都是去拼接整个网络模型的

126
00:06:11,520 --> 00:06:14,875
第五节的内容就去讲讲 Transformer 的

127
00:06:14,875 --> 00:06:16,720
这个结构具体是怎么去训练,

128
00:06:16,720 --> 00:06:21,280
到了第六节的内容就开始真正的实验的环节部分

129
00:06:21,280 --> 00:06:24,400
针对不同的层，不同的结构，同的入参

130
00:06:24,400 --> 00:06:28,080
作者都做了大量的应用实践的对比

131
00:06:28,080 --> 00:06:30,640
因为文章是在 2017 年

132
00:06:30,640 --> 00:06:34,720
那个时候 NLP 下游用户不是说非常的丰富,

133
00:06:34,720 --> 00:06:37,000
所以作者就用了两个简单的评价指标

134
00:06:37,000 --> 00:06:40,880
去评估 Transformer 的网络模型到底好还是不好

135
00:06:40,960 --> 00:06:43,150
现在回到 slide 里面

136
00:06:43,150 --> 00:06:45,360
刚才那个图我已经简单的讲了一讲

137
00:06:45,360 --> 00:06:46,475
现在把它横的去看

138
00:06:46,475 --> 00:06:48,900
Transformer 的结构其实最重要的

139
00:06:48,900 --> 00:06:50,960
 就是 Attention 机制

140
00:06:50,960 --> 00:06:52,750
就是这个 Attention 机制

141
00:06:52,750 --> 00:06:54,640
所有东西都离不开 Attention 

142
00:06:54,640 --> 00:06:57,375
Feed Forward 这个更像于 FF 层

143
00:06:57,375 --> 00:07:00,320
简单的称它为前馈神经网络就可以了

144
00:07:00,320 --> 00:07:03,360
像这种线性的 Softmax 都是原有的

145
00:07:03,360 --> 00:07:05,760
而最重要的就是 Attention

146
00:07:06,240 --> 00:07:08,950
实际上 Transformer 这个网络模型

147
00:07:08,950 --> 00:07:11,830
刚才只是其中一个 Encoder 

148
00:07:11,830 --> 00:07:13,440
或者 Decoder 一个具体的展开形态

149
00:07:13,440 --> 00:07:16,750
那它的网络模型里面可能会有 7 层 Encoder

150
00:07:16,750 --> 00:07:18,880
 然后再加 7 层 Decoder

151
00:07:18,880 --> 00:07:21,075
最后输入可能是中文

152
00:07:21,075 --> 00:07:23,280
输出可能是英文的翻译

153
00:07:23,280 --> 00:07:26,880
这么一种方式去组成 Transformer 的结构

154
00:07:26,880 --> 00:07:29,040
在 17 年提出 Transformer 的时候

155
00:07:29,040 --> 00:07:32,550
它的目的是取代 RNN 和 LSTM

156
00:07:32,550 --> 00:07:36,960
去解决梯度爆炸、梯度消失和长序列的问题

157
00:07:36,960 --> 00:07:39,920
那个时候并没有出现预训练模型

158
00:07:39,920 --> 00:07:42,325
所以网络模型的输入和输出

159
00:07:42,325 --> 00:07:46,320
训练的数据仍然还是使用自监督学习的方式

160
00:07:46,320 --> 00:07:48,025
这就是我的输入和输出

161
00:07:48,025 --> 00:07:50,800
都是人工的进行校准对比标注过的

162
00:07:50,800 --> 00:07:53,075
回到论文的图里面

163
00:07:53,075 --> 00:07:55,025
左边的个叫做 Encoder,

164
00:07:55,025 --> 00:07:55,920
也就是编码的

165
00:07:55,920 --> 00:07:57,625
右边叫做 Decoder

166
00:07:57,625 --> 00:07:58,400
反编码的

167
00:07:58,400 --> 00:08:00,700
左边的这个假设输入的是中文

168
00:08:00,700 --> 00:08:04,640
 把它经过层层的 Encoder 之后得到一个向量

169
00:08:04,640 --> 00:08:07,840
那这个向量又经过层层的 Decoder 之后

170
00:08:07,840 --> 00:08:09,550
反解析成为英文

171
00:08:09,550 --> 00:08:11,275
学习中间的参数

172
00:08:11,275 --> 00:08:13,400
使得输入是中文

173
00:08:13,400 --> 00:08:14,800
输出可以是英文

174
00:08:14,800 --> 00:08:18,640
Attention 模块最重要的就是 QKV 三个矩阵

175
00:08:18,640 --> 00:08:21,840
假设现在的任务是查询向量 Q

176
00:08:21,840 --> 00:08:23,550
就是这个 Q

177
00:08:23,550 --> 00:08:26,960
然后去计算 Q 跟各个 K 之间的相似度

178
00:08:26,960 --> 00:08:28,190
就是我会计算

179
00:08:28,190 --> 00:08:33,120
Q1 跟 K1、K2、K3、K4 之间的相似度

180
00:08:33,120 --> 00:08:35,050
而每个相似度都有一个值

181
00:08:35,050 --> 00:08:37,680
叫做 V，就是 value

182
00:08:37,680 --> 00:08:39,750
所以这里面有 QKV

183
00:08:39,750 --> 00:08:41,800
然后 Q2 会查询

184
00:08:41,800 --> 00:08:45,840
它跟 K1、K2、K3、K4 之间的相似度

185
00:08:46,240 --> 00:08:49,475
得到每个 Key 对应 value 的权重系数

186
00:08:49,475 --> 00:08:52,250
然后对 value 进行加权求和

187
00:08:52,250 --> 00:08:54,880
得到最终 Attention 的值

188
00:08:54,880 --> 00:08:56,775
所以就会有 QKV

189
00:08:56,775 --> 00:08:58,640
三个矩阵三个权重向量

190
00:08:58,640 --> 00:09:01,680
那下面来看看具体的一个形式

191
00:09:01,680 --> 00:09:06,080
首先去计算 Q 跟 K 的一个相似度

192
00:09:06,805 --> 00:09:10,150
然后去计算 Q1 跟 K2 的一个相似度 A12

193
00:09:10,475 --> 00:09:12,960
然后再计算成 A13、A14

194
00:09:13,600 --> 00:09:16,880
最后对所有数据做一个 softmax 的处理

195
00:09:16,880 --> 00:09:18,400
也就是在这一层里面

196
00:09:18,400 --> 00:09:20,400
进行一个 softmax 的计算

197
00:09:20,400 --> 00:09:22,825
通过反向梯度传播去学习

198
00:09:22,825 --> 00:09:25,920
QKV 之间的一个映射的关系

199
00:09:25,920 --> 00:09:29,600
那刚才聊的只是一个 Attention 的机制

200
00:09:29,600 --> 00:09:33,400
实际上 Transformer 里面叫做 MultiHeadAttention

201
00:09:33,400 --> 00:09:34,560
就是多头注意力机制

202
00:09:34,560 --> 00:09:37,600
假设现在的多头有两个 Head

203
00:09:37,600 --> 00:09:39,275
这个 Head 的数量是

204
00:09:39,275 --> 00:09:42,640
通过前面输进去网络模型之前的进行配置的

205
00:09:42,640 --> 00:09:44,150
假设我两个 Head

206
00:09:44,150 --> 00:09:45,840
我可以把这两个做一个并列 

207
00:09:45,840 --> 00:09:48,480
实际上在系统里面为了加速运算

208
00:09:48,480 --> 00:09:51,400
可能我会把第一个 Q 分成两个 Q

209
00:09:51,400 --> 00:09:53,200
把 K 分成两个 K

210
00:09:53,200 --> 00:09:55,475
然后计算的公式和计算的逻辑

211
00:09:55,475 --> 00:09:57,680
都跟刚才所描述的一样

212
00:09:57,680 --> 00:10:00,560
那就得到了两个 Head 的计算方式

213
00:10:00,560 --> 00:10:04,125
通过这种方式可以更好的对 Transformer

214
00:10:04,125 --> 00:10:06,560
这个算子进行并行的操作和并行的计算,

215
00:10:06,560 --> 00:10:09,200
使得执行的时候跑得更快

216
00:10:09,520 --> 00:10:11,375
Transformer 的出现其实是为了解决

217
00:10:11,375 --> 00:10:13,280
Sequence to Sequence 的问题

218
00:10:13,280 --> 00:10:15,200
然后用 Attention

219
00:10:15,200 --> 00:10:18,400
就是注意力的结构去代替 LSTM

220
00:10:18,400 --> 00:10:20,375
那使用这种网络模型的结构

221
00:10:20,375 --> 00:10:22,960
具体对带来有什么好处和结果呢

222
00:10:22,960 --> 00:10:24,775
可以看到实际上刚才看到的

223
00:10:24,775 --> 00:10:27,360
只是一层 Transformer 的结构,

224
00:10:27,360 --> 00:10:29,520
每一层还有很多小算子

225
00:10:29,520 --> 00:10:32,325
于是 Transformer 的结构就使得

226
00:10:32,325 --> 00:10:34,640
每一层的计算复杂度就变得更优

227
00:10:35,040 --> 00:10:39,120
第二个就是不需要像 LSTM 一样有很多的 gate

228
00:10:39,120 --> 00:10:41,760
有输入门、移往门、输出门,

229
00:10:41,760 --> 00:10:44,560
直接用点乘的结果去进行计算

230
00:10:44,560 --> 00:10:46,050
就是 QKV

231
00:10:46,050 --> 00:10:48,240
可以通过矩阵层去进行计算

232
00:10:48,240 --> 00:10:51,840
那第三个就是模型更具有解释性

233
00:10:51,840 --> 00:10:53,425
因为在 LSTM 里面

234
00:10:53,425 --> 00:10:55,680
对长序列进行处理的时候

235
00:10:55,680 --> 00:10:57,920
对网络模型的解析是很难的

236
00:10:57,920 --> 00:11:00,575
网络模型在序列的传播当中

237
00:11:00,575 --> 00:11:02,240
丢失了非常多的信息

238
00:11:02,480 --> 00:11:06,075
第四个就是解决了序列很长的时候所引发

239
00:11:06,075 --> 00:11:07,520
的一系列的问题,

240
00:11:07,520 --> 00:11:11,600
但是带来的就是网络模型急剧的膨胀

241
00:11:11,600 --> 00:11:13,600
当时候在 17 年的时候

242
00:11:13,600 --> 00:11:17,520
觉得网络模型的数量膨胀是个很严重的问题

243
00:11:17,520 --> 00:11:19,600
就像出现了 ResNet50 之后呢

244
00:11:19,600 --> 00:11:22,240
人们希望出现一种 Mobinet

245
00:11:22,240 --> 00:11:25,920
就像出现了 ResNet50、ResNet101 这种网络模型

246
00:11:25,920 --> 00:11:27,840
这种精度已经很好的网络模型

247
00:11:27,840 --> 00:11:29,120
但是人们还不够

248
00:11:29,120 --> 00:11:32,160
希望网络模型的参数量越小越好 

249
00:11:32,160 --> 00:11:34,720
于是谷歌就研究了 Mobinet 一样

250
00:11:34,720 --> 00:11:37,280
当时候觉得网络模型的参数量大

251
00:11:37,280 --> 00:11:38,320
不是个好事

252
00:11:38,320 --> 00:11:40,240
大的话我怎么做推理啊

253
00:11:40,240 --> 00:11:42,525
在大模型真正出现之前

254
00:11:42,525 --> 00:11:44,960
其实有两个奠基的工作的

255
00:11:44,960 --> 00:11:47,280
第一个就是刚才聊到的 Transomer

256
00:11:47,280 --> 00:11:52,240
第二个就是 2017 年同年稍微晚一点的 MOE

257
00:11:52,240 --> 00:11:54,960
同年稍微晚一点的 MOE

258
00:11:54,960 --> 00:11:59,440
实际上大模型很多工作都是谷歌去发起的

259
00:11:59,440 --> 00:12:02,080
虽然现在 TensorFlow 很少人去用了

260
00:12:02,240 --> 00:12:04,640
但是大模型很多相关的工作

261
00:12:04,640 --> 00:12:06,160
谷歌都落在 TensorFlow 了

262
00:12:06,160 --> 00:12:08,150
所以很多时候可能可以去看看

263
00:12:08,150 --> 00:12:10,800
TensorFlow 这个框架是怎么设计的,

264
00:12:10,800 --> 00:12:13,680
并行的一些系统或者一些文章概念

265
00:12:13,680 --> 00:12:17,120
还是非常有帮助于去理解分布式并行的

266
00:12:17,120 --> 00:12:19,225
那 MOE 这个网络模型结构

267
00:12:19,225 --> 00:12:23,840
其实是基于 1990 年 Hitton 提出的 Mixture of Exports

268
00:12:23,840 --> 00:12:27,120
前面就加了一个定语叫做稀疏的门控

269
00:12:27,120 --> 00:12:31,600
这篇文章实际上描述的一个稀疏门控的混合专家模型

270
00:12:31,600 --> 00:12:34,160
但是大部分都会叫做 MOE

271
00:12:34,160 --> 00:12:37,280
就把稀疏 gate 这个定语先把它去掉

272
00:12:37,280 --> 00:12:40,800
这篇文章一开始去介绍一些以前人的工作

273
00:12:40,800 --> 00:12:44,025
然后再去看看 MOE 的网络模型的结构

274
00:12:44,025 --> 00:12:45,360
具体长什么样子的,

275
00:12:45,360 --> 00:12:47,600
可以看到里面有非常多的 exprot

276
00:12:47,600 --> 00:12:49,840
然后这里面有一个 Gate 门控

277
00:12:49,840 --> 00:12:51,900
去控制在什么应用场景

278
00:12:51,900 --> 00:12:54,640
什么场合，什么权重的情况下

279
00:12:54,640 --> 00:12:57,040
去激活对应的专家

280
00:12:57,040 --> 00:13:00,480
在 2.1 里面就具体的描述了我的输出

281
00:13:00,960 --> 00:13:02,960
为了保证稀疏性和均衡性

282
00:13:02,960 --> 00:13:05,680
这篇文章就对 Softmax 进行了一些改造

283
00:13:05,680 --> 00:13:08,560
首先第一个改造就是 NoiseTopKGating

284
00:13:08,560 --> 00:13:11,139
Noise 这个工作就是 StanardNoramlize 

285
00:13:11,139 --> 00:13:13,920
和 WNoise 加入了一个权重的噪声

286
00:13:13,920 --> 00:13:16,960
使得网络模型训练的时候更加均衡

287
00:13:16,960 --> 00:13:19,680
而不是某个 export 的权重特别大

288
00:13:19,680 --> 00:13:21,920
某个专家真的是专家

289
00:13:21,920 --> 00:13:24,480
现在很多时候在网上经常去吐槽

290
00:13:24,480 --> 00:13:27,120
专家呀专家，求你不要再建议了

291
00:13:27,120 --> 00:13:28,480
就是这个概念

292
00:13:28,560 --> 00:13:31,520
要让专家跟专家之间更加均衡

293
00:13:31,520 --> 00:13:35,040
而不是某个专家一直在发表一些错误的言论

294
00:13:39,360 --> 00:13:40,880
实际上专家非常多

295
00:13:40,880 --> 00:13:44,080
不可能听所有专家的建议

296
00:13:44,080 --> 00:13:46,800
第二个内容就是提出了 Top-K Gating

297
00:13:46,800 --> 00:13:49,440
就是这里面的函数 KeepTopK

298
00:13:49,440 --> 00:13:52,880
保证网络模型学习一定量专家的建议

299
00:13:52,880 --> 00:13:55,280
其他专家的建议在某种情况下

300
00:13:55,280 --> 00:13:56,960
可以把它自为负无穷

301
00:13:57,040 --> 00:13:58,880
让他不要再去学习了

302
00:13:58,880 --> 00:14:02,480
在网上就是一些相关的要注意的事项

303
00:14:02,480 --> 00:14:03,600
特别是分布式并行和

304
00:14:03,600 --> 00:14:05,360
BatchSize 的设置

305
00:14:05,360 --> 00:14:08,400
第四点就是回到刚才讲到的

306
00:14:08,400 --> 00:14:10,880
去平衡各个专家之间的一个作用

307
00:14:10,880 --> 00:14:13,680
到了第五节的内容就是具体的实验

308
00:14:13,680 --> 00:14:16,960
可以看到 MOE 里面做了非常大量的实验

309
00:14:16,960 --> 00:14:19,680
比 Transformer 网络模型的实验会更多

310
00:14:19,680 --> 00:14:22,160
而且对比了以前非常多的不同的

311
00:14:22,160 --> 00:14:24,880
关于吸收性或者专家性的相关的工作

312
00:14:25,200 --> 00:14:28,000
这里面比较有意思的就是 MOE 网络模型

313
00:14:28,000 --> 00:14:29,600
虽然看上去很简单

314
00:14:29,600 --> 00:14:32,080
但是这篇文章里面又附带了很多

315
00:14:32,080 --> 00:14:34,320
附录去介绍具体的计算公式

316
00:14:34,320 --> 00:14:36,160
具体的计算逻辑

317
00:14:36,160 --> 00:14:38,880
并且附上了大量的消融实验

318
00:14:38,880 --> 00:14:40,400
回到 PPT 里面

319
00:14:40,400 --> 00:14:43,760
实际上 MOE 它叫做稀疏门控专家混合模型

320
00:14:43,760 --> 00:14:44,800
就是刚才讲的

321
00:14:44,800 --> 00:14:47,600
去控制哪个专家在某种情况下 

322
00:14:47,600 --> 00:14:48,960
应该怎么去计算

323
00:14:48,960 --> 00:14:51,360
为了保证稀疏性和均衡性

324
00:14:51,360 --> 00:14:53,920
对 Softmax 进行了一些处理

325
00:14:53,920 --> 00:14:54,675
加入了 TopK

326
00:14:54,675 --> 00:14:55,760
 加入了 Noised

327
00:14:55,760 --> 00:14:59,600
可以看到原来的网络模型是通过一个 Softmax 的

328
00:14:59,600 --> 00:15:02,400
但是实际上是通过 Gdelta

329
00:15:02,400 --> 00:15:05,200
就 Gdelta 实际上就是下面这条公式,

330
00:15:05,200 --> 00:15:07,280
把 KeytopK 加进去了

331
00:15:07,280 --> 00:15:09,520
而且还加入了 WNoised

332
00:15:09,520 --> 00:15:12,240
MOE 这个 E 算叫做专家

333
00:15:12,240 --> 00:15:14,560
实际上可以把它理解为把大模型

334
00:15:14,560 --> 00:15:16,400
拆分为多个小模型,

335
00:15:16,400 --> 00:15:18,960
每个小模型就是一个专家

336
00:15:18,960 --> 00:15:21,200
对数位一个样本的数量来说

337
00:15:21,280 --> 00:15:24,320
并不需要所有的专家都去计算

338
00:15:24,320 --> 00:15:27,040
而是激活一部分的专家就可以了

339
00:15:27,040 --> 00:15:29,760
这样就可以节省大量的计算资源

340
00:15:29,760 --> 00:15:31,920
而且有多个专家

341
00:15:31,920 --> 00:15:34,960
不同的专家可以处理不同的下游任务

342
00:15:34,960 --> 00:15:36,880
实现网络模型的增长

343
00:15:36,880 --> 00:15:39,520
可以看到在 MOE 的网络模型实验里面

344
00:15:39,520 --> 00:15:42,320
特别的去强调了网络模型的增长

345
00:15:42,320 --> 00:15:45,360
网络模型的参数量不断的增加的情况下

346
00:15:45,360 --> 00:15:48,400
实际上 GPU 的计算量并不会很大

347
00:15:48,400 --> 00:15:51,120
因为是通过小模型专家去进行计算的

348
00:15:51,120 --> 00:15:53,600
而不是所有的专家都进行计算

349
00:15:53,600 --> 00:15:56,400
这里面就是 Keep Top-K 所带来的好处

