1
00:00:04,680 --> 00:00:06,400
Hello 大家好

2
00:00:06,400 --> 00:00:08,760
又回来了

3
00:00:08,760 --> 00:00:09,880
然后我是 ZOMI

4
00:00:09,880 --> 00:00:13,200
今天来聊一聊分布式算法这个系列

5
00:00:13,200 --> 00:00:14,200
那现在呢

6
00:00:14,200 --> 00:00:18,280
已经来到了分布式算法里面的最后一个小内容

7
00:00:18,280 --> 00:00:19,920
分布式的大模型算法

8
00:00:19,920 --> 00:00:22,200
不会在这里面讲太多

9
00:00:22,200 --> 00:00:26,440
而是重点讲讲几个比较有影响力的 SOTA 大模型

10
00:00:26,440 --> 00:00:27,840
那首先第一个就是 BERT

11
00:00:27,840 --> 00:00:29,560
引起预训练的

12
00:00:29,560 --> 00:00:32,160
第二个就是 GTP3

13
00:00:32,160 --> 00:00:33,720
作为一个 FewShot Learning

14
00:00:33,720 --> 00:00:36,080
非常之有名的网络模型

15
00:00:36,080 --> 00:00:39,520
模型参数量也是上了千亿规模的

16
00:00:39,520 --> 00:00:42,040
最后来一个上万亿规模参数量的

17
00:00:42,040 --> 00:00:44,920
用了 MOE 结构的 Switch Transformer

18
00:00:45,880 --> 00:00:46,880
Hi 大家好

19
00:00:46,880 --> 00:00:47,840
我是 ZOMI

20
00:00:47,840 --> 00:00:50,825
又来到了大模型和分布式训练

21
00:00:50,825 --> 00:00:53,040
这个系列里面的分享内容

22
00:00:53,040 --> 00:00:55,500
今天我想给大家一起去分享一下

23
00:00:55,500 --> 00:00:57,080
大模型的一个算法结构

24
00:00:57,120 --> 00:01:00,280
去看看模型的发展的情况

25
00:01:00,280 --> 00:01:01,920
从最熟悉的模型

26
00:01:01,920 --> 00:01:05,360
其实一开始并没有提模型的参数量有多大

27
00:01:05,360 --> 00:01:08,040
一般来说模型的进步有多好而已

28
00:01:08,040 --> 00:01:11,080
后来等引入了大模型之后

29
00:01:11,080 --> 00:01:15,040
就开始的去提模型到底是百亿规模的

30
00:01:15,040 --> 00:01:16,280
还是千亿规模的

31
00:01:16,280 --> 00:01:17,960
还是万亿规模的

32
00:01:17,960 --> 00:01:20,400
可以看到网络模型的规模越大

33
00:01:20,400 --> 00:01:21,960
一台机器是放不下

34
00:01:21,960 --> 00:01:23,360
所以面向大模型

35
00:01:23,360 --> 00:01:26,160
要进行大规模分布式训练

36
00:01:26,200 --> 00:01:29,960
但是今天主要是聊聊大模型的一些结构

37
00:01:29,960 --> 00:01:31,440
而在后面的分享里面

38
00:01:31,440 --> 00:01:34,240
才去看看怎么把这些模型

39
00:01:34,240 --> 00:01:36,840
并行的切分到不同的机器上面

40
00:01:36,840 --> 00:01:38,800
去做一个训练的加速

41
00:01:38,800 --> 00:01:42,520
Transformer 和 MOE 实际上是大模型的一个奠基

42
00:01:42,520 --> 00:01:43,800
而真正的大模型

43
00:01:43,800 --> 00:01:45,960
从 BERT 网络模型开始

44
00:01:45,960 --> 00:01:50,080
然后首个突破十亿规模的 NLP 大模型

45
00:01:50,080 --> 00:01:52,680
它采用的是一个双向编码的方式

46
00:01:52,680 --> 00:01:55,920
来改进整个网络模型的架构

47
00:01:55,920 --> 00:01:57,325
蓝色的这个框实际上

48
00:01:57,325 --> 00:01:59,280
是 Transformer 的一个示例

49
00:01:59,280 --> 00:02:01,760
里面可以看到有非常多的连线

50
00:02:01,760 --> 00:02:03,880
每一条连线都向左向右

51
00:02:03,880 --> 00:02:06,080
这里面就是 BERT 提出的一个概念

52
00:02:06,080 --> 00:02:07,560
双向编码

53
00:02:07,560 --> 00:02:09,920
这里面 Transformer 带来一个很重要的概念

54
00:02:09,920 --> 00:02:12,400
就是预训练的网络模型

55
00:02:12,400 --> 00:02:16,400
然后进行一些简单的效用任务的微调就可以了

56
00:02:16,400 --> 00:02:19,520
现在来看看 BERT 网络模型这篇论文

57
00:02:19,520 --> 00:02:22,120
经常会在一些论文或者博客里面

58
00:02:22,120 --> 00:02:23,920
看到这么一个小玩具

59
00:02:23,920 --> 00:02:25,775
这个呆呆的样子实际上是

60
00:02:25,775 --> 00:02:27,320
芝麻街兄弟里面的一员

61
00:02:27,320 --> 00:02:29,040
他的名字叫做 BERT

62
00:02:29,040 --> 00:02:30,520
看看论文

63
00:02:30,520 --> 00:02:32,840
在论文里面的 BERT 它其实是

64
00:02:32,840 --> 00:02:36,760
Deep Bidirectional Transformers for Language Understanding

65
00:02:36,760 --> 00:02:40,360
而 BERT 取决于双向 Encorder

66
00:02:40,360 --> 00:02:42,120
Representation Transformer

67
00:02:42,120 --> 00:02:45,000
这几个英文单词的首字母大写

68
00:02:45,000 --> 00:02:47,640
这篇论文同样简单的来翻一翻

69
00:02:47,640 --> 00:02:49,760
首先是 Introduction

70
00:02:49,760 --> 00:02:53,360
接着去理解一下什么叫做预训练网络模型

71
00:02:53,360 --> 00:02:54,920
或者 Fine Tuning

72
00:02:54,920 --> 00:02:57,160
接着去讲讲一些相关的工作

73
00:02:57,160 --> 00:02:59,440
第一个就是无监督学习

74
00:02:59,440 --> 00:03:02,480
无监督学习也是 BERT 里面很重要的一个概念

75
00:03:02,480 --> 00:03:03,920
第二个就是 Fine Tuning

76
00:03:03,920 --> 00:03:05,800
就是微调任务

77
00:03:05,800 --> 00:03:07,160
可以看到 Pretraining

78
00:03:07,160 --> 00:03:08,560
就是预训练里面

79
00:03:08,560 --> 00:03:11,080
用的是一个无监督的学习的方法

80
00:03:11,080 --> 00:03:13,280
而后面针对不同的效用任务

81
00:03:13,280 --> 00:03:15,720
使用的是一个 Find Tuning 的工作

82
00:03:15,720 --> 00:03:18,920
在第三步就会详细的去介绍 BERT 网络模型的

83
00:03:18,920 --> 00:03:21,240
组织结构网络模型的方式

84
00:03:21,280 --> 00:03:24,760
3.1 里面就讲了 Pretraining BERT 里面分为两个 text

85
00:03:24,760 --> 00:03:26,760
第一个 text 就是 Masked LM

86
00:03:26,760 --> 00:03:29,000
第二个 text 就是 Next  Sentence Prediction

87
00:03:29,000 --> 00:03:32,440
就是下一句话或者下一个单词的预测

88
00:03:32,440 --> 00:03:34,840
这个图将会在后面进行展开

89
00:03:34,840 --> 00:03:35,560
Experience

90
00:03:35,560 --> 00:03:38,320
Experiments 主要是基于 GLUE 这个 benchmark

91
00:03:38,320 --> 00:03:41,440
就是 GLUE 这个下游任务去进行对比的

92
00:03:41,440 --> 00:03:43,840
而 BERT 网络模型在各个下游任务里

93
00:03:43,840 --> 00:03:46,480
都取得了一个非常 SOTA 的结果

94
00:03:46,480 --> 00:03:49,960
同样 BERT 网络模型后面又有非常多的附录

95
00:03:50,000 --> 00:03:51,450
而附录也是非常值得

96
00:03:51,450 --> 00:03:53,720
深入的去研究这个网络模型

97
00:03:53,720 --> 00:03:55,360
具体长什么样子的

98
00:03:55,360 --> 00:03:57,880
会提供了很多详细的信息

99
00:03:57,880 --> 00:03:59,360
从论文里面知道

100
00:03:59,360 --> 00:04:02,000
BERT 网络模型是一个双向编码的网络模型

101
00:04:02,000 --> 00:04:05,880
但是这个双向编码实际上它只是一个概念

102
00:04:05,880 --> 00:04:09,320
在 transformer 的结构还是那个 transformer 的结构

103
00:04:09,320 --> 00:04:12,640
那看看它具体双向表现在哪里

104
00:04:12,640 --> 00:04:15,640
首先它网络模型分为一个 Pretraining 和 Fine-Tuning

105
00:04:15,640 --> 00:04:16,760
Fine-Tuning 先不管

106
00:04:16,760 --> 00:04:18,200
看看 Pretraining

107
00:04:18,240 --> 00:04:20,475
数据的输出和输入就取决于

108
00:04:20,475 --> 00:04:22,960
网络模型的训练的过程当中

109
00:04:22,960 --> 00:04:26,720
它到底是一个监督学习或者无监督学习的方式

110
00:04:26,720 --> 00:04:29,520
这里面有几个内容可能要稍微注意一下的

111
00:04:29,520 --> 00:04:31,360
这一个就是 CLS

112
00:04:31,360 --> 00:04:33,040
第二个 SEP

113
00:04:33,040 --> 00:04:34,840
第三个就是 MASK

114
00:04:34,840 --> 00:04:37,400
SEP 和 CLS 只是一个标志位

115
00:04:37,400 --> 00:04:39,400
标志我是从句子的开头

116
00:04:39,400 --> 00:04:40,560
句子的段距

117
00:04:40,560 --> 00:04:43,360
而 MASK 这个才是最重要的概念

118
00:04:43,360 --> 00:04:44,920
假设现在输一段话

119
00:04:44,920 --> 00:04:46,200
My Dot is cute

120
00:04:46,200 --> 00:04:48,760
那这个 Cute 我把它 MASK 就是遮住

121
00:04:48,760 --> 00:04:50,760
然后输进去网络模型

122
00:04:50,760 --> 00:04:52,920
输出的时候我把 My Dot is cute

123
00:04:52,920 --> 00:04:54,800
然后同样作为我的输出

124
00:04:54,800 --> 00:04:58,800
然后让神经网络模型去预测这个 Cute

125
00:04:58,800 --> 00:05:00,800
从而实现自监督的功能

126
00:05:00,800 --> 00:05:03,680
就我的数据已经不用人工的去标了

127
00:05:03,680 --> 00:05:05,900
让神经网络模型自动的

128
00:05:05,900 --> 00:05:08,120
去从数据之间发现它的规律

129
00:05:08,120 --> 00:05:09,960
而双向代表的什么意思呢

130
00:05:09,960 --> 00:05:12,960
假设我去预测是它的前后的关系

131
00:05:13,000 --> 00:05:14,720
前面是 Dog 还是 Cute 呢

132
00:05:14,720 --> 00:05:17,240
就是前后左右我都会进行一个预测

133
00:05:17,240 --> 00:05:19,600
假设是 Transformer 的 Q

134
00:05:19,600 --> 00:05:21,000
K1 可能是 Dot

135
00:05:21,000 --> 00:05:21,800
K2 是 Cute

136
00:05:21,800 --> 00:05:23,520
K3 是 He 

137
00:05:23,520 --> 00:05:26,720
因为 Transformer 网络模型 QKV 的特殊结构

138
00:05:26,720 --> 00:05:28,000
所以通过 MASK 之后

139
00:05:28,000 --> 00:05:30,960
它就变得有一种双向的概念

140
00:05:30,960 --> 00:05:33,440
所以双向是来自于这

141
00:05:33,440 --> 00:05:39,120
通过输进去 BERT 网络模型去预测单词到底是什么

142
00:05:39,120 --> 00:05:40,800
同样除了 MASK 单词

143
00:05:40,800 --> 00:05:42,640
还可以 MASK 下一个句子

144
00:05:43,480 --> 00:05:45,125
那第一个任务就是

145
00:05:45,125 --> 00:05:47,960
在句子里面随机的去遮住一部分单词

146
00:05:47,960 --> 00:05:51,640
然后去预测这部分单词到底是什么内容

147
00:05:51,640 --> 00:05:54,360
那第二任务就是 Next Sequence Prediction

148
00:05:54,360 --> 00:05:57,120
就是预测下一个句子是什么

149
00:05:57,960 --> 00:06:00,300
为什么说 BERT 网络模型

150
00:06:00,300 --> 00:06:02,280
对于大模型来说非常重要呢

151
00:06:02,840 --> 00:06:04,025
是因为 BERT 网络模型

152
00:06:04,025 --> 00:06:06,120
串了非常多 Transformer 的 Encoder

153
00:06:06,120 --> 00:06:08,800
它实际上通过超大规模的数据

154
00:06:08,800 --> 00:06:11,080
还有非常大的网络模型结构

155
00:06:11,080 --> 00:06:14,320
并且还用了谷歌的 TPU 去进行计算的

156
00:06:14,320 --> 00:06:17,920
在 11 个 NLP 应用里面都取得了非常好的结果

157
00:06:17,920 --> 00:06:19,760
这里面除了网络模型的大

158
00:06:19,760 --> 00:06:21,440
还用了超大的数据

159
00:06:21,440 --> 00:06:23,600
另外训练还分为了两个阶段

160
00:06:23,600 --> 00:06:26,320
第一个是预训练的阶段和 Fine-Tuning 的阶段

161
00:06:26,320 --> 00:06:28,125
预训练阶段这个概念

162
00:06:28,125 --> 00:06:31,760
把网络模型引入了一个更高规模的规格

163
00:06:33,680 --> 00:06:35,040
这些算法讲的好累

164
00:06:36,160 --> 00:06:37,520
虽然我很努力的在讲

165
00:06:38,800 --> 00:06:41,120
但是可能你仍然还是听不明白

166
00:06:41,120 --> 00:06:42,440
听不明白没关系

167
00:06:42,440 --> 00:06:44,880
其实我更希望大家去阅读原论文

168
00:06:44,880 --> 00:06:48,840
去仔细的去研究一下这个网络模型到底有什么用

169
00:06:48,840 --> 00:06:50,720
具体理念是怎么去实现的

170
00:06:50,720 --> 00:06:53,320
下面来到第四个重要的网络模型

171
00:06:53,320 --> 00:06:55,400
就是 GPT-3

172
00:06:55,400 --> 00:06:57,425
前面的 GPT-1 和 GPT-2

173
00:06:57,425 --> 00:07:00,520
实际上并没有太多的 Outstanding 的工作

174
00:07:00,520 --> 00:07:03,475
但是当 GPT-3 的网络模型的参数量

175
00:07:03,475 --> 00:07:06,520
都已经上到了接近两千亿规模的时候

176
00:07:06,560 --> 00:07:09,600
它带来的是一个全新的语言模型

177
00:07:09,600 --> 00:07:12,760
这个全新的语言模型跟刚才的 BERT

178
00:07:12,760 --> 00:07:14,920
它需要经过微调不一样

179
00:07:14,920 --> 00:07:16,120
GTP-3

180
00:07:16,120 --> 00:07:18,960
居于刚才的 BERT 做了一个无监督之外

181
00:07:18,960 --> 00:07:20,760
它还实现了一个 Zero Shot

182
00:07:20,760 --> 00:07:22,720
就是我不需要 Fine-Turing 了

183
00:07:22,720 --> 00:07:24,680
我直接进行个 zero-shot

184
00:07:24,680 --> 00:07:26,475
然后我的预训练模型

185
00:07:26,475 --> 00:07:28,920
直接可以对应到具体的下游任务

186
00:07:28,920 --> 00:07:30,400
这里面右边的这个图

187
00:07:30,400 --> 00:07:32,325
就是讲 GPT-3 的一个 Few-Shot

188
00:07:32,325 --> 00:07:34,720
One-Shot 和 Zero-Shot 具体的效果

189
00:07:34,720 --> 00:07:38,040
可以看到它的 Few-Shot 的效果是非常好的

190
00:07:38,040 --> 00:07:40,920
至于 Zero 可能还是有点差别

191
00:07:40,920 --> 00:07:43,800
Few-Shot 就代表我需要有一些引导的单词

192
00:07:43,800 --> 00:07:45,360
有一些引导的向下文

193
00:07:45,360 --> 00:07:47,120
让它可以预测的更好

194
00:07:47,120 --> 00:07:49,600
而 Zero-Shot 就我不需要任何引导

195
00:07:49,600 --> 00:07:51,760
我直接去对下游任务进行推理

196
00:07:51,760 --> 00:07:53,800
它的区别就在于这

197
00:07:53,800 --> 00:07:56,200
最近印度小哥真的非常火

198
00:07:56,200 --> 00:07:58,600
除了当上了印度首相之外

199
00:07:58,600 --> 00:08:01,800
他们在 AI 的分享知识和 AI 理论的创新方面

200
00:08:01,800 --> 00:08:03,760
还是有非常多的工作的

201
00:08:03,800 --> 00:08:08,240
下面来看看这个印度小哥的一个分享

202
00:08:08,240 --> 00:08:13,160
现在具体一起来看看 GPT-3 是怎么去实现的

203
00:08:13,160 --> 00:08:15,920
GPT-3 的数现在变成了一个 Prompt

204
00:08:15,920 --> 00:08:17,480
是一个英文的句子

205
00:08:17,480 --> 00:08:19,880
输出也是一个英文的句子

206
00:08:19,880 --> 00:08:22,800
具体是通过一个 Unsuperized Pre-training

207
00:08:22,800 --> 00:08:25,320
就是无监督的一个训练学习

208
00:08:25,320 --> 00:08:29,160
训练的数据集有 300 Billion Token of Text

209
00:08:29,160 --> 00:08:32,920
而训练的目标就是简单的去预测下一个单词

210
00:08:32,920 --> 00:08:35,360
所以它是非常简单的模式

211
00:08:35,360 --> 00:08:37,480
而 robot must 什么呢

212
00:08:37,480 --> 00:08:39,200
它就是预测一个单词

213
00:08:39,200 --> 00:08:41,200
然后给 GPT-3 进行训练

214
00:08:41,200 --> 00:08:45,240
然后最终输出 GPT-3 的一个网络模型

215
00:08:45,240 --> 00:08:48,520
现在来看看具体的 Sentence 有什么不一样

216
00:08:48,520 --> 00:08:50,400
下面有具体的一句话

217
00:08:50,400 --> 00:08:53,400
无论是 123 它还是一句话

218
00:08:53,400 --> 00:08:54,750
而这个冒号就相当于

219
00:08:54,750 --> 00:08:57,200
刚才讲 BERT 的时候一个 MASK

220
00:08:57,200 --> 00:09:00,200
Second law of robotics 什么呢

221
00:09:00,200 --> 00:09:03,240
Second law of robotics 什么 a 呢

222
00:09:03,240 --> 00:09:06,960
Second law of robot 什么 a robot 呢

223
00:09:06,960 --> 00:09:08,880
话呢还是那句话

224
00:09:08,880 --> 00:09:11,120
但是它里面的 Mask 就不一样了

225
00:09:11,120 --> 00:09:13,400
它的 Mask 是预测下一个单词

226
00:09:13,400 --> 00:09:18,160
而给出的 prompt 和提示就更加详细或者直接挖空

227
00:09:18,160 --> 00:09:22,240
使得 GPT-3 具有一个 Zero shot 和 One shot 的功能

228
00:09:22,240 --> 00:09:24,600
这个就是它最大的区别

229
00:09:24,600 --> 00:09:27,120
而这里面的语料非常丰富

230
00:09:27,120 --> 00:09:30,080
Transformer 的层数套了 96 层

231
00:09:30,080 --> 00:09:33,560
刚才讲的 BTER Large 其实只有 24 层

232
00:09:33,560 --> 00:09:35,560
这里面的语料进入不增大了

233
00:09:35,560 --> 00:09:36,680
通过 promot 方式

234
00:09:36,680 --> 00:09:40,400
把一句话可能变成 3,4,5,6 句话非常多

235
00:09:40,400 --> 00:09:43,040
所以需要经过 96 层 Transformer

236
00:09:43,040 --> 00:09:45,400
最后输出预测结果

237
00:09:45,400 --> 00:09:48,280
这就是 GPT-3 的一个最重要的概念

238
00:09:48,280 --> 00:09:52,200
就是它大部分是一个概念和数据处理流程的创新

239
00:09:52,200 --> 00:09:55,200
对网络模型结构就是不断的堆掉

240
00:09:55,200 --> 00:09:56,925
这年不可忽视的就是要

241
00:09:56,925 --> 00:09:59,720
训练一个具有 96 层的 Transformer

242
00:09:59,720 --> 00:10:02,040
一张卡肯定是塞不下的

243
00:10:02,040 --> 00:10:04,320
所以需要分布式并行的一些工作

244
00:10:04,320 --> 00:10:08,080
这些工作会在后面的章节里面详细的去展开

245
00:10:09,080 --> 00:10:13,720
Switch Transformer 是首个突破万亿规模的大模型

246
00:10:13,720 --> 00:10:14,600
万亿规模

247
00:10:14,600 --> 00:10:16,360
万亿规模是非常夸张的

248
00:10:16,680 --> 00:10:18,360
Switch Transformer 最重要的工作

249
00:10:18,360 --> 00:10:19,400
就是把开关

250
00:10:19,400 --> 00:10:22,120
添加 Transformer 的自注意力层之间

251
00:10:22,120 --> 00:10:25,200
可以看到 Transformer 里面有一个 FFN

252
00:10:25,200 --> 00:10:28,480
就是经过 MultiHead 之后它会有个 FFN

253
00:10:28,600 --> 00:10:31,360
这里面把 FFN 当作 Export

254
00:10:31,360 --> 00:10:34,760
然后通过路由机制去选定某个 Export

255
00:10:35,160 --> 00:10:38,440
在 Switch Transformer 标题里面就明确说了

256
00:10:38,440 --> 00:10:41,960
Scaling to Trillion Parameter Models

257
00:10:41,960 --> 00:10:46,000
然后用一个非常高效和简单的稀疏结构

258
00:10:46,000 --> 00:10:49,640
可以看到稀疏系数结构主要是讲 MOE

259
00:10:49,640 --> 00:10:54,280
它把 MOE 的方式放在 Transformer 的结构里面

260
00:10:54,280 --> 00:10:56,680
但是它不是代替 QKV

261
00:10:56,680 --> 00:10:58,480
而是代替 FFN

262
00:10:58,480 --> 00:11:01,080
把 FFN 结构当成它的 Export

263
00:11:01,080 --> 00:11:03,360
但是它跟传统的 MOE 不一样

264
00:11:03,360 --> 00:11:06,400
传统的 MOE 是通过 Export 进行预测的

265
00:11:06,400 --> 00:11:09,000
而这里面通过一个 Export 进行预测的

266
00:11:09,000 --> 00:11:10,880
是因为网络的规模太大了

267
00:11:10,880 --> 00:11:13,080
如果通过多个 Export 进行预测的时候

268
00:11:13,080 --> 00:11:14,440
这篇文章就发现了

269
00:11:14,440 --> 00:11:17,320
它会进一步的加大计算的开销

270
00:11:17,320 --> 00:11:18,320
通信的开销

271
00:11:18,320 --> 00:11:20,440
而通过单个 Export 进行预测

272
00:11:20,440 --> 00:11:24,120
可以有效的加快训练的时间和训练的收敛性

273
00:11:24,600 --> 00:11:27,640
2.1 章节里面就去讲讲稀疏路由

274
00:11:27,640 --> 00:11:29,160
具体是怎么去实现的

275
00:11:29,160 --> 00:11:31,000
它的计算公式是怎么算的

276
00:11:31,720 --> 00:11:34,080
而这里面就是稀疏的选择

277
00:11:34,080 --> 00:11:37,240
这里面就是专家选择的一种具体的方式

278
00:11:37,240 --> 00:11:41,040
再往下就是分布式的一个稀疏专家的选择

279
00:11:41,040 --> 00:11:44,600
当然了这篇文章不仅也是提供了非常多的实验过程

280
00:11:44,600 --> 00:11:46,400
这个万亿规模的网络模型

281
00:11:46,400 --> 00:11:48,360
从这张图里面可以看到

282
00:11:48,360 --> 00:11:51,080
这个就是稠密的计算的时候它的精度

283
00:11:51,120 --> 00:11:54,400
上面蓝色的点就是 Switch Transformer 的性能

284
00:11:54,400 --> 00:11:59,120
实际上它的下标代表的是 101 种语言的翻译任务

285
00:11:59,120 --> 00:12:01,960
都有非常好的性能的提升

286
00:12:01,960 --> 00:12:06,520
通过一个网络模型就解决了 100 多种任务的性能提升

287
00:12:06,520 --> 00:12:09,325
所以这是一个非常 Outstanding

288
00:12:09,325 --> 00:12:11,080
或者非常 SOTA 的一篇文章

289
00:12:11,080 --> 00:12:15,160
而要训练一个万亿规模的网络模型其实没有那么简单

290
00:12:15,160 --> 00:12:17,520
而这里面又有一个更详细的展开

291
00:12:17,560 --> 00:12:19,240
FFN 作用的 Expert

292
00:12:19,240 --> 00:12:22,880
FFN 的输出给是 QKV 这个参数

293
00:12:22,880 --> 00:12:25,600
但实际上要训练一个万亿一规模的参数

294
00:12:25,600 --> 00:12:27,375
其实刚才只是停留在

295
00:12:27,375 --> 00:12:29,040
去讲网络模型结构

296
00:12:29,040 --> 00:12:30,560
具体是怎么实现的

297
00:12:30,560 --> 00:12:32,325
但是万亿规模的参数

298
00:12:32,325 --> 00:12:35,080
实际上没有那么简单的去训练起来

299
00:12:35,080 --> 00:12:37,680
这里面就用到了数据并行

300
00:12:37,680 --> 00:12:38,440
模型并行

301
00:12:38,440 --> 00:12:40,920
模型和数据的这种混合并行

302
00:12:40,920 --> 00:12:44,080
另外它还提出了专家和数据的并行

303
00:12:44,080 --> 00:12:45,680
图里面的不同颜色

304
00:12:45,720 --> 00:12:47,480
就代表不同的专家

305
00:12:47,480 --> 00:12:49,240
专家和模型和数据并行

306
00:12:49,240 --> 00:12:51,440
就像这种方式具体组成

307
00:12:51,440 --> 00:12:53,500
所以说这篇文章不仅仅是

308
00:12:53,500 --> 00:12:56,880
讲了一些网络模型的结构的革新或者一些算法

309
00:12:56,880 --> 00:12:59,880
它还提出了专家和混合并行的具体的方式

310
00:12:59,880 --> 00:13:02,800
而这篇文章同样是出自于谷歌之手

311
00:13:02,800 --> 00:13:05,925
所以 TensorFlow 或者谷歌的 TPU

312
00:13:05,925 --> 00:13:07,480
是非常值得去研究

313
00:13:07,480 --> 00:13:09,200
但是在真正编码的时候

314
00:13:09,200 --> 00:13:12,080
可能会选择 MindSpore 或者 PyTorch 去实现

315
00:13:12,080 --> 00:13:14,480
因为 TensorFlow 确实太难学了

316
00:13:14,520 --> 00:13:17,125
最后一个网络模型就是 GLaM

317
00:13:17,125 --> 00:13:20,760
 1.2 万亿参数的一个通用的稀疏系数语言模型

318
00:13:20,760 --> 00:13:21,720
既然是通用

319
00:13:21,720 --> 00:13:26,520
所以它的 Zero-Shot 或者 One-Shot 的性能肯定是越来越好

320
00:13:26,520 --> 00:13:29,080
比刚才的 Switch Transformer 更好

321
00:13:29,080 --> 00:13:32,240
这里面它对每一层的 MOE 进行控制

322
00:13:32,240 --> 00:13:34,440
里面只有 64 个专家

323
00:13:34,440 --> 00:13:36,320
而有 32 层 MOE

324
00:13:36,320 --> 00:13:38,720
这篇文章就不详细的去展开

325
00:13:38,720 --> 00:13:42,480
大家有兴趣的可以去看看 GLaM 这篇文章

326
00:13:42,520 --> 00:13:43,880
现在来总结一下

327
00:13:43,880 --> 00:13:45,760
今天的内容可能稍微长了一点

328
00:13:45,760 --> 00:13:48,840
就是 Attention is all you need 的这种 Transformer

329
00:13:48,840 --> 00:13:51,960
引发了 AI 迈进大模型的时代

330
00:13:51,960 --> 00:13:54,160
有 Transformer 才有大模型

331
00:13:54,160 --> 00:13:56,440
第二个就是稀疏门控的 MOE

332
00:13:56,440 --> 00:14:01,160
有 MOE 网络模型才有资格迈向万亿规模

333
00:14:01,160 --> 00:14:03,760
只是稠密的 Transformer 的计算是没办法

334
00:14:03,760 --> 00:14:06,720
让网络模型迈入万亿规模的

335
00:14:06,720 --> 00:14:09,480
第三个就讲 BERT 网络模型

336
00:14:09,480 --> 00:14:12,040
这个芝麻街的椒麻鸡兄弟

337
00:14:12,040 --> 00:14:14,680
Transformer 的双向无监督编码的结构

338
00:14:14,680 --> 00:14:18,120
使得网络模型可以进行一个预训练

339
00:14:18,160 --> 00:14:21,320
而 GPT-3 用了 96 层 Transformer

340
00:14:21,320 --> 00:14:25,000
实现了一个自回归的千亿规模的语言模型

341
00:14:25,000 --> 00:14:27,760
就直接用 Zero Shpt 就可以实现了

342
00:14:27,760 --> 00:14:30,320
而不需要像 BERT 一样做 Fine Turning

343
00:14:30,320 --> 00:14:32,775
而 Switch Transformer 将

344
00:14:32,775 --> 00:14:35,360
MOE 和 Transformer 的结构结合在一起

345
00:14:35,360 --> 00:14:39,240
把 Transformer 的 FFT 当作 MOE 的一个专家

346
00:14:39,240 --> 00:14:42,120
最后就是有谷歌的 GLaM

347
00:14:42,120 --> 00:14:46,760
在 Few  Shot 的领域里面直接打败了 GPT-3

348
00:14:46,800 --> 00:14:48,160
讲了这么多大模型

349
00:14:48,160 --> 00:14:50,960
其实并没有深入的去看 CV 大模型

350
00:14:50,960 --> 00:14:53,240
NLP 大模型和多模态大模型

351
00:14:53,240 --> 00:14:56,640
而是重点去讲讲网络模型的结构

352
00:14:56,640 --> 00:15:00,040
对大模型去提出了新的挑战

353
00:15:00,040 --> 00:15:02,240
和新的一些奠基的功能

354
00:15:02,240 --> 00:15:05,720
从 MOE Transformer 使得大模型爆发了

355
00:15:05,720 --> 00:15:07,680
出现了 BERT 和 GPT-3

356
00:15:07,680 --> 00:15:10,960
使得大模型的精度进行了进一步的突破

357
00:15:10,960 --> 00:15:12,880
和解决了碎片化的问题

358
00:15:12,880 --> 00:15:14,250
所以今天的分享

359
00:15:14,250 --> 00:15:16,720
主要还是围绕着大模型的网络模型结构

360
00:15:16,720 --> 00:15:18,960
然后模型的结构模型的参数量

361
00:15:18,960 --> 00:15:22,280
如何一步步的增大去进行展开

362
00:15:22,280 --> 00:15:25,720
对大模型结构有兴趣的同学可以多阅读原论文

