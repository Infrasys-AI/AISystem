1
00:00:00,000 --> 00:00:07,760
Hello 大家好,我是 ZOMI

2
00:00:07,760 --> 00:00:11,040
今天来到了分布式算法系列

3
00:00:11,040 --> 00:00:12,560
那聊到分布式算法

4
00:00:12,560 --> 00:00:14,900
肯定现在只会说

5
00:00:14,900 --> 00:00:17,760
大模型的一个分布式算法

6
00:00:17,760 --> 00:00:21,320
在今天内容聚焦于在这一块

7
00:00:21,320 --> 00:00:24,400
之前的内容讲了分布式加 AI 集群

8
00:00:24,400 --> 00:00:28,560
今天看看大模型和训练遇到的一些挑战

9
00:00:28,680 --> 00:00:31,120
首先要回答一个什么是大模型

10
00:00:31,120 --> 00:00:32,560
大模型有什么用

11
00:00:32,560 --> 00:00:36,560
第二个来给大家汇报一下大模型训练的四个挑战

12
00:00:36,560 --> 00:00:40,640
也是现在去训练大模型所遇到的一些真正的挑战

13
00:00:41,960 --> 00:00:43,760
在视频开始介绍之前

14
00:00:43,760 --> 00:00:45,720
ZOMI 想抛出几个问题

15
00:00:45,720 --> 00:00:48,680
希望能够引起大家的一个思考

16
00:00:48,680 --> 00:00:52,720
第一个是为什么预训练网络模型变得越来越重要

17
00:00:52,720 --> 00:00:55,960
第二个是预训练的大模型未来的发展趋势

18
00:00:56,960 --> 00:01:01,560
第三个就是如何训练一个百亿规模的大模型

19
00:01:01,560 --> 00:01:06,000
2021 年 8 月份李飞飞和 100 多位学子

20
00:01:06,000 --> 00:01:09,040
联名发布了一份 200 多页的研究报告

21
00:01:09,040 --> 00:01:15,360
这份研究报告里面深度的中述了当前大规模预训练模型面临的机遇和挑战

22
00:01:15,360 --> 00:01:18,523
文章当中 AI 专家将大模型统一命名为

23
00:01:18,523 --> 00:01:20,680
大模型统一命名为 Foundation Model

24
00:01:20,680 --> 00:01:24,040
可以翻译为基础模型或者是基石模型

25
00:01:24,280 --> 00:01:27,500
论文里面肯定了 Foundation Model

26
00:01:27,500 --> 00:01:29,640
对智能体基本认知能力的推动作用

27
00:01:29,640 --> 00:01:32,175
同时也指出了大模型呈现出了

28
00:01:32,175 --> 00:01:34,080
涌现和同质化的一个特性

29
00:01:35,160 --> 00:01:36,760
回到大模型里面

30
00:01:36,760 --> 00:01:39,480
2017 年 Transformer 结构的首次提出

31
00:01:39,480 --> 00:01:43,240
使得深度学习模型参数量突破了一个亿

32
00:01:43,240 --> 00:01:45,950
下面这个图就是从一开始的 LeNet、

33
00:01:45,950 --> 00:01:48,240
AlexNet 到 ResNet50

34
00:01:48,240 --> 00:01:50,040
模型参数一个比一个大

35
00:01:50,040 --> 00:01:52,200
到了 Bert 网络模型的提出

36
00:01:52,240 --> 00:01:55,520
使得参数量首次超过了 3 亿规模

37
00:01:55,520 --> 00:01:59,360
那 GTP-3 的模型规模就超过了百亿了

38
00:01:59,360 --> 00:02:02,560
像鹏城盘古实现了千亿稠密的规模

39
00:02:02,560 --> 00:02:05,575
而 Google 的 Switch Transformer 的问世

40
00:02:05,575 --> 00:02:07,400
还一举突破了万亿规模

41
00:02:08,840 --> 00:02:12,760
大模型在产学各界掀起兼顾了一阵阵巨浪

42
00:02:12,760 --> 00:02:15,350
背后彰显的除了分布式并行和

43
00:02:15,350 --> 00:02:17,480
对 AI 算法的掌控能力之外 

44
00:02:17,480 --> 00:02:20,400
还是一次大公司通过 AI 工程的创举

45
00:02:20,440 --> 00:02:22,250
利用大规模 AI 集群来

46
00:02:22,250 --> 00:02:24,040
进行扳手腕的一个故事

47
00:02:25,040 --> 00:02:27,080
随着网络模型越来越大

48
00:02:27,080 --> 00:02:29,575
单机单卡、一机多卡甚至 

49
00:02:29,575 --> 00:02:31,280
多机多卡的小规模集群

50
00:02:31,280 --> 00:02:35,840
只要网络模型参数量一旦超过了 10 个亿以上的规模

51
00:02:35,840 --> 00:02:38,480
就很难用现有的资源去训练了

52
00:02:38,480 --> 00:02:41,080
于是有的研究者就会提出质疑

53
00:02:41,080 --> 00:02:42,720
一味的让模型变大

54
00:02:42,720 --> 00:02:44,760
让参数量爆炸式的增长

55
00:02:44,760 --> 00:02:48,040
真的能让 AI 模型学习变得更好吗

56
00:02:48,080 --> 00:02:50,200
真的能带来真正的智能吗

57
00:02:51,200 --> 00:02:53,600
虽然大模型刚提出的时候

58
00:02:53,600 --> 00:02:55,280
质疑的声音会有

59
00:02:55,280 --> 00:02:56,800
但不可否认的是

60
00:02:56,800 --> 00:02:59,250
大模型做到了早期预训练模型

61
00:02:59,250 --> 00:03:01,080
做不到、做不好的事情

62
00:03:01,080 --> 00:03:04,025
就好像自然语言处理当中的文字生成

63
00:03:04,025 --> 00:03:06,480
文本理解、自动问答等效果的任务

64
00:03:06,480 --> 00:03:08,800
不仅生成的文本更加流畅了

65
00:03:08,800 --> 00:03:12,560
甚至内容的述实性又有了非常显著的改进

66
00:03:12,560 --> 00:03:13,520
当然了

67
00:03:13,520 --> 00:03:16,040
大模型最终能否走向通用人工智能

68
00:03:16,040 --> 00:03:17,200
仍然是一个未知数

69
00:03:19,000 --> 00:03:20,720
有了大模型的基本介绍

70
00:03:20,720 --> 00:03:22,800
来看看大模型的具体作用

71
00:03:22,800 --> 00:03:25,320
首先第一个就是模型碎片化

72
00:03:25,320 --> 00:03:27,840
大模型这时候提供了预训练方案

73
00:03:27,840 --> 00:03:31,440
现在 AI 面向的行业和业务场景非常多

74
00:03:31,440 --> 00:03:34,320
从开发、调参、优化、迭代到应用

75
00:03:34,320 --> 00:03:36,200
AI 模型的研发成本很高

76
00:03:36,200 --> 00:03:39,080
而且很难满足市场的定制化需求

77
00:03:39,080 --> 00:03:41,200
所以这个时候很多人就会说

78
00:03:41,200 --> 00:03:45,120
现阶段的 AI 模型处于一个手工作坊式

79
00:03:45,120 --> 00:03:46,920
而为了解决这个问题

80
00:03:46,960 --> 00:03:48,640
大模型提供了另外一种

81
00:03:48,640 --> 00:03:51,720
预训练大模型加下游任务微调的方式

82
00:03:51,720 --> 00:03:54,760
大模型预训练可以有效地从大量标记

83
00:03:54,760 --> 00:03:57,520
和没有标记的数据当中获取知识

84
00:03:57,520 --> 00:04:00,240
通过将知识存储在大量的参数当中

85
00:04:00,240 --> 00:04:02,320
并对特定项目进行微调

86
00:04:02,320 --> 00:04:05,280
非常好地扩展了模型的泛化能力

87
00:04:05,280 --> 00:04:08,640
第二点就是大模型具备自监督学习的功能

88
00:04:08,640 --> 00:04:11,160
降低训练研发成本

89
00:04:11,160 --> 00:04:13,600
大模型的自监督学习方法

90
00:04:13,600 --> 00:04:15,960
可以减少了数据标注的成本

91
00:04:15,960 --> 00:04:19,960
使得小样本的学习也能够达到比以前更好的能力

92
00:04:19,960 --> 00:04:23,320
并且模型参数规模越大,优势越明显

93
00:04:23,320 --> 00:04:26,120
避免开发人员进行大规模的训练

94
00:04:26,120 --> 00:04:29,680
使用小样本就可以训练自己所需要的模型了

95
00:04:29,680 --> 00:04:31,760
极大地降低了开发的成本

96
00:04:31,760 --> 00:04:36,440
其实在一年前国家同时也出台了一个政策

97
00:04:36,440 --> 00:04:39,360
重新定义了什么是新一代农民工

98
00:04:39,360 --> 00:04:41,440
像这种搞数据标注的人

99
00:04:41,440 --> 00:04:43,960
就是新一代农民工了

100
00:04:43,960 --> 00:04:45,840
希望大家都不要惊吭

101
00:04:45,840 --> 00:04:47,920
不要成为新一代的农民工

102
00:04:47,920 --> 00:04:50,960
而是应该成为新一代的技术专家

103
00:04:54,680 --> 00:04:57,360
第三点就是大模型有望进一步

104
00:04:57,360 --> 00:05:00,240
突破现有模型结构的精度局限

105
00:05:00,240 --> 00:05:03,480
从深度学习的发展前十年的历程来看

106
00:05:03,480 --> 00:05:05,040
模型精度的提升

107
00:05:05,040 --> 00:05:07,840
主要是依赖于网络模型结构上的变革

108
00:05:07,840 --> 00:05:10,080
例如从 AlexNet 到 ResNet50

109
00:05:10,080 --> 00:05:12,480
再到 NAS 搜索出来的 EfficientNet

110
00:05:12,520 --> 00:05:14,400
ImageNet 比赛的精度

111
00:05:14,400 --> 00:05:18,200
Top1 的精度从 58%提升到 84%

112
00:05:18,200 --> 00:05:21,240
但是随着神经网络结构设计的技术 

113
00:05:21,240 --> 00:05:23,600
逐渐地走向成熟并收敛

114
00:05:23,600 --> 00:05:26,000
想要通过优化神经网络的结构

115
00:05:26,000 --> 00:05:27,840
来打破精度的局限

116
00:05:27,840 --> 00:05:30,160
现在来说是非常难的

117
00:05:30,160 --> 00:05:31,000
而近几年

118
00:05:31,000 --> 00:05:34,360
随着数据规模和模型的规模不断地增大

119
00:05:34,360 --> 00:05:37,400
模型的精度也进一步地得到了提升

120
00:05:37,400 --> 00:05:38,760
研究实验表明

121
00:05:38,760 --> 00:05:40,720
模型和数据规模的增大

122
00:05:40,760 --> 00:05:43,360
确实能够突破现有精度的一个局限

123
00:05:45,640 --> 00:05:49,000
由于大模型训练会比分布式训练更加复杂

124
00:05:49,000 --> 00:05:51,080
在深入了解大模型训练之前

125
00:05:51,080 --> 00:05:54,320
先来看看大模型训练会遇到哪些挑战

126
00:05:54,320 --> 00:05:57,520
理论上集群中的 AI 芯片数量越多

127
00:05:57,520 --> 00:05:59,360
模型训练的越快

128
00:05:59,360 --> 00:06:02,560
但是训练资源扩大到一定的规模的时候

129
00:06:02,560 --> 00:06:05,040
分布式并行就会出现局限性

130
00:06:05,040 --> 00:06:08,240
因为通讯的瓶颈会限制整个系统的性能

131
00:06:08,280 --> 00:06:11,520
增加计算资源反而会降低整体的加速比

132
00:06:13,440 --> 00:06:14,120
另外

133
00:06:14,120 --> 00:06:15,760
随着网络模型参数量

134
00:06:15,760 --> 00:06:18,250
从 AlexNet 的 6000 万

135
00:06:18,250 --> 00:06:21,040
到现在鹏城盘谷大模型的两千亿参数

136
00:06:21,040 --> 00:06:24,560
模型参数量的增大会使得内存急剧膨胀

137
00:06:24,560 --> 00:06:26,750
算子的增加也会使得就算

138
00:06:26,750 --> 00:06:28,760
使用模型或者流水线并行

139
00:06:28,760 --> 00:06:32,880
也很难在一个合理的时间内完成一个 Step 的训练

140
00:06:32,880 --> 00:06:35,225
最终会导致分布式训练

141
00:06:35,225 --> 00:06:37,600
不再适用于大模型训练任务

142
00:06:37,640 --> 00:06:40,960
上面就是大模型出现对 AI 系统的挑战

143
00:06:40,960 --> 00:06:43,760
面对大模型甚至分布式训练的技术上

144
00:06:43,760 --> 00:06:48,200
需要引入大规模训练的技术解决内存不够用的同时

145
00:06:48,200 --> 00:06:50,600
也不会被计算资源给限制住

146
00:06:51,600 --> 00:06:53,400
相比普通的分布式训练

147
00:06:53,400 --> 00:06:55,250
大模型训练在技术上

148
00:06:55,250 --> 00:06:57,960
需要考虑的问题就更加复杂了

149
00:06:57,960 --> 00:07:01,640
下面将大模型训练技术面临的挑战分为

150
00:07:01,640 --> 00:07:04,100
内存、通讯、计算和调优

151
00:07:04,100 --> 00:07:06,160
四个部分分别进行介绍

152
00:07:07,000 --> 00:07:11,000
首先,模型训练无可避免的问题就是内存墙

153
00:07:11,000 --> 00:07:13,360
以鹏城盘古大模型为例子

154
00:07:13,360 --> 00:07:15,725
2000 亿的参数内存

155
00:07:15,725 --> 00:07:18,000
占用就消耗了 754GB

156
00:07:18,000 --> 00:07:19,925
训练的过程因为会有

157
00:07:19,925 --> 00:07:21,960
权重、激活、优化器状态 

158
00:07:21,960 --> 00:07:25,360
再加上自动微分所产生的临时变量

159
00:07:25,360 --> 00:07:27,960
需要 3500GB 的内存

160
00:07:27,960 --> 00:07:30,720
一个大模型训练就需要 100 多块

161
00:07:30,720 --> 00:07:33,440
具有 32 个 G 内存的 AI 芯片

162
00:07:34,240 --> 00:07:36,800
以 ResNet50 的一轮迭代为例子

163
00:07:36,800 --> 00:07:38,760
看看内存的占用变化

164
00:07:38,760 --> 00:07:41,680
首先是网络模型开始计算的时候

165
00:07:41,680 --> 00:07:43,400
内存会不断的增加

166
00:07:43,400 --> 00:07:47,240
直到达到峰值 1.2GB

167
00:07:47,240 --> 00:07:50,120
峰值过后内存会开始逐渐的释放

168
00:07:50,120 --> 00:07:53,640
内存占用慢慢降回到 324Mbps

169
00:07:53,640 --> 00:07:55,120
一个 step 计算完之后

170
00:07:55,120 --> 00:07:56,920
仍然有一部分内存注流

171
00:07:56,920 --> 00:08:00,320
内存保持在 324Mbps 里面

172
00:08:00,320 --> 00:08:02,320
从上面的例子可以看出

173
00:08:02,360 --> 00:08:04,280
模型训练对内存的占用

174
00:08:04,280 --> 00:08:06,040
由常驻的静态内存

175
00:08:06,040 --> 00:08:09,400
和动态分配的动态内存两个部分组成

176
00:08:09,400 --> 00:08:10,760
在计算过程当中

177
00:08:10,760 --> 00:08:12,600
是否会遇到内存墙

178
00:08:12,600 --> 00:08:15,840
主要是由动态内存来决定的

179
00:08:15,840 --> 00:08:19,000
下面进一步打开静态和动态内存

180
00:08:19,000 --> 00:08:21,600
静态内存主要是由模型参数

181
00:08:21,600 --> 00:08:23,960
优化器状态信息组成

182
00:08:23,960 --> 00:08:26,320
在一个神经网络的计算过程当中

183
00:08:26,320 --> 00:08:28,320
卷积或者全连接层

184
00:08:28,320 --> 00:08:31,520
都会把权重 WM 长期保存下来

185
00:08:31,520 --> 00:08:34,440
用作网络的权重参数更新

186
00:08:34,440 --> 00:08:36,520
另外针对诸如 Adam 的优化器

187
00:08:36,520 --> 00:08:39,200
会存储优化器的动量等信息

188
00:08:40,200 --> 00:08:41,880
静态内存比较好理解

189
00:08:41,880 --> 00:08:44,400
下面主要是看看动态内存

190
00:08:44,400 --> 00:08:46,720
神经网络在计算的过程当中

191
00:08:46,720 --> 00:08:49,040
每一层的向前输出 Fo

192
00:08:49,040 --> 00:08:50,320
都需要保存下来

193
00:08:50,320 --> 00:08:52,880
给反向传播的时候所使用

194
00:08:52,880 --> 00:08:55,600
而在神经网络的反向传播过程当中

195
00:08:55,600 --> 00:08:59,520
需要记录下权重的梯度 WG 给优化器使用 

196
00:08:59,520 --> 00:09:01,400
在后续的 Step 更新当中

197
00:09:01,480 --> 00:09:02,960
梯度的信息

198
00:09:02,960 --> 00:09:04,960
另外还有梯度的输出 OG

199
00:09:04,960 --> 00:09:07,000
同样在反向传播过程当中

200
00:09:07,000 --> 00:09:09,480
作为上一层网络模型的输入

201
00:09:09,480 --> 00:09:11,080
最后还有算子计算的时候的

202
00:09:11,080 --> 00:09:13,080
临时变量 OPT 等

203
00:09:13,080 --> 00:09:14,440
刚刚提到的 OF

204
00:09:14,440 --> 00:09:15,000
OG

205
00:09:15,000 --> 00:09:15,720
WG

206
00:09:15,720 --> 00:09:16,240
OP

207
00:09:16,240 --> 00:09:17,120
积累下来

208
00:09:17,120 --> 00:09:19,280
会引起巨大的内存开销

209
00:09:19,280 --> 00:09:20,760
从 RestNet50 的例子

210
00:09:20,760 --> 00:09:21,960
可以看到

211
00:09:21,960 --> 00:09:24,480
动态内存是静态内存的三倍

212
00:09:25,360 --> 00:09:26,960
静态内存和动态内存

213
00:09:26,960 --> 00:09:29,160
都可能造成内存墙的问题

214
00:09:29,160 --> 00:09:29,920
相互独立

215
00:09:29,920 --> 00:09:31,320
但是又相互制约

216
00:09:31,320 --> 00:09:33,240
所以必须同时优化

217
00:09:34,840 --> 00:09:36,720
大模型通过模型并行

218
00:09:36,720 --> 00:09:39,640
流水线并行切分到 AI 集群之后

219
00:09:39,640 --> 00:09:42,160
通讯就会成为主要的性能瓶颈

220
00:09:42,840 --> 00:09:45,040
从模型切分的角度来考虑

221
00:09:45,040 --> 00:09:46,400
大模型再怎么切分

222
00:09:46,400 --> 00:09:48,240
其实还是一个模型

223
00:09:48,240 --> 00:09:50,840
因此模型切分到不同的机器后

224
00:09:50,840 --> 00:09:53,225
仍然需要通讯来对分布

225
00:09:53,225 --> 00:09:55,640
在不同的机器的参数进行聚合

226
00:09:56,200 --> 00:09:58,000
从通讯的角度来看

227
00:09:58,000 --> 00:09:59,160
参数聚合

228
00:09:59,200 --> 00:10:01,720
就会对通讯提出了很多要求

229
00:10:01,720 --> 00:10:03,560
例如使用同步的更新策略

230
00:10:03,560 --> 00:10:05,520
还是使用异步的更新策略

231
00:10:05,520 --> 00:10:08,840
或者还是对模型局部的变量进行同步更新呢

232
00:10:09,720 --> 00:10:11,840
另外在网络通讯的角度

233
00:10:11,840 --> 00:10:13,680
由于专用的 AI 芯片

234
00:10:13,680 --> 00:10:17,120
内存和计算单元 ALU 之间非常的接近

235
00:10:17,120 --> 00:10:20,160
使得片内的带宽可以非常非常大

236
00:10:20,160 --> 00:10:22,560
而计算速度也可以很快

237
00:10:22,560 --> 00:10:25,080
但是在集讯的网络传输速度

238
00:10:25,080 --> 00:10:26,975
远远不能够匹配专用的

239
00:10:26,975 --> 00:10:28,840
AI 加速芯片的运算速度

240
00:10:30,120 --> 00:10:31,520
有人就会问了

241
00:10:31,520 --> 00:10:33,250
直接使用更大的带宽

242
00:10:33,250 --> 00:10:35,080
不就能解决这些问题吗

243
00:10:35,080 --> 00:10:39,400
随着网络带宽从 1Gbps 到 100Gbps

244
00:10:39,400 --> 00:10:41,225
实际利用率将会从

245
00:10:41,225 --> 00:10:44,160
接近 100%下降到 40%左右

246
00:10:44,160 --> 00:10:45,880
网络带宽的增加

247
00:10:45,880 --> 00:10:48,320
带宽的利用率就会降得越来越低

248
00:10:48,320 --> 00:10:51,240
高带宽所带来的收益就会遇到瓶颈

249
00:10:51,240 --> 00:10:52,880
这个结论告诉

250
00:10:52,880 --> 00:10:54,760
不能随便增加带宽哦

251
00:10:54,760 --> 00:10:56,040
总体而言

252
00:10:56,040 --> 00:10:57,240
通讯过程当中

253
00:10:57,240 --> 00:10:59,480
需要综合考虑到数据参数量

254
00:10:59,480 --> 00:11:00,280
计算量

255
00:11:00,280 --> 00:11:01,200
计算类型

256
00:11:01,200 --> 00:11:02,400
数据样本量

257
00:11:02,400 --> 00:11:04,160
还有集讯的带宽 TOP

258
00:11:04,160 --> 00:11:06,360
和通讯策略等不同的因素

259
00:11:06,360 --> 00:11:09,480
才能够设计出比较优的切分策略

260
00:11:09,480 --> 00:11:11,200
最大化的利用通讯效率

261
00:11:11,200 --> 00:11:12,840
提高通讯比

262
00:11:13,880 --> 00:11:15,640
第三点就是性能墙

263
00:11:15,640 --> 00:11:17,800
主要是指计算资源的利用率

264
00:11:18,520 --> 00:11:19,840
随着大模型的提出

265
00:11:19,840 --> 00:11:21,920
对算力的需求更加迫切

266
00:11:21,920 --> 00:11:23,800
理论上在 4K 的集讯集群上面

267
00:11:23,800 --> 00:11:26,000
每块卡要是快了一分钟

268
00:11:26,000 --> 00:11:27,600
那可以总体就快了

269
00:11:27,600 --> 00:11:29,480
接近 68 个小时

270
00:11:29,480 --> 00:11:32,080
大模型会增加对算力的需求

271
00:11:32,080 --> 00:11:33,560
但是随着大模型引入

272
00:11:33,560 --> 00:11:35,920
各项分布式并行技术的同时

273
00:11:35,920 --> 00:11:38,400
反过来又会降低 AI 芯片的

274
00:11:38,400 --> 00:11:39,480
计算吞吐量

275
00:11:40,520 --> 00:11:43,000
大集群大模型的分布式训练

276
00:11:43,000 --> 00:11:45,800
混合了其实很多种并行的策略

277
00:11:45,800 --> 00:11:47,640
从算子级别上面

278
00:11:47,640 --> 00:11:49,120
如何利用编译技术

279
00:11:49,120 --> 00:11:50,880
对小算子进行融合

280
00:11:50,880 --> 00:11:53,520
如何找到更多的多线程计算策略

281
00:11:53,520 --> 00:11:55,840
是个非常值得探讨的话题

282
00:11:55,880 --> 00:11:57,640
那在计算图上面

283
00:11:57,640 --> 00:11:59,160
如何对整图搜索出

284
00:11:59,160 --> 00:12:02,240
更适合在 AI 集群的切分子图策略

285
00:12:02,240 --> 00:12:03,960
是个凸优化的问题

286
00:12:04,960 --> 00:12:06,640
到了集群任务上

287
00:12:06,640 --> 00:12:09,600
性能的瓶颈从计算转移到通讯

288
00:12:09,600 --> 00:12:12,600
这个时候需要尽可能的把通讯时延

289
00:12:12,600 --> 00:12:14,160
隐藏在计算里面

290
00:12:14,160 --> 00:12:17,200
是大规模训练的一个核心关注点

291
00:12:17,200 --> 00:12:18,160
因此

292
00:12:18,160 --> 00:12:20,560
性能强不仅仅要求 AI 芯片的

293
00:12:20,560 --> 00:12:22,080
计算性能足够的彪悍

294
00:12:23,240 --> 00:12:25,040
同时也依赖于 AI 框架的

295
00:12:25,040 --> 00:12:28,680
大规模分布式训练的运行和调度策略

296
00:12:28,680 --> 00:12:30,450
以及分布式并行等

297
00:12:30,450 --> 00:12:32,560
各种优化手段的一个权衡

298
00:12:33,880 --> 00:12:36,840
在数千节点的集群上面进行模型开发

299
00:12:36,840 --> 00:12:38,960
听到就头皮发麻

300
00:12:38,960 --> 00:12:41,640
我平时把 Efficient 的网络模型魔改

301
00:12:41,640 --> 00:12:43,360
还不一定能够收敛

302
00:12:43,360 --> 00:12:45,520
调一次参数再训练一次

303
00:12:45,520 --> 00:12:48,200
单机多卡一个星期就过去了

304
00:12:48,200 --> 00:12:50,080
要是再大模型训练一个月

305
00:12:50,080 --> 00:12:51,240
遇到 Loss 跑飞呢

306
00:12:51,240 --> 00:12:52,160
那怎么办呢

307
00:12:53,160 --> 00:12:55,640
所以在数千节点的集群上

308
00:12:55,640 --> 00:12:57,960
需要考虑到提升算法工程师

309
00:12:57,960 --> 00:12:59,960
分布式调试调优的效率

310
00:12:59,960 --> 00:13:01,880
另外还要考虑降低工程师

311
00:13:01,880 --> 00:13:04,880
对大模型进行并行切分的一个难度

312
00:13:05,880 --> 00:13:07,120
除了对人的考虑

313
00:13:07,120 --> 00:13:09,120
还需要对硬件集群的管理

314
00:13:09,120 --> 00:13:10,920
需要保证计算的正确性

315
00:13:10,920 --> 00:13:12,560
性能还有可用性

316
00:13:12,560 --> 00:13:14,600
例如要是有一台机器坏了

317
00:13:14,600 --> 00:13:17,320
如何快速的恢复训练的参数

318
00:13:18,320 --> 00:13:19,960
总结一句话

319
00:13:20,960 --> 00:13:25,240
大模型训练考验的是算法，数据，框架 

320
00:13:25,240 --> 00:13:28,920
资源调度等全栈全流程的综合能力

321
00:13:28,920 --> 00:13:31,240
最后希望 MindSpore 能够在大模型

322
00:13:31,240 --> 00:13:33,160
继续引领业界

323
00:13:33,160 --> 00:13:33,760
好了

324
00:13:33,760 --> 00:13:34,560
谢谢各位

325
00:13:34,560 --> 00:13:35,760
拜了个拜

