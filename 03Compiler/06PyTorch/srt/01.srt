1
00:00:00,000 --> 00:00:02,000
字幕生成: BLACK 字幕校对: 杨绎

1
00:00:05,360 --> 00:00:06,240
诶 开始了

2
00:00:06,240 --> 00:00:08,240
诶 大家好 我是 ZOMI

3
00:00:08,240 --> 00:00:10,400
刚才 11 点半才看完会

4
00:00:10,400 --> 00:00:11,200
然后刚洗完澡

5
00:00:11,200 --> 00:00:12,560
现在已经 12 点半了

6
00:00:12,560 --> 00:00:13,440
晚上

7
00:00:13,440 --> 00:00:15,760
那今天来开始一个新的内容

8
00:00:15,760 --> 00:00:17,680
就是 AI 编译器里面的 PyTorch

9
00:00:17,680 --> 00:00:21,200
大家都知道 PyTorch 最近发布了一个最新的版本 2.0

10
00:00:21,325 --> 00:00:23,040
它尝鲜了 还没有发布

11
00:00:23,040 --> 00:00:23,165
那现在来看看 PyTorch 2.0 有哪些新的内容

12
00:00:23,165 --> 00:00:27,200
那现在来看看 PyTorch 2.0 有哪些新的内容

13
00:00:27,200 --> 00:00:29,360
可以看到 PyTorch 2.0

14
00:00:29,360 --> 00:00:31,120
其实它引入了一个最大的概念

15
00:00:31,120 --> 00:00:32,800
就是编译

16
00:00:32,800 --> 00:00:36,160
这就完全符合最近聊到的 AI 编译器这个系列

17
00:00:36,160 --> 00:00:39,520
所以我把 PyTorch 2.0 这个新的话题往前提

18
00:00:39,520 --> 00:00:41,440
看一下这个汇报里面

19
00:00:41,440 --> 00:00:43,360
我给大家分开了四个内容

20
00:00:43,360 --> 00:00:46,080
第一个就是 PyTorch 2.0 的新特性

21
00:00:46,080 --> 00:00:48,880
首先来看看 PyTorch 2.0 有哪些新特性

22
00:00:48,880 --> 00:00:50,160
然后对它进行安装

23
00:00:50,160 --> 00:00:51,760
然后去使用一下

24
00:00:51,760 --> 00:00:55,200
接着一起来解读一下 PyTorch 的一些新的特性

25
00:00:55,280 --> 00:00:59,760
例如 Torch、Dynamo、AOTAutoGrid

26
00:00:59,760 --> 00:01:02,000
最后一个就是 TorchInductor

27
00:01:02,000 --> 00:01:03,280
三个最重要的特性

28
00:01:03,280 --> 00:01:05,200
将会后面的内容去展开

29
00:01:05,200 --> 00:01:09,440
今天先围绕 PyTorch 2.0 新特性一起去看看

30
00:01:12,080 --> 00:01:13,760
首先打开 PyTorch 的官网

31
00:01:13,760 --> 00:01:14,800
然后去到 Get Satrted

32
00:01:14,800 --> 00:01:16,480
点击 PyTorch 2.0

33
00:01:16,480 --> 00:01:20,000
下面这些就是 PyTorch 2.0 最新的特性介绍

34
00:01:20,000 --> 00:01:21,520
跳到 Install

35
00:01:21,520 --> 00:01:23,440
然后去看看怎么安装

36
00:01:24,400 --> 00:01:26,640
现在把这条命令复制粘贴

37
00:01:28,240 --> 00:01:29,840
到 Terminal 里面

38
00:01:34,480 --> 00:01:34,960
粘贴

39
00:01:35,600 --> 00:01:38,640
然后 Torch 2.0 居然已经开始安装了

40
00:01:39,600 --> 00:01:42,320
可以看到 PyTorch 2.0 虽然是 PyTorch 2.0

41
00:01:42,320 --> 00:01:47,040
但是现在的尝鲜版 Nightly 版还是 1.14 来给大家提供的

42
00:01:49,600 --> 00:01:50,240
居然错误

43
00:01:52,160 --> 00:01:52,960
没有权限

44
00:01:53,520 --> 00:01:54,000
没有权限

45
00:01:54,000 --> 00:01:56,320
那我加个 sudo 提升一下权限

46
00:02:09,120 --> 00:02:09,760
安装完了

47
00:02:11,600 --> 00:02:13,760
现在从上面到下面

48
00:02:13,760 --> 00:02:16,640
逐个的简单的浏览一下 PyTorch 2.0

49
00:02:16,640 --> 00:02:20,160
可以看到 PyTorch 2.0 主要推出 4 个重要的特性

50
00:02:20,160 --> 00:02:25,680
其中 Torch Compiler 里面这个技术栈就包含了 4 个最重要的特性

51
00:02:25,680 --> 00:02:26,640
Torch Dynamo

52
00:02:26,640 --> 00:02:27,440
Autograd

53
00:02:27,440 --> 00:02:28,640
还有 Torch Inductor

54
00:02:30,960 --> 00:02:34,880
这个图等一下后面会详细的去展开解读

55
00:02:34,880 --> 00:02:35,760
这个就是版本

56
00:02:38,480 --> 00:02:39,600
Crazy Mode

57
00:02:39,600 --> 00:02:40,400
有点意思

58
00:02:40,400 --> 00:02:41,760
这个平衡杆

59
00:02:43,840 --> 00:02:46,720
看看这个就是 PyTorch 的编译的流程

60
00:02:46,720 --> 00:02:47,120
先不管

61
00:02:47,120 --> 00:02:48,320
后面详细展开

62
00:02:49,280 --> 00:02:50,960
有 4 个重要的特性

63
00:02:50,960 --> 00:02:52,000
有 Torch Grid

64
00:02:52,000 --> 00:02:53,040
Torch Inductor

65
00:02:53,840 --> 00:02:54,800
PrimTorch

66
00:02:54,800 --> 00:02:56,080
下面这个图

67
00:02:57,200 --> 00:02:58,560
下面这个图帅

68
00:02:58,560 --> 00:02:59,600
下面这个图帅气

69
00:03:01,040 --> 00:03:03,520
把所有的算子都做了一个归约 分类

70
00:03:04,480 --> 00:03:05,120
不错

71
00:03:05,120 --> 00:03:07,040
变成了 250 多个算子

72
00:03:07,040 --> 00:03:09,600
然后收编到 ATen 里面 700 多个

73
00:03:11,040 --> 00:03:12,480
对于三方芯片厂商来说

74
00:03:12,480 --> 00:03:13,680
这个特性有点吸引

75
00:03:14,560 --> 00:03:16,000
现在打开 Notebook

76
00:03:16,000 --> 00:03:17,120
来去尝鲜一下

77
00:03:17,120 --> 00:03:18,080
这些新的特性

78
00:03:20,160 --> 00:03:22,080
现在来真正的录代码了

79
00:03:22,640 --> 00:03:23,840
首先 Import

80
00:03:23,840 --> 00:03:24,480
Time

81
00:03:24,480 --> 00:03:25,360
Import Torch

82
00:03:26,480 --> 00:03:29,520
Import Torch.Dynamo as Dynamo

83
00:03:29,520 --> 00:03:30,880
这个就是新的特性

84
00:03:33,920 --> 00:03:35,840
Import Torch Vision.Models

85
00:03:35,840 --> 00:03:38,000
用于后面直接把模型加载进来用

86
00:03:42,160 --> 00:03:42,960
Print Torch

87
00:03:42,960 --> 00:03:45,040
刚刚目前看看是用哪个版本

88
00:03:45,040 --> 00:03:45,680
1.14

89
00:03:45,680 --> 00:03:46,720
刚好是想要的

90
00:03:47,440 --> 00:03:48,240
Define

91
00:03:48,240 --> 00:03:50,000
建立一个简单的函数

92
00:03:55,120 --> 00:03:56,960
返回 A 加上 B

93
00:03:56,960 --> 00:04:00,160
现在做一个简单的测试实验

94
00:04:06,720 --> 00:04:08,880
首先定一个 Compiled Model

95
00:04:09,760 --> 00:04:11,520
等于 Torch Compiled

96
00:04:12,080 --> 00:04:12,640
-

97
00:04:12,640 --> 00:04:15,120
这个就是使用 PyTorch 最新的编译功能

98
00:04:15,680 --> 00:04:17,360
里面隐藏了一个编译器

99
00:04:17,360 --> 00:04:19,520
然后输出等于 Compiled Model

100
00:04:19,520 --> 00:04:22,080
然后把两个输入参数输进去

101
00:04:30,800 --> 00:04:33,280
然后算一下输出的时间

102
00:04:33,840 --> 00:04:34,960
接着打印出来

103
00:04:35,680 --> 00:04:38,160
End-Start

104
00:04:38,160 --> 00:04:38,640
可以看到

105
00:04:39,840 --> 00:04:40,960
好这么简单一个程序

106
00:04:40,960 --> 00:04:43,520
居然耗了我 1.59 秒

107
00:04:44,320 --> 00:04:45,520
有点长有点长

108
00:04:45,520 --> 00:04:47,200
接下来做另外一个实验去看看

109
00:04:48,400 --> 00:04:50,560
现在假设使用 Dynamo

110
00:04:50,560 --> 00:04:52,560
这个功能去做一个优化

111
00:04:52,560 --> 00:04:54,960
然后里面的后端使用 Inductor

112
00:04:56,160 --> 00:05:00,400
Inductor 在 CPU 上面使用是 OpenMP 去编译的

113
00:05:01,280 --> 00:05:02,560
0.02 秒

114
00:05:02,560 --> 00:05:03,120
有点快

115
00:05:03,520 --> 00:05:04,640
使用了 Dynamo

116
00:05:04,640 --> 00:05:06,160
然后指定了后端之后

117
00:05:06,160 --> 00:05:07,360
它编译快了好多

118
00:05:08,000 --> 00:05:09,520
再试一个实验

119
00:05:09,520 --> 00:05:10,960
假设我啥都不用

120
00:05:11,040 --> 00:05:13,600
直接用以前的动态图

121
00:05:13,600 --> 00:05:15,040
或者 Eagle 的模式

122
00:05:15,040 --> 00:05:16,800
直接使用 Pytorch 原生的动态图

123
00:05:16,800 --> 00:05:18,000
或者 Eagle 模式更快

124
00:05:18,000 --> 00:05:20,240
直接用了 0.0006 毫秒

125
00:05:21,200 --> 00:05:22,720
那我为啥要编译层呢

126
00:05:22,720 --> 00:05:24,320
我为啥要编译器呢

127
00:05:24,320 --> 00:05:25,200
编译器有啥用

128
00:05:25,200 --> 00:05:26,320
编译器不是说快了吗

129
00:05:26,320 --> 00:05:27,520
为啥编译器会慢的呢

130
00:05:28,160 --> 00:05:28,800
不着急

131
00:05:28,800 --> 00:05:29,840
后面会讲到

132
00:05:30,640 --> 00:05:33,440
现在定义一个 AlexNet

133
00:05:33,440 --> 00:05:34,640
还是定义 AlexNet

134
00:05:37,360 --> 00:05:39,040
然后定义优化器

135
00:05:41,760 --> 00:05:46,480
定义 AlexNet

136
00:05:51,840 --> 00:05:52,800
现在 compile more

137
00:05:53,360 --> 00:05:56,480
然后对 models 进行一个编译

138
00:05:56,480 --> 00:05:59,440
下面这个就是真正的去执行一下

139
00:05:59,440 --> 00:06:00,960
这个网络模型

140
00:06:00,960 --> 00:06:02,560
随机产生一个数据

141
00:06:02,560 --> 00:06:04,240
然后定一个反向

142
00:06:19,005 --> 00:06:20,675
做个正反向的推理发现

143
00:06:20,675 --> 00:06:23,900
用了 2.79 毫秒

144
00:06:26,865 --> 00:06:28,425
使用了 Pytorch 编译器之后

145
00:06:28,425 --> 00:06:29,900
用了 2.79 毫秒

146
00:06:29,900 --> 00:06:31,100
假设我现在不用

147
00:06:36,160 --> 00:06:37,600
不用我看看时间

148
00:06:37,600 --> 00:06:40,150
哇 不用时间只有 1.027 毫秒

149
00:06:40,150 --> 00:06:42,160
就是 我为啥还要用编译器

150
00:06:42,160 --> 00:06:43,740
这样慢了那么多

151
00:06:44,160 --> 00:06:45,280
这边原生挺好的

152
00:06:46,000 --> 00:06:46,960
再往下看

153
00:06:47,920 --> 00:06:49,040
多做几个实验

154
00:06:50,400 --> 00:06:52,240
现在刚才只是跑一个正向

155
00:06:52,240 --> 00:06:53,280
跑一个反向

156
00:06:53,280 --> 00:06:55,680
假设我现在多做几轮 epoch

157
00:06:57,360 --> 00:06:59,120
模拟在真实对网络模型

158
00:06:59,120 --> 00:07:00,720
进行训练的一个场景

159
00:07:04,880 --> 00:07:05,920
然后用 count

160
00:07:05,920 --> 00:07:08,640
然后去把每一轮迭代的时间算进来

161
00:07:09,200 --> 00:07:11,600
现在这个就是使用动态图

162
00:07:11,600 --> 00:07:12,640
或者一个模式的

163
00:07:12,640 --> 00:07:14,240
看看总的耗时

164
00:07:14,240 --> 00:07:15,680
或者每一次的耗时有多少

165
00:07:16,880 --> 00:07:17,760
finish training

166
00:07:25,280 --> 00:07:27,520
接着每一秒跑一次

167
00:07:28,160 --> 00:07:28,960
挺快的

168
00:07:28,960 --> 00:07:30,400
跑个 AlexNet 还是挺快的

169
00:07:30,400 --> 00:07:32,880
然后平均时间是 1.069 毫秒

170
00:07:33,840 --> 00:07:36,640
没有跑编译的时候是这个

171
00:07:36,720 --> 00:07:39,120
那跑跑编译的方式

172
00:07:39,120 --> 00:07:40,160
那 compile model

173
00:07:40,160 --> 00:07:41,600
然后看一下它的时间

174
00:07:49,280 --> 00:07:50,880
好像比上面快

175
00:07:50,880 --> 00:07:52,000
那看一下快多少

176
00:07:52,720 --> 00:07:53,600
1.024

177
00:07:54,160 --> 00:07:56,560
明显比刚才 1.069 快了

178
00:07:56,560 --> 00:07:58,720
那么 0.04 毫秒

179
00:07:59,360 --> 00:08:00,320
这有点意思

180
00:08:00,880 --> 00:08:02,320
在单一期执行的时候

181
00:08:02,320 --> 00:08:03,520
我有编译的开销

182
00:08:03,520 --> 00:08:05,440
所以单一期执行的时候会慢一点

183
00:08:05,520 --> 00:08:08,000
但是它敌不过我做一个训练的时候

184
00:08:08,000 --> 00:08:09,360
我是多轮迭代的

185
00:08:09,360 --> 00:08:10,720
其实我图已经编译好了

186
00:08:10,720 --> 00:08:13,600
我下次直接把静态的图拿出来去执行

187
00:08:13,600 --> 00:08:15,200
所以就快了很多

188
00:08:15,200 --> 00:08:17,760
那这只是用一个最简单的 AlexNet

189
00:08:17,760 --> 00:08:20,240
在 CPU 苹果上面去跑

190
00:08:20,240 --> 00:08:22,720
假设我在 GPU 上面跑会不会快很多

191
00:08:22,720 --> 00:08:25,440
刚才简单的去浏览了一次新的特性

192
00:08:25,440 --> 00:08:27,120
就把刚才那个网页看了一遍

193
00:08:27,120 --> 00:08:28,400
然后又装了一遍

194
00:08:28,400 --> 00:08:30,480
然后把新特性也用了一遍

195
00:08:30,480 --> 00:08:34,000
现在去看看它的一些新特性

196
00:08:34,080 --> 00:08:36,080
最后再做一个启发性的思考

197
00:08:36,080 --> 00:08:38,240
首先回顾一下未来的版本

198
00:08:38,240 --> 00:08:41,280
它说在 2.0 的时候会支持 Torch Combined

199
00:08:41,280 --> 00:08:42,960
with DDP 和 FSDP

200
00:08:42,960 --> 00:08:44,480
就是我有了静态图之后

201
00:08:44,480 --> 00:08:46,320
就更好的去做分布式了

202
00:08:46,320 --> 00:08:49,040
然后大部分的模型有 30%的提升

203
00:08:49,040 --> 00:08:51,920
怪不得刚才说性能确实是好了

204
00:08:51,920 --> 00:08:54,400
只是在一毫秒里面就快了那么一点点

205
00:08:54,400 --> 00:08:55,680
你感觉不出来

206
00:08:55,680 --> 00:08:59,840
那 2.S 里面就会有更 stable 的版本去推出

207
00:08:59,840 --> 00:09:01,120
那后面的版本先不管

208
00:09:01,120 --> 00:09:02,320
先看近的

209
00:09:02,400 --> 00:09:05,520
在之前认为 PyTorch 它是没有编译器的

210
00:09:05,520 --> 00:09:07,760
因为它大部分的时候你都用不到

211
00:09:07,760 --> 00:09:09,360
因为现在 PyTorch 2.0 之后

212
00:09:09,360 --> 00:09:12,640
明确的支持 Torch Dynamos 加 AOT AutoGrid

213
00:09:12,640 --> 00:09:16,320
那 AOT 就是 Ahead of Time 的一种编译的方式

214
00:09:16,320 --> 00:09:19,200
那在底层走的是 ATen 还有 Prim IR

215
00:09:19,200 --> 00:09:20,960
最后就给 Torch Inductor

216
00:09:20,960 --> 00:09:22,240
然后去执行的

217
00:09:22,240 --> 00:09:25,680
那 Torch Inductor 就是由 Triton 拿去提供的

218
00:09:26,880 --> 00:09:29,280
现在来看看它的模型泛化性

219
00:09:29,280 --> 00:09:31,280
这里面用了 TIMM,TorchBench

220
00:09:31,280 --> 00:09:34,320
还有 Huggingface 这三个套件去测

221
00:09:34,320 --> 00:09:37,280
那基本上大部分模型都有非常大的性能提升

222
00:09:37,280 --> 00:09:38,720
看看用了多少模型

223
00:09:38,720 --> 00:09:42,000
46 个加 61 个再加 56 个

224
00:09:42,000 --> 00:09:45,200
一共有 150 多个模型有性能的提升

225
00:09:45,200 --> 00:09:47,360
确实它做了很多的泛化性测试

226
00:09:47,360 --> 00:09:49,040
看起来还是很不错的

227
00:09:49,680 --> 00:09:52,800
那第一个重要的特性就是 Torch Dynamo

228
00:09:52,800 --> 00:09:55,280
Dynamo 就是发电机的意思

229
00:09:55,280 --> 00:09:57,920
主要是从 PyTorch 以前的静态图

230
00:09:57,920 --> 00:09:59,600
然后获取了一个图的概念

231
00:09:59,600 --> 00:10:01,280
然后进行一个执行的

232
00:10:01,280 --> 00:10:02,320
这里面说了很多

233
00:10:02,320 --> 00:10:04,240
主要是在 99%的时间

234
00:10:04,240 --> 00:10:05,200
它都是能工作的

235
00:10:05,200 --> 00:10:06,480
就工作的非常好

236
00:10:06,480 --> 00:10:07,760
在保持应用性的同时

237
00:10:07,920 --> 00:10:10,960
还提升了非常稳健的性能提升

238
00:10:11,520 --> 00:10:13,440
这个特性也是 2.0 里面

239
00:10:13,440 --> 00:10:15,040
主打最重要的一个特性

240
00:10:15,040 --> 00:10:17,120
所以放在第一 Dynamo

241
00:10:17,120 --> 00:10:19,120
那第二个就是 AOT

242
00:10:19,120 --> 00:10:20,800
Ahead of Time Autogrid

243
00:10:20,800 --> 00:10:23,120
刚才 Torch Dynamo 只是一个编译层

244
00:10:23,280 --> 00:10:25,200
但是实际上 Autogrid

245
00:10:25,200 --> 00:10:27,120
就是反向自动求导的时候怎么做

246
00:10:27,600 --> 00:10:29,360
它使用了一个 AOT 的

247
00:10:29,360 --> 00:10:31,760
一个对图的编译的方式

248
00:10:31,760 --> 00:10:33,600
里面就通过 Torch Dispatch

249
00:10:33,600 --> 00:10:35,280
去做一个分辨和追踪的

250
00:10:35,280 --> 00:10:36,960
这个特性也是非常有意思

251
00:10:36,960 --> 00:10:39,200
会在后面去详细展开

252
00:10:39,200 --> 00:10:41,200
它的算法原理

253
00:10:41,200 --> 00:10:43,920
那第三个就是 Torch Inductor

254
00:10:43,920 --> 00:10:44,960
最重要的一个特点

255
00:10:44,960 --> 00:10:47,200
就是 Define-by-run 的 IR

256
00:10:47,200 --> 00:10:48,720
提出了一个 Define by One IR

257
00:10:48,720 --> 00:10:51,040
然后可以很快的做一个 CoreGene

258
00:10:51,040 --> 00:10:54,160
里面主要是用了 OpenAI 提供的 Triton

259
00:10:54,240 --> 00:10:57,200
去生成 GPU 的一些代码

260
00:10:57,200 --> 00:11:01,360
在 CPU 上面用了 OpenMP 去生成代码

261
00:11:01,360 --> 00:11:02,560
那下面再看看

262
00:11:02,560 --> 00:11:04,160
有没有最后的一个特性

263
00:11:04,160 --> 00:11:05,360
最后一个特性是

264
00:11:05,360 --> 00:11:08,720
PrimTorch Stable Primitive Operator

265
00:11:08,720 --> 00:11:10,080
其实在 PrimTorch 里面

266
00:11:10,080 --> 00:11:11,520
有 1200 多个算子

267
00:11:11,520 --> 00:11:13,520
还有 2000 多个 API 接口

268
00:11:13,520 --> 00:11:14,720
对生成厂商来说

269
00:11:14,720 --> 00:11:15,760
要对接 PrimTorch 里面

270
00:11:15,760 --> 00:11:17,360
确实是很痛苦的

271
00:11:17,360 --> 00:11:19,600
而这里面 PrimOPS

272
00:11:19,600 --> 00:11:22,320
提供了 250 个最基础的

273
00:11:22,320 --> 00:11:23,655
或者最核心算子

274
00:11:23,655 --> 00:11:23,680
然后再往外层的 ATen ops

275
00:11:23,680 --> 00:11:25,815
然后再往外层的 ATen ops

276
00:11:25,840 --> 00:11:28,240
就提供了 750 多个算子

277
00:11:28,240 --> 00:11:30,160
这个特性对于生堂来说

278
00:11:30,160 --> 00:11:31,840
可能是最重要的

279
00:11:32,960 --> 00:11:34,240
到了最后一个环节

280
00:11:34,240 --> 00:11:36,480
就是启发和思考

281
00:11:36,480 --> 00:11:38,240
第一点就是 PrimTorch 2.0

282
00:11:38,240 --> 00:11:40,880
最大的改进或者最 outstanding 的一个

283
00:11:40,880 --> 00:11:43,280
就是引入了一个图编译的模式

284
00:11:43,280 --> 00:11:44,640
就是 Torch Compiler

285
00:11:44,640 --> 00:11:46,720
PrimTorch 正在走 OneFold

286
00:11:46,720 --> 00:11:47,440
MineSport

287
00:11:47,440 --> 00:11:49,200
还有 TensorFlow 的路

288
00:11:49,200 --> 00:11:51,200
就是走它的编译的模式

289
00:11:51,200 --> 00:11:52,400
PrimTorch 走着走着

290
00:11:52,400 --> 00:11:53,920
会不会让别人无路可走

291
00:11:54,240 --> 00:11:56,080
这是我最大的一个启发

292
00:11:56,080 --> 00:11:58,000
第二点就是 Torch Inductor

293
00:11:58,000 --> 00:11:59,200
引入了 TreeTime

294
00:11:59,200 --> 00:12:02,320
去支持用 Python 去写 CUDA 的代码

295
00:12:02,320 --> 00:12:04,240
这个特性到底好还是坏

296
00:12:04,720 --> 00:12:06,640
我觉得是由开发者去验证的

297
00:12:07,120 --> 00:12:09,120
第三点就是 PrimTorch

298
00:12:09,120 --> 00:12:10,480
把 2000 多个算子

299
00:12:10,480 --> 00:12:12,320
用 250 个算子进行收编

300
00:12:12,800 --> 00:12:15,040
对于像华为昇腾这样的厂商来说

301
00:12:15,520 --> 00:12:17,120
对接肯定是更加方便的

302
00:12:17,120 --> 00:12:19,520
可以聚焦到 250 个基础算子

303
00:12:19,520 --> 00:12:21,040
如果想要提升性能的话

304
00:12:21,040 --> 00:12:21,680
那很简单

305
00:12:21,920 --> 00:12:24,480
去对接 ATen 里面的一些算子

306
00:12:24,480 --> 00:12:25,520
做一些融合优化

307
00:12:27,120 --> 00:12:28,240
另外还有几点

308
00:12:28,960 --> 00:12:30,880
就是 PrimTorch 2.0 的官方

309
00:12:30,880 --> 00:12:32,240
就反复的去强调了

310
00:12:32,240 --> 00:12:33,440
我引用了 Tree 模式之后

311
00:12:33,440 --> 00:12:34,800
肯定完全相互兼容的

312
00:12:35,440 --> 00:12:36,800
TensorFlow 2.0 这个时候

313
00:12:36,800 --> 00:12:37,520
就跳出来说

314
00:12:38,080 --> 00:12:39,120
我可没惹到你

315
00:12:39,760 --> 00:12:40,560
你别乱说

316
00:12:41,280 --> 00:12:42,320
类似于 MineSport

317
00:12:42,320 --> 00:12:44,160
或者其他的 AI 框架来说

318
00:12:44,560 --> 00:12:45,920
互相的兼容

319
00:12:45,920 --> 00:12:47,360
或者 API 的兼容

320
00:12:47,360 --> 00:12:48,720
其实是很重要的

321
00:12:48,720 --> 00:12:50,480
而且 API 在设计之初

322
00:12:50,880 --> 00:12:53,120
就应该考虑很多问题

323
00:12:53,120 --> 00:12:55,120
第五点就是 PyTorch

324
00:12:55,120 --> 00:12:56,800
加入 Linux 基金会之后

325
00:12:57,040 --> 00:12:58,800
更加拥抱开源了

326
00:12:58,800 --> 00:13:00,880
从引入 OpenAI 的 Triton

327
00:13:00,880 --> 00:13:02,240
互相兼容 API

328
00:13:02,240 --> 00:13:03,120
新的特性

329
00:13:03,120 --> 00:13:04,800
充分考虑模型的泛化性

330
00:13:04,800 --> 00:13:06,160
还有缩编算子

331
00:13:06,160 --> 00:13:07,680
它越来越开源了

332
00:13:07,680 --> 00:13:09,440
就引出了第 6 个问题

333
00:13:09,440 --> 00:13:11,680
还有必要开发自己的 AI 框架吗

334
00:13:12,240 --> 00:13:15,280
这个就是对于思考和启发

335
00:13:15,280 --> 00:13:16,960
也希望引起大家的一个

336
00:13:16,960 --> 00:13:18,400
更广泛的讨论

337
00:13:19,360 --> 00:13:20,000
好了

338
00:13:20,000 --> 00:13:20,720
谢谢各位

