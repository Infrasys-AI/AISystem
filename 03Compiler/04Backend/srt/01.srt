1
00:00:00,117 --> 00:00:05,498
【字幕生成: 奔崩 字幕校对: 奔崩】

2
00:00:05,560 --> 00:00:06,440
哈喽大家好

3
00:00:06,440 --> 00:00:09,120
我是那个身体倍儿棒的 ZOMI

4
00:00:09,400 --> 00:00:10,800
可能讲完这一话之后

5
00:00:10,920 --> 00:00:12,160
我已经病倒了

6
00:00:12,840 --> 00:00:14,800
今天来到一个新的内容

7
00:00:15,800 --> 00:00:17,120
这话招人打

8
00:00:17,880 --> 00:00:18,560
新的内容

9
00:00:18,560 --> 00:00:20,880
AI 编译器的后端优化

10
00:00:21,080 --> 00:00:22,280
后端优化这个话题

11
00:00:22,280 --> 00:00:23,320
还是很有意思的

12
00:00:23,320 --> 00:00:24,960
今天要给大家汇报一下

13
00:00:24,960 --> 00:00:26,920
接下来要讲哪些内容

14
00:00:26,920 --> 00:00:29,160
首先今天的主要的内容

15
00:00:29,160 --> 00:00:31,200
就是去看看整个后端优化的

16
00:00:31,200 --> 00:00:32,120
整体的概念

17
00:00:32,120 --> 00:00:33,480
什么是后端优化

18
00:00:33,480 --> 00:00:35,880
后端优化会做些什么东西

19
00:00:35,880 --> 00:00:37,400
接着去看看

20
00:00:37,400 --> 00:00:39,280
算子的执行和调度

21
00:00:39,280 --> 00:00:42,720
其实后端优化主要的优化的内容

22
00:00:42,720 --> 00:00:44,320
就是算子的执行

23
00:00:44,320 --> 00:00:45,760
和它的调度方式

24
00:00:46,040 --> 00:00:46,880
执行和调度

25
00:00:47,080 --> 00:00:48,920
最重要的就是调度这一块

26
00:00:48,920 --> 00:00:50,600
怎么在不同的硬件上面调度

27
00:00:50,600 --> 00:00:52,200
于是就会去展开一个

28
00:00:52,200 --> 00:00:53,200
很重要的概念

29
00:00:53,720 --> 00:00:56,040
这是算子的循环优化

30
00:00:56,040 --> 00:00:57,720
在算子的循环优化

31
00:00:57,720 --> 00:00:59,000
具体的实现过程

32
00:00:59,240 --> 00:01:01,600
其实现在业界是有两条路径的

33
00:01:01,600 --> 00:01:03,600
一条是走 Auto-tuning 的路径

34
00:01:03,600 --> 00:01:05,748
一条是走 Polyhedral

35
00:01:05,748 --> 00:01:06,840
多面体的技术

36
00:01:06,840 --> 00:01:08,520
将会分开两个点

37
00:01:08,520 --> 00:01:10,000
去给大家汇报的

38
00:01:11,400 --> 00:01:12,840
现在正式的进入

39
00:01:12,840 --> 00:01:14,160
这个内容里面

40
00:01:14,160 --> 00:01:15,240
首先看一下

41
00:01:15,240 --> 00:01:17,160
整个 AI 编译器的架构图

42
00:01:17,160 --> 00:01:18,240
从上往下

43
00:01:18,240 --> 00:01:20,480
首先用 Python 写了一些

44
00:01:21,080 --> 00:01:22,360
神经网络的代码

45
00:01:22,480 --> 00:01:23,960
这些代码是调用

46
00:01:23,960 --> 00:01:25,440
AI 框架的 API 的

47
00:01:25,680 --> 00:01:27,160
AI 框架就会帮

48
00:01:27,160 --> 00:01:28,560
把一些写的 Python 的代码

49
00:01:28,600 --> 00:01:29,880
转成 Graph IR

50
00:01:29,880 --> 00:01:30,840
就是计算图

51
00:01:30,840 --> 00:01:32,800
接着会做一些图算的融合

52
00:01:32,800 --> 00:01:35,680
内存排布、内存优化等不同的优化的 Path

53
00:01:35,840 --> 00:01:36,720
这些优化的 Path

54
00:01:36,880 --> 00:01:39,280
都是基于计算图去实现的

55
00:01:39,280 --> 00:01:40,760
接着再往下

56
00:01:40,760 --> 00:01:43,640
就是需要把高层的 IR Graph IR

57
00:01:43,640 --> 00:01:44,400
计算图 IR

58
00:01:44,400 --> 00:01:46,120
转成底层 IR

59
00:01:46,120 --> 00:01:47,040
Tensor 的 IR

60
00:01:47,040 --> 00:01:48,680
或者叫 OPS IR

61
00:01:48,680 --> 00:01:52,320
针对算子 OPS 去进行一个优化

62
00:01:52,560 --> 00:01:53,600
这里面的优化方式

63
00:01:53,760 --> 00:01:55,080
有循环优化、张量融合

64
00:01:55,080 --> 00:01:56,880
还有存储 Tiling、还有张量化

65
00:01:57,000 --> 00:01:58,720
将会在第三节内容里面

66
00:01:58,720 --> 00:01:59,960
详细的展开

67
00:02:01,080 --> 00:02:03,040
进入到 OPS Optimizer

68
00:02:03,040 --> 00:02:04,560
这一个就是正式的

69
00:02:04,560 --> 00:02:06,800
进入到后端优化的过程当中

70
00:02:07,600 --> 00:02:09,000
后面的 Backend 后端

71
00:02:09,120 --> 00:02:11,360
更多的是跟 Core Gemm 相关

72
00:02:11,360 --> 00:02:12,240
这一块的内容

73
00:02:12,440 --> 00:02:14,360
跟传统的编译器更加贴近

74
00:02:14,480 --> 00:02:16,680
在这个时候 其实不会展开太多

75
00:02:17,040 --> 00:02:19,040
主要是围绕着红色的这一框

76
00:02:19,360 --> 00:02:21,080
中间的 OPS Optimizer

77
00:02:21,080 --> 00:02:22,440
这一层去展开

78
00:02:24,520 --> 00:02:26,640
下面重点的去看看

79
00:02:26,640 --> 00:02:27,760
Graph Optimizer

80
00:02:27,760 --> 00:02:29,160
跟 OPS Optimizer

81
00:02:29,160 --> 00:02:31,200
一个主要的区别

82
00:02:31,560 --> 00:02:32,600
前端的优化

83
00:02:32,760 --> 00:02:34,280
就是 Graph Optimizer

84
00:02:34,280 --> 00:02:36,040
输入是一个计算图

85
00:02:36,040 --> 00:02:38,520
这个时候 更关注于整个计算图的

86
00:02:38,520 --> 00:02:39,560
拓扑的优化

87
00:02:39,560 --> 00:02:42,160
而不是关心某个具体算子的实现

88
00:02:42,160 --> 00:02:44,000
从下面这个图 也可以看到

89
00:02:44,120 --> 00:02:45,960
左边的 这个是计算图

90
00:02:45,960 --> 00:02:47,640
进行了一些优化

91
00:02:47,640 --> 00:02:49,520
不断的 很多的优化的 Path

92
00:02:49,520 --> 00:02:50,880
最终把左边

93
00:02:51,200 --> 00:02:52,600
有 6 个算子的计算图

94
00:02:52,920 --> 00:02:55,800
变成只有 3 个算子的计算图

95
00:02:56,400 --> 00:02:57,560
这里面可以看出

96
00:02:57,840 --> 00:02:59,520
在 AI 编辑器的前端优化

97
00:02:59,840 --> 00:03:01,760
主要是对算子的节点

98
00:03:01,760 --> 00:03:03,880
进行一些消除、融合、化简

99
00:03:04,160 --> 00:03:06,560
使得计算图整个的计算

100
00:03:06,560 --> 00:03:08,920
还有存储的开销 做到最小

101
00:03:08,920 --> 00:03:10,640
这个就是前端优化的目的

102
00:03:11,120 --> 00:03:12,120
下面来看看

103
00:03:12,120 --> 00:03:14,120
后端优化做了哪些工作

104
00:03:14,400 --> 00:03:15,360
而后端优化

105
00:03:15,520 --> 00:03:17,000
主要是关注于具体

106
00:03:17,000 --> 00:03:18,200
某个算子的

107
00:03:18,480 --> 00:03:19,720
具体内部实现

108
00:03:20,000 --> 00:03:21,520
这里面看看这个图

109
00:03:21,800 --> 00:03:23,880
左边是一个简单的神级网络

110
00:03:24,000 --> 00:03:26,080
针对这里面一个卷积的算子

111
00:03:26,400 --> 00:03:28,320
卷积算子 CPU 的实现

112
00:03:28,760 --> 00:03:30,080
代码就像右边

113
00:03:30,440 --> 00:03:31,400
演示的一个代码

114
00:03:31,720 --> 00:03:33,600
具体希望能够实现

115
00:03:33,880 --> 00:03:34,920
达到性能的最优

116
00:03:35,200 --> 00:03:36,120
但可以看到

117
00:03:36,120 --> 00:03:37,960
其实它的嵌套非常的深

118
00:03:38,320 --> 00:03:40,760
这种嵌套或者这种方式去实现

119
00:03:41,080 --> 00:03:42,720
它性能绝对不是最优的

120
00:03:43,040 --> 00:03:44,520
所以去实现的过程当中

121
00:03:44,800 --> 00:03:47,200
就有非常多的优化的方式

122
00:03:47,480 --> 00:03:48,880
可以对输入输出

123
00:03:49,120 --> 00:03:50,560
还有内部的循环的方式

124
00:03:50,840 --> 00:03:52,120
还有内存的访问

125
00:03:52,960 --> 00:03:54,840
进行各种各样的优化

126
00:03:55,240 --> 00:03:57,840
这两点就是后端优化和前端优化

127
00:03:58,200 --> 00:03:59,560
最大的区别

128
00:04:01,320 --> 00:04:04,160
现在回到 AI 编辑器的后端优化

129
00:04:04,160 --> 00:04:05,440
去看一下后端优化的

130
00:04:05,760 --> 00:04:06,920
主要有哪几个步骤

131
00:04:07,280 --> 00:04:09,560
首先我把这里面分为三层

132
00:04:09,880 --> 00:04:11,640
第一层会把计算图

133
00:04:11,640 --> 00:04:12,760
就是高级 IR

134
00:04:13,080 --> 00:04:14,800
转换成为 Tensor IR

135
00:04:15,120 --> 00:04:16,280
或者 OPS IR

136
00:04:16,840 --> 00:04:19,240
把高级的 IR 变成一个低级的 IR

137
00:04:19,240 --> 00:04:19,880
这是第一步

138
00:04:20,240 --> 00:04:21,640
第二步就是正式的

139
00:04:21,640 --> 00:04:24,320
来到后端的算子的优化

140
00:04:24,320 --> 00:04:26,280
或者直接叫后端优化就完了

141
00:04:26,720 --> 00:04:27,760
接着后端优化

142
00:04:27,760 --> 00:04:30,160
会把它变成一个更 lower 的 IR

143
00:04:30,160 --> 00:04:31,200
就更低级的 IR

144
00:04:31,480 --> 00:04:33,600
然后去做一些代码的生成

145
00:04:33,760 --> 00:04:35,400
这里面就叫 Core Gemm

146
00:04:36,720 --> 00:04:38,080
生成完具体的代码

147
00:04:38,080 --> 00:04:40,400
就会在硬件上面去执行

148
00:04:41,440 --> 00:04:43,240
下面详细去看一看

149
00:04:44,760 --> 00:04:47,600
首先第一步就是生成低级的 IR

150
00:04:47,960 --> 00:04:50,120
其实针对某一个特定的算子

151
00:04:50,120 --> 00:04:51,400
例如 加 这个算子

152
00:04:51,600 --> 00:04:53,000
它不同的编译器

153
00:04:53,320 --> 00:04:54,960
不同的定义是不同的

154
00:04:55,200 --> 00:04:56,600
但是对于 ADD 这个算子

155
00:04:57,040 --> 00:04:59,800
它的算法的原理其实都是相同的

156
00:05:00,080 --> 00:05:01,680
所以针对每一个具体的算子

157
00:05:01,840 --> 00:05:04,040
需要用 AI 编译器的底层的接口

158
00:05:04,320 --> 00:05:06,080
去来定义这个算法

159
00:05:06,360 --> 00:05:07,760
这里面 relay.add

160
00:05:07,760 --> 00:05:09,760
就是 TVM IR 的一个定义

161
00:05:10,040 --> 00:05:11,680
最后再由 TVM 的编译器

162
00:05:11,680 --> 00:05:15,360
来生成底层内部的一些更加低级的 IR

163
00:05:15,600 --> 00:05:17,000
接下来看看第二步

164
00:05:17,000 --> 00:05:18,480
就是后端的优化

165
00:05:18,680 --> 00:05:19,560
后端的优化

166
00:05:19,560 --> 00:05:20,532
更多的是针对

167
00:05:20,532 --> 00:05:22,800
一些硬件的微架构或者架构

168
00:05:23,040 --> 00:05:25,280
不同的算法的实现方式不同

169
00:05:25,560 --> 00:05:27,480
例如举一个简单的例子

170
00:05:27,720 --> 00:05:29,320
我对卷积这个算子

171
00:05:29,560 --> 00:05:31,160
在某个特定的硬件架构

172
00:05:31,160 --> 00:05:32,360
例如华为达芬奇

173
00:05:32,600 --> 00:05:34,640
找到一个最优的实现方式能不能呢

174
00:05:34,960 --> 00:05:35,280
可以

175
00:05:35,680 --> 00:05:37,320
但是值得注意的一点

176
00:05:37,320 --> 00:05:38,680
就是一个算子

177
00:05:38,680 --> 00:05:40,840
Conv1×1、Conv3×3

178
00:05:41,120 --> 00:05:43,040
Conv5×5 包括 Conv7×7

179
00:05:43,400 --> 00:05:45,720
它的性能都是不一样的

180
00:05:46,120 --> 00:05:48,160
它的实现方式也是不一样的

181
00:05:48,480 --> 00:05:51,000
所以都会有不同的循环的优化方法

182
00:05:51,440 --> 00:05:53,920
这里面的优化方法就非常讲究了

183
00:05:53,920 --> 00:05:56,200
有人工实现的、有人工加经验

184
00:05:56,200 --> 00:05:57,236
也有 AutoTuning

185
00:05:57,236 --> 00:05:58,640
自动去实现的这种方式

186
00:05:58,920 --> 00:06:00,120
三种不同的路径

187
00:06:00,840 --> 00:06:03,360
最后看看代码生成

188
00:06:03,760 --> 00:06:06,440
代码生成 会把刚才讲到的

189
00:06:07,200 --> 00:06:08,720
OPS IR 或者 Tensor IR

190
00:06:09,160 --> 00:06:11,640
再转换成为一个更低级的 IR

191
00:06:11,640 --> 00:06:13,640
然后通过代码生成

192
00:06:13,920 --> 00:06:16,560
转换成为机器能够执行的一些指令

193
00:06:17,000 --> 00:06:19,480
这个其实不是 AI 编译器的核心内容

194
00:06:19,800 --> 00:06:22,560
例如怎么把底层的 IR 转换成为 LLVM

195
00:06:22,920 --> 00:06:25,280
或者 NVCC 英伟达推出的编译器

196
00:06:25,960 --> 00:06:26,880
AI 火起来之后

197
00:06:27,000 --> 00:06:29,840
这些古老的技术也是随着火起来了

198
00:06:31,759 --> 00:06:34,040
（战术变声）
ZOMI 老师 我有一个问题

199
00:06:34,520 --> 00:06:36,200
为什么 AI 的后端优化

200
00:06:36,200 --> 00:06:38,480
不直接用传统的通用的编译器

201
00:06:39,120 --> 00:06:40,600
就好像 GCC LLVM

202
00:06:40,960 --> 00:06:43,320
去替换掉后端的优化可以吗

203
00:06:43,320 --> 00:06:46,440
这个问题问得非常好

204
00:06:46,600 --> 00:06:49,440
其实还真的是不太行

205
00:06:49,720 --> 00:06:51,000
来看看两个原因

206
00:06:51,480 --> 00:06:53,920
深度学习里面主要的数据

207
00:06:53,920 --> 00:06:55,080
大部分都是张量

208
00:06:55,280 --> 00:06:57,000
当然不能说标量和向量没有

209
00:06:57,280 --> 00:06:58,280
但大部分是张量

210
00:06:58,600 --> 00:06:59,480
而传统的编译器

211
00:06:59,640 --> 00:07:01,520
其实是不擅长对张量进行计算的

212
00:07:02,040 --> 00:07:03,760
更多的是对标量进行计算

213
00:07:04,040 --> 00:07:04,800
包括对向量

214
00:07:05,000 --> 00:07:06,640
可能说还不是说非常擅长

215
00:07:07,560 --> 00:07:09,360
第二个就是通用的编译器

216
00:07:09,360 --> 00:07:11,440
主要是针对通用的编程语言

217
00:07:11,680 --> 00:07:13,560
例如 C++ Python Java

218
00:07:13,560 --> 00:07:15,000
这些通用的编程语言

219
00:07:15,320 --> 00:07:17,400
而很少缺乏领域特定的

220
00:07:17,400 --> 00:07:19,880
就 domain specific language（DSL）的支持

221
00:07:20,160 --> 00:07:22,120
特别是针对神经网络

222
00:07:22,400 --> 00:07:24,040
还有相关的神经网络的优化

223
00:07:24,400 --> 00:07:26,440
所以会说 AI 编译器的后端

224
00:07:27,000 --> 00:07:29,320
不适合直接用传统的通用编译器

225
00:07:30,760 --> 00:07:32,440
下面来看看具体的一些

226
00:07:32,440 --> 00:07:33,520
算子的问题

227
00:07:33,520 --> 00:07:34,360
算子很重要

228
00:07:34,920 --> 00:07:36,600
算子的分类主要有两种

229
00:07:36,600 --> 00:07:39,000
一种是访存密集型

230
00:07:39,440 --> 00:07:40,880
可以看看下面这个图

231
00:07:41,000 --> 00:07:42,080
有一个向量 A

232
00:07:42,080 --> 00:07:42,920
有个向量 B

233
00:07:42,920 --> 00:07:44,240
那向量 A 和向量 B

234
00:07:44,600 --> 00:07:46,520
都是从内存里面取出来的

235
00:07:46,920 --> 00:07:48,960
执行了一个 ALU 的乘法之后

236
00:07:49,411 --> 00:07:50,800
再放入 cache 里面

237
00:07:50,800 --> 00:07:51,920
或者内存里面

238
00:07:52,200 --> 00:07:54,000
那接着再从内存里面读出来

239
00:07:54,240 --> 00:07:55,040
再读下一次

240
00:07:55,400 --> 00:07:56,800
这种计算是大量的

241
00:07:56,800 --> 00:07:58,240
对内存进行访问

242
00:07:58,240 --> 00:08:00,400
所以叫做访存密集型

243
00:08:00,680 --> 00:08:02,400
特别是在神经网络里面

244
00:08:02,800 --> 00:08:04,640
大部分针对 RNN

245
00:08:05,200 --> 00:08:06,320
循环神经网络

246
00:08:06,640 --> 00:08:08,800
它的计算密集度是比较低的

247
00:08:08,800 --> 00:08:12,880
那第二种分类就是计算密集型

248
00:08:12,880 --> 00:08:16,080
计算密集型 其实我觉得是非常好理解的

249
00:08:16,320 --> 00:08:17,840
我需要大量的计算

250
00:08:17,840 --> 00:08:19,560
不断的去算数的

251
00:08:20,000 --> 00:08:21,360
但是有一点值得注意的

252
00:08:21,360 --> 00:08:23,400
就是看看下面的第一个表

253
00:08:23,400 --> 00:08:25,880
第一个表前面是卷积的参数

254
00:08:25,880 --> 00:08:27,840
右边是它对应的计算量

255
00:08:28,229 --> 00:08:29,920
不同卷积的入参

256
00:08:29,920 --> 00:08:31,800
计算量是不同的

257
00:08:31,800 --> 00:08:33,760
里面的访存数也是不同的

258
00:08:33,760 --> 00:08:35,600
而可以看到这里面的计算量

259
00:08:35,880 --> 00:08:39,360
比访存量是大了非常的多

260
00:08:39,360 --> 00:08:42,760
那这种形态叫做计算密集型

261
00:08:45,120 --> 00:08:48,120
针对访存密集型和计算密集型的算子

262
00:08:48,120 --> 00:08:49,800
对它进行优化的时候

263
00:08:49,800 --> 00:08:52,040
其实还是非常多的挑战的

264
00:08:52,040 --> 00:08:53,640
现在来简单的看看

265
00:08:54,560 --> 00:08:58,520
第一个就是优化的手段是非常多样性的

266
00:08:58,840 --> 00:09:00,000
在不同的情况下

267
00:09:00,000 --> 00:09:02,240
就是刚才的算子它有不同的入参

268
00:09:02,240 --> 00:09:03,840
对应有非常多的参数

269
00:09:03,840 --> 00:09:05,200
对于专家来说

270
00:09:05,400 --> 00:09:08,760
其实是非常难去穷举所有的优化方式

271
00:09:09,880 --> 00:09:12,120
第二种就是通用性和移植性

272
00:09:12,120 --> 00:09:14,560
可以看到不同的硬件架构

273
00:09:14,800 --> 00:09:17,120
算子的优化方式也是不一样的

274
00:09:17,120 --> 00:09:19,120
虽然算法的原理是相同

275
00:09:19,120 --> 00:09:20,800
但是硬件的指令不同

276
00:09:20,800 --> 00:09:22,920
硬件的内存的排布方式不同

277
00:09:22,920 --> 00:09:25,840
访问的性能也会有差异

278
00:09:26,120 --> 00:09:29,320
第三个就是优化之间会相互影响

279
00:09:29,840 --> 00:09:32,400
访问和存储是天平的两端

280
00:09:32,520 --> 00:09:35,520
很难去找到一个非常好的组合优化

281
00:09:35,520 --> 00:09:36,840
互相不影响的

282
00:09:37,280 --> 00:09:39,840
所以算子是有很多的挑战

283
00:09:39,840 --> 00:09:41,120
而面对这些挑战

284
00:09:41,280 --> 00:09:43,520
其实的目的非常明确

285
00:09:43,520 --> 00:09:47,080
就是针对访存密集型和计算密集型的算子

286
00:09:47,320 --> 00:09:48,480
进行优化

287
00:09:49,760 --> 00:09:52,080
但很难的就是对一个确定的算子

288
00:09:52,200 --> 00:09:53,040
例如卷积

289
00:09:53,040 --> 00:09:55,040
要正确实现它的逻辑

290
00:09:55,360 --> 00:09:55,880
很简单

291
00:09:55,880 --> 00:09:57,400
我在 CPU 上面也可以实现

292
00:09:57,720 --> 00:09:59,760
但是我要结合具体的硬件

293
00:09:59,760 --> 00:10:01,760
达到它的高性能就比较难的

294
00:10:02,080 --> 00:10:06,000
如果要对这个算子穷举了它所有的情况

295
00:10:06,000 --> 00:10:07,520
而达到极致的性能

296
00:10:07,760 --> 00:10:09,080
更是难上加难的

297
00:10:09,440 --> 00:10:11,440
现在业界比较通用的一个方式

298
00:10:11,560 --> 00:10:13,520
就是预置一些算子库

299
00:10:14,000 --> 00:10:16,040
这些算子库里面最出名的

300
00:10:16,040 --> 00:10:18,400
就是经常听到的英伟达推出的

301
00:10:18,400 --> 00:10:19,120
CuDNN

302
00:10:19,440 --> 00:10:20,720
我在使用 CuDNN 9 的时候

303
00:10:20,840 --> 00:10:22,640
其实只有 120 多个算子

304
00:10:22,960 --> 00:10:25,480
现在 CuDNN 10 11 12 都出来了

305
00:10:25,800 --> 00:10:27,240
接近有 200 多个算子

306
00:10:27,680 --> 00:10:30,920
后来还有 CuDNN CuBLAS OpenBLAS Eigen

307
00:10:30,920 --> 00:10:32,600
这些优秀的算子库

308
00:10:32,840 --> 00:10:34,200
去提供给大家实现的

309
00:10:34,560 --> 00:10:35,840
现在看看这个图

310
00:10:35,840 --> 00:10:37,640
这个就是 CuDNN 7.6

311
00:10:37,920 --> 00:10:38,800
CuDNN 8.1

312
00:10:39,080 --> 00:10:41,600
CuDNN 是人工去实现的一些算子

313
00:10:41,600 --> 00:10:43,000
把它变成一个算子加速库

314
00:10:43,320 --> 00:10:45,120
可以看到针对不同的网络模型

315
00:10:45,480 --> 00:10:47,360
都是有非常好的性能的收益

316
00:10:49,560 --> 00:10:50,960
但是又但是了

317
00:10:50,960 --> 00:10:52,200
这里面问题又来了

318
00:10:52,560 --> 00:10:53,760
看看有几个问题

319
00:10:53,760 --> 00:10:56,840
就是面对 AI 领域算子的迭代更新

320
00:10:57,040 --> 00:10:57,640
这么快

321
00:10:58,080 --> 00:10:59,320
例如 BN 一开始

322
00:10:59,320 --> 00:11:01,600
其实只有一个 BN (Batch Normalization)

323
00:11:01,920 --> 00:11:03,880
后来又出现了 Layer Normalization

324
00:11:04,160 --> 00:11:05,640
又出现了 Group Normalization

325
00:11:06,000 --> 00:11:08,000
AI 这个领域发展的太快了

326
00:11:08,000 --> 00:11:10,280
算子的迭代速度也非常快

327
00:11:11,160 --> 00:11:13,760
怎么去应对快速增长的算子呢

328
00:11:14,000 --> 00:11:15,000
都是人工实现吗

329
00:11:15,320 --> 00:11:16,680
显然是不靠谱的

330
00:11:18,160 --> 00:11:19,080
第二个问题就是

331
00:11:19,080 --> 00:11:20,920
假设我现在有一个算子

332
00:11:21,240 --> 00:11:23,800
卷积这个算子在不同的平台移植之后

333
00:11:24,160 --> 00:11:25,600
我在 NPU 我在 GPU

334
00:11:25,600 --> 00:11:28,680
我在昇腾里面 跑的结果都是一致的

335
00:11:29,120 --> 00:11:30,040
我怎么去保证

336
00:11:31,520 --> 00:11:32,880
第三个问题就是

337
00:11:32,880 --> 00:11:36,000
怎么去面对算子组合爆炸的问题

338
00:11:36,240 --> 00:11:38,600
刚才说了卷积这个算子

339
00:11:38,600 --> 00:11:40,000
简单卷积一个算子

340
00:11:40,240 --> 00:11:41,760
它就有非常多的入参

341
00:11:42,040 --> 00:11:42,960
不同的入参

342
00:11:42,960 --> 00:11:45,320
它有不同的优化组合的方式

343
00:11:45,600 --> 00:11:47,840
而且可能会把几个小算子

344
00:11:48,120 --> 00:11:49,200
合成一个大算子

345
00:11:49,480 --> 00:11:51,520
像这种算子组合爆炸的问题

346
00:11:51,840 --> 00:11:52,640
怎么解决

347
00:11:53,640 --> 00:11:56,320
还靠刚才的算子的优化库吗

348
00:11:56,600 --> 00:11:58,000
显然是不行的了

349
00:11:58,520 --> 00:12:00,320
所以需要有 AI 编译器

350
00:12:00,680 --> 00:12:02,840
这个时候业界就提出了一个

351
00:12:02,840 --> 00:12:04,680
自动 Kernel 生成的方式

352
00:12:05,440 --> 00:12:06,680
到硬件的底下了

353
00:12:06,680 --> 00:12:07,960
就不叫算子了

354
00:12:07,960 --> 00:12:09,520
就叫做 Kernel 了

355
00:12:09,520 --> 00:12:10,800
自动 Kernel 的生成

356
00:12:11,880 --> 00:12:14,080
它的目的就是给定一定的算法

357
00:12:14,080 --> 00:12:15,400
就假设卷积

358
00:12:15,400 --> 00:12:17,120
它可能是个具体的算法实现

359
00:12:17,520 --> 00:12:19,200
针对不同的硬件平台

360
00:12:19,200 --> 00:12:21,200
去做一个高效的实现

361
00:12:22,080 --> 00:12:23,640
这种就是自动生成

362
00:12:23,920 --> 00:12:26,440
现在自动生成里面又分为两个

363
00:12:26,440 --> 00:12:28,600
上面是一些比较好的

364
00:12:28,600 --> 00:12:30,440
或者比较经典的一些文章

365
00:12:30,440 --> 00:12:31,560
也是这几年推出的

366
00:12:31,560 --> 00:12:33,320
我建议大家可以搜一搜

367
00:12:33,320 --> 00:12:35,880
现在业界主要分为两个方向

368
00:12:35,880 --> 00:12:37,080
第一个方向就是

369
00:12:37,080 --> 00:12:38,640
AI 编译器尝试使用

370
00:12:38,640 --> 00:12:40,560
融合机器学习的方法

371
00:12:40,560 --> 00:12:41,760
例如 Auto Tuning

372
00:12:42,280 --> 00:12:43,800
TVM 里面做的比较好的一点

373
00:12:43,800 --> 00:12:45,520
就是把机器学习这个方法

374
00:12:45,720 --> 00:12:47,320
引入到 AI 编译器里面

375
00:12:48,400 --> 00:12:51,040
第二种就是使用编译技术来解决

376
00:12:51,440 --> 00:12:53,040
这里面比较出名的就是

377
00:12:53,040 --> 00:12:54,600
Polyhedral 多面体技术

378
00:12:54,600 --> 00:12:56,960
去解决算子计算的加速

379
00:12:56,960 --> 00:12:58,680
自动生成算子

380
00:12:59,240 --> 00:12:59,720
好了

381
00:12:59,720 --> 00:13:01,600
今天的内容就这么多

382
00:13:02,120 --> 00:13:02,920
谢谢各位

383
00:13:03,240 --> 00:13:04,040
卷的不行了

384
00:13:04,040 --> 00:13:04,880
卷的不行了

385
00:13:04,880 --> 00:13:06,320
记得一键三连加关注

386
00:13:06,680 --> 00:13:08,080
所有的内容都会开源

387
00:13:08,080 --> 00:13:09,920
在下面这条链接里面

388
00:13:10,280 --> 00:13:11,200
拜了个拜