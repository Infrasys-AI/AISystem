1
00:00:00,000 --> 00:00:04,500
字幕生成：BLACK 字幕校对：凝渊

2
00:00:05,339 --> 00:00:06,360
哈喽大家好

3
00:00:06,360 --> 00:00:08,840
来到 AI 编译器系列里面的

4
00:00:08,840 --> 00:00:11,440
AI 编译器里面真正的第二个内容

5
00:00:11,440 --> 00:00:13,640
就是 AI 编译器的架构的发展

6
00:00:13,640 --> 00:00:17,600
会重点去讲讲 AI 编译器的一个具体的架构

7
00:00:17,600 --> 00:00:20,040
当然了在 AI 编译器里面

8
00:00:20,040 --> 00:00:22,040
它的架构并不是一成不变的

9
00:00:22,040 --> 00:00:23,680
而是随着时间的推移

10
00:00:23,680 --> 00:00:26,400
还有随着技术的发展而不断的演进的

11
00:00:26,400 --> 00:00:28,520
这里面分开三个阶段来看

12
00:00:28,520 --> 00:00:31,120
现在正式的进入内容

13
00:00:31,120 --> 00:00:32,800
什么是 AI 编译器

14
00:00:39,480 --> 00:00:41,320
其实我抛出的这个问题

15
00:00:41,320 --> 00:00:43,520
其实想跟大家一起去探讨的

16
00:00:43,520 --> 00:00:47,000
我认为 AI 编译器主要分为两个场景

17
00:00:47,000 --> 00:00:49,640
两个场景的编译器的内容其实是不太一样的

18
00:00:49,640 --> 00:00:51,000
第一个就是推理了

19
00:00:51,000 --> 00:00:54,600
现在大部分应用场景都在推理里面的

20
00:00:54,600 --> 00:00:57,200
它的输入是 AI 框架训练出来的

21
00:00:57,200 --> 00:00:59,840
模型文件或者模型权重

22
00:00:59,840 --> 00:01:03,560
输出是能够在不同硬件高效执行的程序

23
00:01:03,560 --> 00:01:06,520
编译器就是去链接文件

24
00:01:06,520 --> 00:01:08,880
还有硬件之间的一个关系

25
00:01:10,200 --> 00:01:13,040
第二个就是指训练场景

26
00:01:13,040 --> 00:01:15,080
训练场景跟推理场景不太一样

27
00:01:15,080 --> 00:01:19,360
训练场景输入的是高级语言表示的神经网络的代码

28
00:01:19,360 --> 00:01:20,920
这句话听上去很隐晦

29
00:01:20,920 --> 00:01:22,560
其实就是很简单

30
00:01:22,560 --> 00:01:26,120
我用 python 写的一些代码去表示神经网络

31
00:01:26,120 --> 00:01:28,280
然后我丢给 AI 框架去处理

32
00:01:28,280 --> 00:01:31,120
然后 AI 框架就能够在不同的硬件上面

33
00:01:31,120 --> 00:01:33,560
高效的去执行训练任务了

34
00:01:33,560 --> 00:01:36,720
这种情况就是训练场景的 AI 编译器

35
00:01:36,720 --> 00:01:39,160
去连接高级语言的代码

36
00:01:39,160 --> 00:01:41,000
然后在不同硬件上面去执行

37
00:01:41,000 --> 00:01:44,360
所以我觉得主要是分为两个场景去探讨的

38
00:01:45,360 --> 00:01:47,360
在正式进入具体的内容之前

39
00:01:47,480 --> 00:01:49,200
我想抛出三个问题

40
00:01:49,320 --> 00:01:52,640
跟大家一起去学习和汇报一下

41
00:01:52,640 --> 00:01:54,760
第一个就是什么是训练场景

42
00:01:54,760 --> 00:01:57,200
刚才讲了有训练场景有推理场景

43
00:01:57,200 --> 00:01:58,880
那它们之间有什么区别了

44
00:01:58,880 --> 00:02:00,680
什么是训练什么是推理

45
00:02:00,680 --> 00:02:03,400
如果不懂得 AI 的基础的同学

46
00:02:03,520 --> 00:02:07,560
可以翻看之前关于 AI 基础训练的一些内容

47
00:02:07,560 --> 00:02:10,520
也可以现在网上有非常多的相关的资料

48
00:02:10,520 --> 00:02:15,360
第二个就是去做 AI 编译器的人

49
00:02:15,360 --> 00:02:18,520
搞系统底层的人为什么要了解算法

50
00:02:18,760 --> 00:02:20,200
有没有必要了解算法

51
00:02:21,000 --> 00:02:24,000
第三个问题就是我是算子开发的人员

52
00:02:24,000 --> 00:02:27,015
我只是对我的 kernel 去做一些操作

53
00:02:27,015 --> 00:02:27,040
为什么我需要了解编译器呢

54
00:02:27,040 --> 00:02:29,495
为什么我需要了解编译器呢

55
00:02:29,880 --> 00:02:31,960
或者我有必要去了解编译器吗

56
00:02:32,400 --> 00:02:34,840
带着这三个问题继续往下看

57
00:02:37,480 --> 00:02:39,200
对于什么是 AI 编译器

58
00:02:39,360 --> 00:02:42,000
这里总结了几个关键的特性

59
00:02:42,000 --> 00:02:45,400
这也是参考金雪峰老师的一个思考

60
00:02:45,640 --> 00:02:47,480
下面来看看第一个特性

61
00:02:47,600 --> 00:02:50,920
就是 AI 编译器主要是以 python 为主的

62
00:02:50,920 --> 00:02:53,160
动态解析语言作为前端

63
00:02:53,520 --> 00:02:56,840
第二个就是会有多层的 IR 的设计

64
00:02:57,480 --> 00:03:01,360
多层的 IR 可能跟 LLVM 的那种单一的 IR 不太一样

65
00:03:01,360 --> 00:03:03,080
因为这里面包括图的编译

66
00:03:03,200 --> 00:03:04,520
就会有图的 IR

67
00:03:04,520 --> 00:03:06,280
需要对算子进行编译

68
00:03:06,280 --> 00:03:08,080
所以会出现算子的 IR

69
00:03:08,080 --> 00:03:10,560
所以这里面就已经有两层 IR 了

70
00:03:10,560 --> 00:03:11,880
最后还有代码生成

71
00:03:12,000 --> 00:03:14,320
代码生成又是另外一套 IR

72
00:03:14,320 --> 00:03:15,560
就 CodeGen 的那一套

73
00:03:16,160 --> 00:03:19,040
第三个特性主要是它是面向神经网络

74
00:03:19,040 --> 00:03:21,560
面向深度学习的特定的优化

75
00:03:21,560 --> 00:03:25,840
也就是 Domain Specific DS 面向特定领域的

76
00:03:26,120 --> 00:03:29,840
第四种就会去支持很多不同的芯片

77
00:03:30,200 --> 00:03:33,280
这个就是 AI 编译器的四个重要的特点

78
00:03:35,280 --> 00:03:38,040
下面这个图有没有点似曾相识的感觉

79
00:03:38,480 --> 00:03:41,720
其实这个图是取自于 MindSpore 总体架构师

80
00:03:41,720 --> 00:03:42,720
叫总架

81
00:03:42,720 --> 00:03:45,360
然后金雪峰老师在知乎上面的一篇文章

82
00:03:45,560 --> 00:03:47,680
然后我对它加以了一些划分

83
00:03:47,680 --> 00:03:48,840
还有进行了一些

84
00:03:49,720 --> 00:03:51,720
我个人觉得可以修改的小地方

85
00:03:52,000 --> 00:03:54,040
然后看看主要是分为三个阶段

86
00:03:54,040 --> 00:03:56,320
Stage1 就类似于 TensorFlow 的

87
00:03:56,320 --> 00:03:57,920
可能 TensorFlow 出现的比较早

88
00:03:57,920 --> 00:04:00,200
然后是第一个把 AI 编译器

89
00:04:00,200 --> 00:04:03,840
引入到 AI 框架里面的最早的一个框架了

90
00:04:04,120 --> 00:04:07,840
好像 Caffe 那种可能是游离在 Stage1 之外的

91
00:04:07,840 --> 00:04:09,840
或者在 Stage1 准备之前的

92
00:04:10,160 --> 00:04:12,320
后来现在到了 Stage2 之后

93
00:04:12,480 --> 00:04:16,680
可以发现有非常多不同的 AI 框架的涌现

94
00:04:16,680 --> 00:04:18,000
包括 PyTorch

95
00:04:18,000 --> 00:04:20,360
虽然 PyTorch 有点意思

96
00:04:20,360 --> 00:04:23,080
最近的出现的 PyTorch2.0 已经发布了

97
00:04:23,440 --> 00:04:24,760
然后到 Stage3

98
00:04:24,880 --> 00:04:26,880
Stage3 可能现在还没有达到

99
00:04:26,880 --> 00:04:28,640
而是在一个过渡的阶段

100
00:04:28,640 --> 00:04:30,200
那 Stage3 未来会怎么样

101
00:04:30,200 --> 00:04:32,120
做一些简单的畅想

102
00:04:32,240 --> 00:04:34,720
现在跟着这个图一起来去看一下

103
00:04:34,720 --> 00:04:37,520
Stage1,Stage2,Stage3 各有什么不同

104
00:04:38,280 --> 00:04:39,960
首先叫做 Stage1

105
00:04:40,000 --> 00:04:42,440
是一个朴素的 AI 编译器

106
00:04:42,840 --> 00:04:43,920
既然是朴素

107
00:04:43,920 --> 00:04:46,640
它存在于 TensorFlow 的一个早期的版本

108
00:04:46,640 --> 00:04:48,920
基于神经网络的编程模型

109
00:04:48,960 --> 00:04:51,200
进行了图、计算图

110
00:04:51,200 --> 00:04:53,520
还有算子两层 IR 的抽象

111
00:04:54,160 --> 00:04:56,520
第一层就是图层

112
00:04:56,760 --> 00:04:57,760
像 TensorFlow 早期

113
00:04:57,920 --> 00:05:00,520
它采用的是一个声明式的编程方式

114
00:05:01,280 --> 00:05:03,880
主要是以静态图的方式去执行

115
00:05:03,880 --> 00:05:04,840
然后在执行之前

116
00:05:04,920 --> 00:05:07,920
会做一些硬件无关和硬件相关的编译优化

117
00:05:08,120 --> 00:05:11,040
这里面的编译优化主要是针对图结构

118
00:05:11,040 --> 00:05:14,000
对神经网络做一些优化和融合

119
00:05:14,520 --> 00:05:15,400
可以看一下

120
00:05:15,400 --> 00:05:17,680
假设现在这里面有非常多的算子

121
00:05:17,960 --> 00:05:19,200
每一次我下发的时候

122
00:05:19,200 --> 00:05:21,320
每一次让硬件去执行的时候

123
00:05:21,600 --> 00:05:22,520
每执行一个算子

124
00:05:22,520 --> 00:05:23,960
它就有它的输出

125
00:05:23,960 --> 00:05:25,480
就要占用 IO

126
00:05:25,840 --> 00:05:28,000
能不能做一些算子的融合

127
00:05:28,240 --> 00:05:29,800
做一些提前内存的分配

128
00:05:30,320 --> 00:05:32,520
所以这个就是对图层的优化

129
00:05:33,040 --> 00:05:36,400
第二个就是算子层面的优化

130
00:05:36,800 --> 00:05:37,800
在早期的版本

131
00:05:38,000 --> 00:05:40,160
算子层其实没有编译的概念

132
00:05:40,160 --> 00:05:42,760
主要是通过手写 Kernel 的方式

133
00:05:42,760 --> 00:05:45,080
例如在英伟达的 GPU 上面

134
00:05:45,240 --> 00:05:47,320
可能会提供了一些 CUDA

135
00:05:47,320 --> 00:05:49,160
自己写的一些.cu 的算子

136
00:05:49,160 --> 00:05:51,440
或者依赖于 CuDNN 的算子库

137
00:05:52,520 --> 00:05:53,960
从这里面可以看到

138
00:05:53,960 --> 00:05:56,800
实际上最开始的朴素的 AI 编译器

139
00:05:56,960 --> 00:05:58,760
它的概念也是比较简单

140
00:05:58,760 --> 00:06:01,880
我就是对神经网络所表示的计算图

141
00:06:02,160 --> 00:06:04,520
做一些编译相关的优化

142
00:06:04,800 --> 00:06:05,920
这些编译相关的优化

143
00:06:06,040 --> 00:06:08,840
其实我在进入 MindSpore 的前几年

144
00:06:09,360 --> 00:06:10,960
也就是 18-19 年的时候

145
00:06:11,160 --> 00:06:12,920
其实是亲自去写了这些

146
00:06:12,920 --> 00:06:14,375
大家不要觉得编译器

147
00:06:14,375 --> 00:06:14,400
或者编译底层的 pass 很难写

148
00:06:14,400 --> 00:06:16,855
或者编译底层的 pass 很难写

149
00:06:16,880 --> 00:06:18,520
它其实就是一个硬规则

150
00:06:18,800 --> 00:06:19,880
我拿到一个图

151
00:06:19,880 --> 00:06:20,920
拿到一个节点

152
00:06:20,920 --> 00:06:22,640
然后去自己控制

153
00:06:22,640 --> 00:06:24,800
最难的工作就是去思考

154
00:06:24,800 --> 00:06:27,680
去抽象计算图的基本的 pattern

155
00:06:27,680 --> 00:06:29,640
去抽象计算图的模式

156
00:06:30,520 --> 00:06:31,200
刚才提到

157
00:06:31,200 --> 00:06:32,920
如果不了解声明式编程

158
00:06:32,920 --> 00:06:33,800
命令式编程

159
00:06:33,800 --> 00:06:35,040
静态图和动态图

160
00:06:35,160 --> 00:06:36,320
可以看一下

161
00:06:36,600 --> 00:06:39,440
AI 框架基础的第 4 节和第 5 节的内容

162
00:06:41,000 --> 00:06:41,600
右边这个图

163
00:06:41,840 --> 00:06:43,520
就是我简单的总结了一下

164
00:06:43,520 --> 00:06:45,440
关于朴素 AI 编译器的一个

165
00:06:45,800 --> 00:06:46,760
简单的架构

166
00:06:46,760 --> 00:06:49,480
前端可能会有一些 Python 的 API

167
00:06:49,680 --> 00:06:51,160
这里面以 TensorFlow 为主

168
00:06:51,160 --> 00:06:52,440
它主要是写了自己

169
00:06:52,440 --> 00:06:53,960
关于计算图的一些表示

170
00:06:54,240 --> 00:06:55,120
用户用的时候

171
00:06:55,280 --> 00:06:58,840
就需要学它这一套 Python 的 API 的前端

172
00:06:59,080 --> 00:07:00,320
这里面前端 Python 的 API

173
00:07:00,440 --> 00:07:02,880
只是借用了 Python 去表达神经网络

174
00:07:03,200 --> 00:07:04,040
但实际上的编译

175
00:07:04,240 --> 00:07:05,760
是用 TensorFlow 自己的一个

176
00:07:05,760 --> 00:07:07,040
计算图的编译层

177
00:07:07,320 --> 00:07:08,560
所以大家用户用起来

178
00:07:08,680 --> 00:07:09,800
就会觉得很奇怪

179
00:07:09,800 --> 00:07:12,480
我要去学 TensorFlow 的一个 Python 的解析

180
00:07:12,480 --> 00:07:14,040
跟平时写的 Python 代码

181
00:07:14,280 --> 00:07:15,080
好像不太一样

182
00:07:15,440 --> 00:07:17,560
在算子层就直接是到 Runtime

183
00:07:17,560 --> 00:07:18,800
然后去调用 CuDNN

184
00:07:18,800 --> 00:07:19,600
这些算子库

185
00:07:19,600 --> 00:07:22,120
最后执行在异构芯片上面

186
00:07:22,400 --> 00:07:25,080
这个就是最简单最朴素的 AI 编译器

187
00:07:26,600 --> 00:07:28,080
讲完朴素 AI 编译器之后

188
00:07:28,200 --> 00:07:30,120
看一下它具体遇到哪些问题

189
00:07:30,560 --> 00:07:31,880
第一个就是易用性

190
00:07:31,880 --> 00:07:34,440
易用性它在表达上是非 Python 原生的

191
00:07:34,440 --> 00:07:36,720
也就是它不是真正的 Python 的代码

192
00:07:36,720 --> 00:07:38,200
只是类 Python 的代码

193
00:07:38,560 --> 00:07:40,640
这时候开发者就需要利用

194
00:07:40,640 --> 00:07:43,880
TensorFlow 提供的 API 去显示的构图

195
00:07:44,240 --> 00:07:46,040
我必须要知道我构这个图

196
00:07:46,040 --> 00:07:47,440
我需要用哪些 API

197
00:07:47,440 --> 00:07:48,800
这个时候是很头痛的

198
00:07:48,800 --> 00:07:50,600
所以大家用 TensorFlow 学 TensorFlow

199
00:07:50,600 --> 00:07:52,640
觉得它难学就在于这一点

200
00:07:53,080 --> 00:07:55,120
第二点就是性能上

201
00:07:55,680 --> 00:07:57,960
越来越多的 AI 加速器的出现

202
00:07:57,960 --> 00:08:00,840
所以导致对性能的挑战很大

203
00:08:00,880 --> 00:08:03,160
可能在某些芯片上面跑得特别快

204
00:08:03,400 --> 00:08:05,880
在某些芯片上面跑得特别的慢

205
00:08:06,360 --> 00:08:08,600
第二个就是用的是 CuDNN

206
00:08:08,600 --> 00:08:10,480
或者自己手工写的一些算子

207
00:08:10,480 --> 00:08:12,280
而且走的是一个静态图

208
00:08:12,280 --> 00:08:14,280
所以算子的边界

209
00:08:14,280 --> 00:08:16,560
还有算子属性某些特定的情况

210
00:08:16,840 --> 00:08:18,320
是已经明确确定的

211
00:08:18,600 --> 00:08:20,600
例如我在 LSTM 这个算子里面

212
00:08:20,920 --> 00:08:23,880
发现我的 NLP 输入的序列非常长

213
00:08:24,200 --> 00:08:26,400
这种情况可能超出了我手写 Kernel

214
00:08:26,400 --> 00:08:29,240
或者提供的算子的一个边界

215
00:08:29,360 --> 00:08:31,720
这个时候我的执行就会变得非常慢

216
00:08:31,720 --> 00:08:34,000
甚至可能出现精度的问题

217
00:08:34,640 --> 00:08:38,200
第三点就是算子层它没有通过编译

218
00:08:38,200 --> 00:08:40,560
而是直接使用 CuDNN 的这种算子

219
00:08:40,560 --> 00:08:42,640
所以硬件厂商提供的优化库

220
00:08:43,200 --> 00:08:43,960
一定是最优的

221
00:08:44,200 --> 00:08:45,080
如果是最优的话

222
00:08:45,080 --> 00:08:47,320
就不会出现类似于 PyTorch

223
00:08:47,320 --> 00:08:48,880
类似于 PyTorch Atom 里面

224
00:08:48,880 --> 00:08:51,360
大量的 CUDA 手写的算子

225
00:08:52,200 --> 00:08:54,280
所以说硬件厂商提供的算子库

226
00:08:54,280 --> 00:08:55,320
未必是最优的

227
00:08:55,320 --> 00:08:58,440
但是它给提供了一个方便的前提

228
00:09:00,240 --> 00:09:04,040
接下来就遇到了一个专用的 AI 编译器

229
00:09:04,040 --> 00:09:05,240
那专用的 AI 编译器

230
00:09:05,400 --> 00:09:07,600
会以 PyTorch JAX

231
00:09:07,600 --> 00:09:09,000
还有 Mindspore 作为例子

232
00:09:09,320 --> 00:09:10,680
像 PyTorch 大家都觉得

233
00:09:10,680 --> 00:09:13,360
它没有一个计算图的概念

234
00:09:13,360 --> 00:09:15,200
它其实只是 PyTorch 的动态图

235
00:09:15,200 --> 00:09:16,600
它没有计算图的概念

236
00:09:16,600 --> 00:09:19,200
但是 PyTorch 后来又出现了

237
00:09:19,200 --> 00:09:20,400
PyTorch.fx

238
00:09:20,400 --> 00:09:21,880
PyTorch.git

239
00:09:21,880 --> 00:09:24,120
还有包括 PyTorch.dynamic

240
00:09:24,120 --> 00:09:25,840
包括现在的 PyTorch 2.0

241
00:09:26,040 --> 00:09:29,080
它其实已经出现了自己的一个 AI 编译器

242
00:09:29,160 --> 00:09:33,040
当然它没有一种说解决方案特别的完善

243
00:09:33,640 --> 00:09:35,640
如果大家有兴趣或者看的人比较多

244
00:09:35,640 --> 00:09:37,920
可以单独开一节去讲一讲

245
00:09:38,080 --> 00:09:41,200
PyTorch 的一个 PyTorch 2.0 的一些新特性

246
00:09:41,200 --> 00:09:43,480
最重要的是 PyTorch.compile

247
00:09:43,800 --> 00:09:46,080
这个新的重要的特点

248
00:09:46,760 --> 00:09:48,600
现在回过头来看看 stage2

249
00:09:48,600 --> 00:09:50,960
一个专用 AI 编译器有什么特点

250
00:09:50,960 --> 00:09:54,320
首先表达上它是类似于 PyTorch 的灵活表达

251
00:09:54,320 --> 00:09:55,680
就现在不管哪个框架

252
00:09:55,680 --> 00:09:57,440
基本上我觉得大部分的 API

253
00:09:57,880 --> 00:09:59,800
都是去参考 PyTorch 为主的

254
00:09:59,800 --> 00:10:02,440
因为 PyTorch 的应用性实在是太好了

255
00:10:02,640 --> 00:10:04,480
参考了 PyTorch API 之后

256
00:10:04,760 --> 00:10:07,440
图层的表达就是希望能够把类似于

257
00:10:07,440 --> 00:10:10,920
PyTorch 的表达转换成为图的 IR 进行优化

258
00:10:11,160 --> 00:10:14,360
第二点就是 AI 专用的一个编译器的架构

259
00:10:14,360 --> 00:10:18,480
就希望能够打开图和算子的边界进行融合优化

260
00:10:18,960 --> 00:10:22,320
详细的内容将会在后面给大家一起去展开的

261
00:10:22,680 --> 00:10:27,160
第二个重要的特点就是性能上面做一个新的突破

262
00:10:27,440 --> 00:10:30,760
可以看到性能上面其实有很多不同的尝试

263
00:10:30,760 --> 00:10:31,920
包括 TBM

264
00:10:31,920 --> 00:10:33,880
还有 Facebook 推出的 TC

265
00:10:33,880 --> 00:10:35,840
还有谷歌的 XLA

266
00:10:35,840 --> 00:10:39,320
这里面主要是希望能够打开计算图和算子的边界

267
00:10:39,320 --> 00:10:41,240
进行重新的组合优化

268
00:10:41,240 --> 00:10:43,640
极度的去发挥芯片的算力

269
00:10:43,640 --> 00:10:45,120
就简单的来说

270
00:10:45,120 --> 00:10:47,080
你不要再给我分开什么计算图

271
00:10:47,080 --> 00:10:47,960
什么算子

272
00:10:47,960 --> 00:10:50,360
我能不能把它看成一个事情

273
00:10:50,720 --> 00:10:54,160
我能不能尽量的把它变成一个统一的 IR

274
00:10:54,760 --> 00:10:57,920
把所有东西都变成最细粒度的子图

275
00:10:57,920 --> 00:10:58,960
然后进行边界优化

276
00:10:58,960 --> 00:10:59,840
包括 Buffer 融合

277
00:11:00,040 --> 00:11:00,720
水平融合

278
00:11:00,800 --> 00:11:01,920
还有垂直融合

279
00:11:02,600 --> 00:11:06,560
这里面最大的一个挑战就是这些算子怎么去打开

280
00:11:06,560 --> 00:11:08,200
这些图怎么去打开

281
00:11:08,200 --> 00:11:09,520
我有了小算子之后

282
00:11:09,520 --> 00:11:12,520
我怎么去进行各种情况的融合

283
00:11:12,520 --> 00:11:15,240
这个点是对性能上最大的一个挑战

284
00:11:15,680 --> 00:11:16,520
总结一下

285
00:11:16,520 --> 00:11:18,680
表达上以 PyTorch 作为标杆

286
00:11:18,680 --> 00:11:21,480
然后进行一个图层或者算子层的 IR

287
00:11:21,480 --> 00:11:22,640
第二层性能上

288
00:11:22,640 --> 00:11:25,400
希望能够打开计算图和算子的边界

289
00:11:25,400 --> 00:11:28,440
右边的这个就是 ZOMI 来总结的一个图

290
00:11:28,440 --> 00:11:30,840
首先前端是有一个 Python 的代码

291
00:11:30,840 --> 00:11:32,840
用 Python 原生的数据语言

292
00:11:32,840 --> 00:11:34,760
然后对 Python 进行解析

293
00:11:34,760 --> 00:11:36,920
最后传给图层的 IR

294
00:11:36,920 --> 00:11:38,120
还有算子层的 IR

295
00:11:38,120 --> 00:11:40,760
这一层是希望进行融合的

296
00:11:40,760 --> 00:11:42,600
接着传给后端

297
00:11:42,600 --> 00:11:45,200
后端会根据不同的硬件进行一个编译

298
00:11:45,200 --> 00:11:46,440
例如 CPU 跟 TPU

299
00:11:46,440 --> 00:11:48,120
可能会用 LLVM IR

300
00:11:48,120 --> 00:11:51,080
GPU 可能会用 NVCC 去编译

301
00:11:51,080 --> 00:11:53,200
NPU 可能会用去用 GE IR

302
00:11:53,200 --> 00:11:54,280
然后去编译的

303
00:11:54,280 --> 00:11:55,640
在真正执行的时候

304
00:11:55,640 --> 00:11:57,000
可能就没有编译过程了

305
00:11:57,000 --> 00:11:58,000
而要直接 Runtime

306
00:11:58,000 --> 00:11:59,480
然后提供一些算子的库

307
00:11:59,480 --> 00:12:00,760
然后去执行

308
00:12:00,760 --> 00:12:02,600
对接到不同的硬件

309
00:12:02,600 --> 00:12:06,320
这个就是专用编译器的一个具体的架构图

310
00:12:06,320 --> 00:12:08,640
在阶段二专用 AI 编译器里面

311
00:12:08,640 --> 00:12:10,600
其实还有几个比较重要的问题

312
00:12:10,600 --> 00:12:13,680
其实在规避或者在努力的解决的

313
00:12:13,680 --> 00:12:15,480
第一种就是表达的分离

314
00:12:15,480 --> 00:12:17,080
因为计算图和算子层

315
00:12:17,080 --> 00:12:19,680
其实现在还是分开去表达的

316
00:12:19,680 --> 00:12:23,120
算子工程师主要是关心图层的表达

317
00:12:23,120 --> 00:12:24,320
就是我用 Tensorflow

318
00:12:24,320 --> 00:12:25,000
我用 PyTorch

319
00:12:25,000 --> 00:12:25,920
我用 MindSpore

320
00:12:25,920 --> 00:12:28,880
怎么去实现神经网络

321
00:12:28,880 --> 00:12:30,240
怎么实现算法

322
00:12:30,240 --> 00:12:32,440
但是具体的算子的表达和实现

323
00:12:32,640 --> 00:12:35,360
是由框架开发者和芯片厂商提供的

324
00:12:35,360 --> 00:12:36,960
这里面就会遇到一个大问题

325
00:12:36,960 --> 00:12:39,200
如果我在底层做了一个算子的融合

326
00:12:39,200 --> 00:12:41,320
算法工程师不知道 AI 框架

327
00:12:41,320 --> 00:12:43,600
或者 AI 编译器给做了一个算子融合

328
00:12:43,600 --> 00:12:46,280
算法工程师对算子的定义

329
00:12:46,280 --> 00:12:48,240
跟 AI 框架的开发者

330
00:12:48,240 --> 00:12:50,520
和芯片厂商提供的算子不对等了

331
00:12:50,520 --> 00:12:51,200
这个时候

332
00:12:51,200 --> 00:12:54,360
返回给算法工程师的一个报错的栈

333
00:12:54,360 --> 00:12:55,640
可能就会不一样了

334
00:12:55,640 --> 00:12:57,120
算法工程师就觉得

335
00:12:57,120 --> 00:12:59,440
怎么我写的代码

336
00:12:59,440 --> 00:13:00,760
跟我的报错不一样

337
00:13:00,760 --> 00:13:02,040
我找不到我的报错点

338
00:13:02,040 --> 00:13:04,440
但是他告诉我这段代码执行错误

339
00:13:04,640 --> 00:13:06,120
这就是分离表达

340
00:13:06,120 --> 00:13:07,880
遇到比较典型的一个问题

341
00:13:07,880 --> 00:13:10,160
第 2 个就是功能的泛化性

342
00:13:10,160 --> 00:13:11,920
在开发 MindSpore 的时候

343
00:13:11,920 --> 00:13:14,320
会遇到非常多的这种功能泛化性的问题

344
00:13:14,320 --> 00:13:16,120
其实现在已经解决了大部分

345
00:13:16,120 --> 00:13:18,360
例如在动静态图的转换

346
00:13:18,360 --> 00:13:20,440
就是我从静态图转成动态图

347
00:13:20,440 --> 00:13:22,960
我再从动态图转成静态图

348
00:13:22,960 --> 00:13:24,800
那这个时候动态图的灵活表达

349
00:13:24,800 --> 00:13:26,440
是不是全部 python 的表达

350
00:13:26,440 --> 00:13:28,360
都能够在静态图里面去承载

351
00:13:28,560 --> 00:13:29,080
不一定

352
00:13:29,080 --> 00:13:30,440
它可能会有一个 gap

353
00:13:30,440 --> 00:13:31,560
第 3 个就是动态 shape

354
00:13:31,560 --> 00:13:33,520
动态 shape 这个事情特别头疼

355
00:13:33,520 --> 00:13:36,240
后面会单独分开一节来去介绍的

356
00:13:36,240 --> 00:13:38,920
那另外还有稀疏计算和分布式并行优化

357
00:13:38,920 --> 00:13:41,400
这些需求都需要对 AI 框架

358
00:13:41,400 --> 00:13:43,000
或者对 AI 编译器

359
00:13:43,000 --> 00:13:45,200
做一个充分的功能的泛化

360
00:13:45,200 --> 00:13:47,880
最后一个就是平衡效率和性能

361
00:13:47,880 --> 00:13:49,920
效率和性能这个主要的体现

362
00:13:50,040 --> 00:13:51,520
就是在 kernel 层

363
00:13:51,520 --> 00:13:53,040
就是算子的底层

364
00:13:53,040 --> 00:13:54,080
怎么在 schedule

365
00:13:54,080 --> 00:13:54,480
tiling

366
00:13:54,480 --> 00:13:57,040
codegen 上面去自动化的表示

367
00:13:57,320 --> 00:13:58,680
因为现在来说

368
00:13:59,560 --> 00:14:00,920
算子的实现工程师

369
00:14:00,920 --> 00:14:02,200
就 kernel 的实现工程师

370
00:14:02,360 --> 00:14:04,720
他需要了解算子的一个逻辑

371
00:14:04,720 --> 00:14:07,960
同时他要了解硬件的一个架构体系

372
00:14:08,040 --> 00:14:09,240
你写算子的人

373
00:14:09,240 --> 00:14:10,440
你写你自己算法的人

374
00:14:10,440 --> 00:14:11,440
你不懂硬件架构

375
00:14:11,440 --> 00:14:12,240
然后随便写

376
00:14:12,240 --> 00:14:15,000
不能够充分发挥架构的一个优势

377
00:14:15,120 --> 00:14:16,480
那写算子的就会说

378
00:14:16,800 --> 00:14:18,000
你不懂我这个算法

379
00:14:18,120 --> 00:14:19,360
这个算法不能随便的

380
00:14:19,360 --> 00:14:20,920
按你这套逻辑来实现的

381
00:14:20,920 --> 00:14:22,320
不然它的逻辑就不对了

382
00:14:22,320 --> 00:14:23,800
它的计算结果就不对了

383
00:14:27,120 --> 00:14:28,920
第三个就是未来

384
00:14:28,920 --> 00:14:30,360
通用的 AI 编译器

385
00:14:30,360 --> 00:14:32,880
就是希望对图算的进行表达

386
00:14:32,880 --> 00:14:34,040
实现融合优化

387
00:14:34,040 --> 00:14:35,200
第二个就是 kernel

388
00:14:35,200 --> 00:14:37,280
算子层面去实现的自动的

389
00:14:37,280 --> 00:14:37,760
schedule

390
00:14:37,760 --> 00:14:38,080
tiling

391
00:14:38,080 --> 00:14:38,760
还有 codegen

392
00:14:38,760 --> 00:14:40,960
降低整体的开发门槛

393
00:14:40,960 --> 00:14:43,200
第三个就是对神经网络

394
00:14:43,200 --> 00:14:44,640
对 AI 的功能

395
00:14:44,840 --> 00:14:46,240
更进一步的泛化

396
00:14:46,240 --> 00:14:48,120
最后一个就是

397
00:14:48,120 --> 00:14:49,200
Chris 提出来的

398
00:14:49,200 --> 00:14:50,640
包括编译器运行是

399
00:14:50,640 --> 00:14:51,520
异构计算

400
00:14:51,520 --> 00:14:53,960
所有的东西都变成模块化的

401
00:14:53,960 --> 00:14:55,280
表示和组合

402
00:14:55,280 --> 00:14:57,480
专注于整体的可用性

403
00:14:57,480 --> 00:14:58,840
同样的前端

404
00:14:58,840 --> 00:15:00,480
还是用 python 进行解析

405
00:15:00,480 --> 00:15:02,160
然后对 python 的数据语言

406
00:15:02,160 --> 00:15:03,760
解析成 graph IR

407
00:15:03,760 --> 00:15:05,360
然后通过 Graph IR

408
00:15:05,560 --> 00:15:08,880
去进行一个图算融合的编译优化

409
00:15:08,880 --> 00:15:10,920
最后传给 backend

410
00:15:10,920 --> 00:15:11,760
就是后端

411
00:15:11,800 --> 00:15:14,520
后端就可以在不同的芯片架构上面

412
00:15:14,520 --> 00:15:15,440
或者加速器上面

413
00:15:15,640 --> 00:15:17,280
去执行和编译的

414
00:15:17,280 --> 00:15:20,040
这个就是希望未来可以出现的

415
00:15:20,400 --> 00:15:21,840
现在又回到了

416
00:15:21,840 --> 00:15:24,040
金雪峰雪峰总里面的图

417
00:15:24,240 --> 00:15:26,040
这个图我加了一些小标

418
00:15:26,320 --> 00:15:26,960
可以看到

419
00:15:26,960 --> 00:15:27,840
在阶段一的时候

420
00:15:28,000 --> 00:15:29,000
主要是有 TensorFlow

421
00:15:29,000 --> 00:15:30,280
但是在阶段二的时候

422
00:15:30,400 --> 00:15:31,960
就极度的去大量的

423
00:15:31,960 --> 00:15:34,680
爆发了不同的 AI 编译器的出现

424
00:15:34,680 --> 00:15:38,000
而现在主要是处在这一个阶段

425
00:15:38,960 --> 00:15:41,440
了解完 AI 编译器的一个发展情况之后

426
00:15:41,600 --> 00:15:43,440
或者什么是 AI 编译器

427
00:15:43,720 --> 00:15:45,600
现在抛出几个问题

428
00:15:45,600 --> 00:15:48,560
希望能够跟大家一起去思考和汇报的

429
00:15:48,920 --> 00:15:49,880
第一个问题就是

430
00:15:49,880 --> 00:15:52,560
AI 编译器跟传统编译器有什么区别

431
00:15:52,880 --> 00:15:54,680
这个其实在上一节分享里面

432
00:15:54,840 --> 00:15:55,760
有回答了

433
00:15:55,760 --> 00:15:58,520
只是希望能够让大家去 review 一下

434
00:15:58,520 --> 00:16:00,440
AI 编译器作为传统编译器的

435
00:16:00,440 --> 00:16:01,880
一个具体的补充

436
00:16:01,880 --> 00:16:04,640
而不是一个全新的替换

437
00:16:04,640 --> 00:16:06,240
另外它还借鉴了大量的

438
00:16:06,240 --> 00:16:08,040
传统编译器的一个思想

439
00:16:08,040 --> 00:16:08,800
第二个问题

440
00:16:09,160 --> 00:16:12,440
其实我觉得我自己也没有搞太清楚

441
00:16:12,440 --> 00:16:14,520
或者边界没有划分太明确了

442
00:16:14,720 --> 00:16:17,240
就是 AI 框架跟 AI 编译器什么关系吗

443
00:16:17,240 --> 00:16:20,200
到底是 AI 框架包含一个 AI 编译器

444
00:16:20,440 --> 00:16:23,440
还是 AI 框架就是一个大型移动的

445
00:16:23,440 --> 00:16:24,400
AI 编译器呢

446
00:16:24,760 --> 00:16:26,600
这个问题我觉得是非常有意思

447
00:16:26,600 --> 00:16:29,480
也希望引起大家去思考和讨论的

448
00:16:30,840 --> 00:16:31,840
第三点就是

449
00:16:31,840 --> 00:16:34,120
AI 领域真正需要编译器吗

450
00:16:34,640 --> 00:16:37,120
为什么像 Pytorch 的动态图模式

451
00:16:37,120 --> 00:16:37,960
这么多人用

452
00:16:38,080 --> 00:16:39,760
大家都觉得 Pytorch 很好

453
00:16:39,760 --> 00:16:42,240
但 Pytorch 的动态图模式是没有编译器的

454
00:16:42,920 --> 00:16:45,920
那现在 AI 领域真的是需要编译器吗

455
00:16:45,920 --> 00:16:49,120
第四个就是从投入的

456
00:16:49,360 --> 00:16:52,280
第四个就是从商业的角度去考虑的

457
00:16:52,280 --> 00:16:54,000
就是技术的投入比

458
00:16:54,640 --> 00:16:58,040
现在的实现一个编译器的工作

459
00:16:58,240 --> 00:17:00,520
跟人工实现的编译器的工作来看

460
00:17:00,720 --> 00:17:02,080
哪个性价比更高

461
00:17:02,080 --> 00:17:03,440
从最终的结果来看

462
00:17:03,600 --> 00:17:05,960
不见得 AI 编译器编译出来的算子

463
00:17:05,960 --> 00:17:08,720
就比人工实现的算子的性能还要好

464
00:17:09,120 --> 00:17:10,080
抛出了四个问题

465
00:17:10,080 --> 00:17:13,040
我觉得更聚焦的是第二个问题和第三个问题

466
00:17:13,040 --> 00:17:15,120
AI 框架跟 AI 编译器的关系

467
00:17:15,480 --> 00:17:17,560
AI 领域真的需要 AI 编译器吗

468
00:17:17,560 --> 00:17:20,160
带着这两个问题继续往下走

469
00:17:21,400 --> 00:17:23,000
卷得不行了 卷得不行了

470
00:17:23,000 --> 00:17:24,720
记得一键三连加关注哦

471
00:17:24,960 --> 00:17:27,880
所有的内容都会开源在下面这条链接里面

472
00:17:28,480 --> 00:17:29,200
拜了个拜

