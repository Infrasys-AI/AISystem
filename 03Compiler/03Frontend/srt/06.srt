1
00:00:00,000 --> 00:00:04,666
字幕生成：qiaokai 字幕校对：mkwei

2
00:00:05,800 --> 00:00:07,700
哈喽大家好我是 ZOMI

3
00:00:07,700 --> 00:00:08,100
今天呢

4
00:00:08,100 --> 00:00:10,766
来到 AI 编译器系列里面的

5
00:00:10,766 --> 00:00:13,299
前端优化那前端优化这里面呢

6
00:00:13,300 --> 00:00:14,900
会讲一个新的内容

7
00:00:14,900 --> 00:00:17,000
就是内存的分配

8
00:00:17,466 --> 00:00:18,333
内存分配呢

9
00:00:18,333 --> 00:00:19,799
是在前端优化里面

10
00:00:19,800 --> 00:00:21,700
基于计算图去感知

11
00:00:21,700 --> 00:00:23,166
去做一个具体的计算

12
00:00:23,166 --> 00:00:25,466
但实际上内存分配的执行呢

13
00:00:25,466 --> 00:00:26,299
不是在这里面

14
00:00:26,300 --> 00:00:28,566
而是在具体的 backend 里面去执行的

15
00:00:28,700 --> 00:00:29,266
那现在呢

16
00:00:29,266 --> 00:00:31,199
来到内存分配的内容里面

17
00:00:31,466 --> 00:00:34,166
今天呢要给大家带来三个内容

18
00:00:34,200 --> 00:00:34,700
第一个呢

19
00:00:34,700 --> 00:00:37,300
就看看模型和硬件关于内存

20
00:00:37,300 --> 00:00:38,566
相关的一个演进

21
00:00:38,800 --> 00:00:39,666
接着第二个内容

22
00:00:39,666 --> 00:00:42,766
去看看内存的划分和复用的好处

23
00:00:42,766 --> 00:00:46,133
AI 的内存跟传统的内存有哪些区别

24
00:00:46,266 --> 00:00:47,099
那第三个呢

25
00:00:47,100 --> 00:00:49,733
就是节省内存的具体的算法

26
00:00:49,733 --> 00:00:51,533
那第三个可能是重点

27
00:00:51,533 --> 00:00:53,866
而第二个呢是概念的澄清

28
00:00:54,366 --> 00:00:55,599
接下来看一下

29
00:00:55,600 --> 00:00:58,300
现在在整个 AI 编译器的全栈里面呢

30
00:00:58,333 --> 00:01:00,566
还是在图优化层里面

31
00:01:00,566 --> 00:01:01,699
那内存的分配呢

32
00:01:01,700 --> 00:01:03,366
更多的是对神经网络

33
00:01:03,366 --> 00:01:04,866
或者对计算图

34
00:01:04,866 --> 00:01:05,766
进行感知

35
00:01:05,866 --> 00:01:08,099
做一个预分配的工作

36
00:01:09,466 --> 00:01:10,299
来到第一个内容

37
00:01:10,300 --> 00:01:12,200
看看模型和硬件对

38
00:01:12,200 --> 00:01:13,333
内存的一个具体的需求

39
00:01:13,333 --> 00:01:16,099
可以看到其实最近这几年呢

40
00:01:16,100 --> 00:01:18,366
随着 AI 或者神经网络的

41
00:01:18,366 --> 00:01:19,933
模型越来越大

42
00:01:19,966 --> 00:01:23,166
也是需要非常大量的显存空间的

43
00:01:23,166 --> 00:01:24,699
例如像一个 BERT

44
00:01:24,933 --> 00:01:26,699
就 BERT 已经非常出名的

45
00:01:26,700 --> 00:01:29,400
一个芝麻街家族的芝麻鸡里面的一个

46
00:01:29,400 --> 00:01:30,266
小娃娃嘛

47
00:01:30,566 --> 00:01:33,666
它里面有 768 个隐藏 Hide layer

48
00:01:33,700 --> 00:01:36,466
那 Batch size 设置为 64 的时候呢

49
00:01:36,466 --> 00:01:39,199
就需要 73 个 GB 的显存空间了

50
00:01:39,333 --> 00:01:40,366
73 个 GB

51
00:01:40,366 --> 00:01:42,499
一般的一些消费卡是塞不进去的

52
00:01:42,666 --> 00:01:45,533
训练一个 BERT 呢大半都需要一个单机

53
00:01:45,533 --> 00:01:48,299
多卡的情况就利用分布式的功能了

54
00:01:48,933 --> 00:01:49,699
接下来呢

55
00:01:49,700 --> 00:01:52,566
看一下一个硬件的相关的能力

56
00:01:52,566 --> 00:01:54,733
这里面呢我就总结了一个

57
00:01:54,733 --> 00:01:56,499
英伟达的一个系列啊

58
00:01:56,666 --> 00:01:58,266
那可以看到 CUDA 的核心数啊

59
00:01:58,266 --> 00:01:59,999
确实是不断的去增长

60
00:02:00,066 --> 00:02:02,299
而单浮点精度的性能呢

61
00:02:02,300 --> 00:02:04,066
确实也是增长的很厉害

62
00:02:04,200 --> 00:02:05,533
但是内存的容量啊

63
00:02:05,533 --> 00:02:06,099
可以看到

64
00:02:06,100 --> 00:02:07,800
其实内存的容量

65
00:02:07,800 --> 00:02:10,566
并不是说增加的非常的夸张

66
00:02:10,566 --> 00:02:12,499
而且基本上不同的型号呢

67
00:02:12,500 --> 00:02:13,966
有不同的内存的配置

68
00:02:14,000 --> 00:02:14,733
那这个时候呢

69
00:02:14,733 --> 00:02:17,999
可以看到模型的参数量越来越大

70
00:02:18,000 --> 00:02:20,466
但是硬件的内存的容量

71
00:02:20,466 --> 00:02:22,699
却没有变得特别的大

72
00:02:22,866 --> 00:02:24,333
那这个时候回顾一下

73
00:02:24,333 --> 00:02:26,266
深度学习的整个训练流程里面

74
00:02:26,366 --> 00:02:29,366
哪些流程需要用到大量的内存空间

75
00:02:29,400 --> 00:02:31,500
那第一个就是 Data

76
00:02:31,600 --> 00:02:33,000
数的数据啊

77
00:02:33,000 --> 00:02:34,733
都要塞到内存里面哦

78
00:02:35,000 --> 00:02:36,533
在具体的训练过程当中呢

79
00:02:36,533 --> 00:02:39,299
还要塞进去神经网络

80
00:02:39,300 --> 00:02:40,166
那神经网络呢

81
00:02:40,166 --> 00:02:42,666
有包括正向的神经网络的图

82
00:02:42,666 --> 00:02:45,166
还有反向的神经网络的图

83
00:02:45,200 --> 00:02:46,600
正反向加起来

84
00:02:46,600 --> 00:02:48,866
内存空间就变大了

85
00:02:48,866 --> 00:02:50,766
所以说大部分内存呢

86
00:02:50,766 --> 00:02:52,199
主要是消耗在数据

87
00:02:52,200 --> 00:02:54,533
还有网络模型里面

88
00:02:56,300 --> 00:02:58,966
第二个内容就是具体在 AI

89
00:02:58,966 --> 00:03:00,166
在神经网络里面呢

90
00:03:00,166 --> 00:03:02,066
内存的划分到底分什么呢

91
00:03:02,066 --> 00:03:03,299
主要分为两个

92
00:03:03,300 --> 00:03:06,400
一个是静态的内存一个是动态的内存

93
00:03:06,766 --> 00:03:08,499
现在看一下静态的内存

94
00:03:08,500 --> 00:03:10,166
静态的内存呢主要有三个

95
00:03:10,166 --> 00:03:11,266
第一个是 Parameter

96
00:03:11,400 --> 00:03:13,066
就是网络模型当中的

97
00:03:13,066 --> 00:03:14,466
一些权重参数

98
00:03:14,500 --> 00:03:16,900
权重参数呢基本上就是不变的

99
00:03:16,900 --> 00:03:18,933
我申请了这么多就塞在内存里面

100
00:03:18,933 --> 00:03:20,866
不断的去训练学习更新

101
00:03:21,166 --> 00:03:22,599
接着第二个就是 Value Node

102
00:03:22,600 --> 00:03:24,700
就是网络模型中的常量

103
00:03:24,733 --> 00:03:26,999
那有一些常量呢不能够被折叠的

104
00:03:27,000 --> 00:03:27,700
这些常量呢

105
00:03:27,700 --> 00:03:30,666
在继续训练的过程当中不断的去用到

106
00:03:30,700 --> 00:03:32,800
然后呢也是作为一个静态内存

107
00:03:32,800 --> 00:03:34,066
塞在内存里面

108
00:03:34,100 --> 00:03:37,400
那第三个呢就是 Output 网络模型的输出

109
00:03:37,666 --> 00:03:38,199
这个时候呢

110
00:03:38,200 --> 00:03:39,900
一般呢对静态内存呢

111
00:03:39,900 --> 00:03:42,266
就是一次过在初始化的时候

112
00:03:42,266 --> 00:03:43,466
申请完之后呢

113
00:03:43,466 --> 00:03:46,166
就不再在推理或者训练的场景里面

114
00:03:46,200 --> 00:03:47,466
频繁的申请

115
00:03:47,566 --> 00:03:50,266
从而提高整个系统的一个性能

116
00:03:50,300 --> 00:03:53,400
那现在以一个具体的图来看一下

117
00:03:53,400 --> 00:03:55,000
大部分这些静态内存呢

118
00:03:55,000 --> 00:03:57,000
主要是指权重

119
00:03:57,000 --> 00:03:59,100
还有优化器

120
00:03:59,100 --> 00:04:02,100
所对应到的一些内存空间

121
00:04:03,100 --> 00:04:06,200
下面看第二个内容就是动态内存

122
00:04:06,200 --> 00:04:08,500
那动态内存呢主要分开两个

123
00:04:08,500 --> 00:04:10,000
一个是 Output Tensor

124
00:04:10,100 --> 00:04:12,366
这是算子每一层的算子哦

125
00:04:12,366 --> 00:04:13,366
不是总体哦

126
00:04:13,466 --> 00:04:15,499
是每一层的算子输出的 Tensor

127
00:04:15,566 --> 00:04:17,199
做一个动态的内存

128
00:04:17,200 --> 00:04:19,766
因为这里面有很多啊他可以复用的

129
00:04:19,766 --> 00:04:21,766
后面具体讲具体算法时候

130
00:04:21,766 --> 00:04:22,599
就讲到了

131
00:04:22,766 --> 00:04:24,766
那第二个就是 Workpace 的 Tensor

132
00:04:24,900 --> 00:04:27,400
如果有用户或者有开发者自己去

133
00:04:27,400 --> 00:04:29,166
写过一些 CUDA 的代码的时候呢

134
00:04:29,166 --> 00:04:30,866
就会发现很多时候呢

135
00:04:30,900 --> 00:04:32,166
在写 CUDA 之前

136
00:04:32,200 --> 00:04:34,966
就需要单独的去申请一个 Workspace

137
00:04:35,000 --> 00:04:36,866
这种呢就是在网络模型

138
00:04:36,866 --> 00:04:38,333
特别是在 Kernel

139
00:04:38,500 --> 00:04:39,900
计算的过程当中呢

140
00:04:39,900 --> 00:04:41,800
申请了一些临时的 buffer

141
00:04:43,533 --> 00:04:44,566
动态内存呢

142
00:04:44,566 --> 00:04:46,666
是在整个神经网络里面

143
00:04:46,666 --> 00:04:48,533
占了非常大的一个大头

144
00:04:48,533 --> 00:04:50,099
看一看这个图

145
00:04:50,166 --> 00:04:52,566
动态内存的红色的框框

146
00:04:52,566 --> 00:04:55,266
它的内容呢确实比静态内存要多很多

147
00:04:55,266 --> 00:04:56,366
例如像这个呢

148
00:04:56,366 --> 00:04:57,766
就是卷积的时候

149
00:04:57,800 --> 00:04:59,900
额外的申请的一些额外的空间

150
00:04:59,900 --> 00:05:01,566
那网络模型的输出呢

151
00:05:01,866 --> 00:05:04,599
其实也是对应于动态的内存的

152
00:05:07,200 --> 00:05:10,133
看一下下面这个图就是 MobileNet v2

153
00:05:10,133 --> 00:05:10,733
那上面呢

154
00:05:10,733 --> 00:05:12,399
就是没有经过内存优化的

155
00:05:12,400 --> 00:05:13,700
一个具体的内存

156
00:05:13,733 --> 00:05:15,533
空间开辟的一个图

157
00:05:15,700 --> 00:05:16,133
下面呢

158
00:05:16,133 --> 00:05:18,366
就是经过内存优化之后的一个图

159
00:05:18,366 --> 00:05:20,266
可以看到以颜色来看呢

160
00:05:20,266 --> 00:05:21,599
这个是神经网络 MobileNet v2

161
00:05:21,600 --> 00:05:22,933
的一个图结构

162
00:05:22,933 --> 00:05:24,699
那可以看到橙色的这一块呢

163
00:05:24,700 --> 00:05:27,466
就是做了内存优化之后的

164
00:05:27,466 --> 00:05:29,266
整体的内存的消耗量

165
00:05:29,266 --> 00:05:32,499
可以看到一个 MobileNet v2 的网络模型

166
00:05:32,800 --> 00:05:34,966
进行了一个内存优化之后呢

167
00:05:35,066 --> 00:05:36,499
它整体的网络模型占

168
00:05:36,500 --> 00:05:38,300
的一个内存空间的数

169
00:05:38,333 --> 00:05:39,299
是非常的少

170
00:05:39,300 --> 00:05:42,200
但是没有做优化之前呢就非常多倍了

171
00:05:42,300 --> 00:05:44,300
那现在怎么做优化呢

172
00:05:44,300 --> 00:05:45,966
继续往下看

173
00:05:47,800 --> 00:05:51,100
就是节省内存的算法

174
00:05:51,300 --> 00:05:53,000
oh my god

175
00:05:53,866 --> 00:05:54,699
哇哦

176
00:05:55,766 --> 00:05:57,766
在进行节省内存的算法之前呢

177
00:05:57,766 --> 00:06:00,499
我要讲四个算法啊

178
00:06:00,500 --> 00:06:01,466
这四个算法呢

179
00:06:01,466 --> 00:06:04,166
今天的重点就是内存的复用

180
00:06:04,166 --> 00:06:05,666
就是利用 AI 编译器

181
00:06:05,900 --> 00:06:06,900
对计算图

182
00:06:06,900 --> 00:06:08,933
Graph IR 的数据流进行分析

183
00:06:08,933 --> 00:06:11,533
复用内存是今天的重点

184
00:06:11,566 --> 00:06:12,866
但是呢我这里面呢

185
00:06:12,866 --> 00:06:15,966
也给大家介绍一下几个不同的算法

186
00:06:16,100 --> 00:06:18,933
那第一个呢就是空间换内存

187
00:06:18,933 --> 00:06:19,766
那这种呢

188
00:06:19,766 --> 00:06:23,366
其实在传统的一些程序里面呢

189
00:06:23,400 --> 00:06:24,733
用的也很多

190
00:06:24,766 --> 00:06:27,333
那在 AI 里面或者在深度学习里面呢

191
00:06:27,333 --> 00:06:30,066
一般都会做一些 CPU 的 Offload

192
00:06:30,200 --> 00:06:31,600
就是把内存空间

193
00:06:31,600 --> 00:06:33,366
把 NPU 的内存空间

194
00:06:33,900 --> 00:06:35,166
丢给 CPU

195
00:06:35,266 --> 00:06:37,799
可能他临时用不到了这一个模块

196
00:06:37,800 --> 00:06:40,000
那就直接卸载到 CPU 里面

197
00:06:40,133 --> 00:06:41,466
那这种算法呢

198
00:06:41,466 --> 00:06:44,533
更多的是针对 MOE 的一些算法结构

199
00:06:44,533 --> 00:06:46,266
就是有很多个 Expert

200
00:06:46,500 --> 00:06:49,333
Multi of Experts 的这种算法或者模型结构

201
00:06:49,333 --> 00:06:51,266
去做一个优化的

202
00:06:52,400 --> 00:06:54,766
第二个呢就是计算换内存

203
00:06:54,766 --> 00:06:56,099
在计算换内存里面呢

204
00:06:56,100 --> 00:06:58,533
最重要的一个算法呢就是 Gradient Checkpointing

205
00:06:58,533 --> 00:07:00,099
叫做重计算

206
00:07:00,166 --> 00:07:01,799
那基本上很多时候啊

207
00:07:01,800 --> 00:07:04,066
如果大家去把一些

208
00:07:04,100 --> 00:07:06,933
计算的中间结果存在内存空间

209
00:07:07,500 --> 00:07:08,333
需要把

210
00:07:08,333 --> 00:07:10,166
从 ALU 就计算单元

211
00:07:10,166 --> 00:07:11,699
里面算出来的结果

212
00:07:11,700 --> 00:07:13,333
存到内存空间

213
00:07:13,400 --> 00:07:16,066
这里面传输的速率就很重要了

214
00:07:16,066 --> 00:07:18,399
假设我重新再算一遍的速率

215
00:07:18,400 --> 00:07:21,000
都比我读取数据的效率要高

216
00:07:21,300 --> 00:07:23,866
那我肯定会选择重新算一把吗

217
00:07:23,866 --> 00:07:25,599
因为重新算一把的效率

218
00:07:25,600 --> 00:07:28,166
可能会比我直接存起来更快

219
00:07:28,166 --> 00:07:30,666
那这个时候呢叫做计算换内存

220
00:07:30,666 --> 00:07:32,966
在做一些大模型的时候呢

221
00:07:32,966 --> 00:07:35,199
这一个计算换内存的算法优化呢

222
00:07:35,200 --> 00:07:36,800
也是非常常用的

223
00:07:38,000 --> 00:07:40,400
第三点呢就是模型压缩

224
00:07:40,400 --> 00:07:40,933
模型压缩

225
00:07:40,933 --> 00:07:44,099
这个在端侧推理的时候特别常用

226
00:07:44,333 --> 00:07:44,933
例如呢

227
00:07:44,933 --> 00:07:47,266
会做一些低比特量化的一些工作

228
00:07:47,266 --> 00:07:48,366
还有模型剪枝

229
00:07:48,366 --> 00:07:50,866
还有模型蒸馏相关的一些算法

230
00:07:50,866 --> 00:07:53,066
这些都是内存节省的算法

231
00:07:53,066 --> 00:07:53,999
而今天的主角

232
00:07:54,000 --> 00:07:56,100
就是真正的内存复用

233
00:07:58,466 --> 00:07:59,299
内存复用呢

234
00:07:59,300 --> 00:08:00,733
里面有几个操作

235
00:08:00,733 --> 00:08:03,166
我现在给大家简单的讲一讲

236
00:08:03,200 --> 00:08:06,166
那第一种呢就是替换的操作

237
00:08:06,166 --> 00:08:08,399
叫做 Inplace operation

238
00:08:08,666 --> 00:08:10,866
就是一个内存呢假设这里面是一个

239
00:08:10,866 --> 00:08:12,599
左边呢是一个计算图

240
00:08:12,666 --> 00:08:14,366
每个方框呢是一个节点

241
00:08:14,366 --> 00:08:16,933
三角形呢代表的是内存

242
00:08:17,100 --> 00:08:19,466
那如果有一块的内存算完

243
00:08:19,466 --> 00:08:22,266
这一块就这个 B 算子里面不用了

244
00:08:22,300 --> 00:08:24,666
而且下一个算子呢也是 element-wise

245
00:08:24,666 --> 00:08:26,466
就跟他的操作是一模一样的

246
00:08:26,466 --> 00:08:28,333
可以原地的覆盖掉

247
00:08:28,333 --> 00:08:30,166
就这个内存空间可以

248
00:08:30,466 --> 00:08:32,799
直接用下一个内存空间覆盖掉

249
00:08:32,800 --> 00:08:33,900
就不需要了

250
00:08:33,933 --> 00:08:36,666
那这种呢叫做 Inplace operation

251
00:08:36,766 --> 00:08:38,166
但是呢有种情况啊

252
00:08:38,166 --> 00:08:40,199
由右边的这个图所示啊

253
00:08:40,266 --> 00:08:42,333
像这个 B 算完之后呢

254
00:08:42,500 --> 00:08:44,533
如果没有另外一个分支

255
00:08:44,533 --> 00:08:45,766
绿色的这个分支呢

256
00:08:45,766 --> 00:08:46,933
其实我 C 呢

257
00:08:47,000 --> 00:08:48,866
可以做一个 Inplace operation

258
00:08:48,900 --> 00:08:50,266
直接把它覆盖掉的

259
00:08:50,466 --> 00:08:53,699
但是因为需要一个 F 等于 B 加 2

260
00:08:53,700 --> 00:08:56,066
就是我需要 B 的这个计算的结果

261
00:08:56,100 --> 00:08:56,800
去加一个 2

262
00:08:56,800 --> 00:08:59,133
那这个时候呢 B 的结果其实是有用的

263
00:08:59,133 --> 00:09:01,666
它就不能做一个 Inplace operation 的

264
00:09:01,666 --> 00:09:03,166
就这块内存还是有用的

265
00:09:03,166 --> 00:09:05,699
你不能把它改写你不能把它复用掉

266
00:09:07,800 --> 00:09:09,133
第二种内存优化的方式呢

267
00:09:09,133 --> 00:09:12,499
叫做内存的共享 memory sharing

268
00:09:13,466 --> 00:09:15,366
在计算图里面呢

269
00:09:15,366 --> 00:09:16,933
这个就是计算图嘛

270
00:09:16,933 --> 00:09:19,066
如果有两个数据的内存呢

271
00:09:19,066 --> 00:09:21,099
使用的大小空间是一样

272
00:09:21,133 --> 00:09:22,399
那这个时候呢

273
00:09:22,533 --> 00:09:24,566
上面的不需要参与计算呢

274
00:09:24,800 --> 00:09:27,733
后面呢其实可以重新的去覆盖掉

275
00:09:27,733 --> 00:09:29,599
去共享这一块的内存

276
00:09:29,600 --> 00:09:30,733
当然了这一块的内存

277
00:09:30,733 --> 00:09:33,166
不用重复的去申请和释放

278
00:09:33,166 --> 00:09:34,299
只需要进行一个

279
00:09:34,300 --> 00:09:35,900
共享内存空间就可以了

280
00:09:36,200 --> 00:09:38,600
那如下面这个图所示以

281
00:09:38,600 --> 00:09:41,333
一个例子 A 呢是一个上面的一个数据

282
00:09:41,333 --> 00:09:43,733
W 呢是下面的一个权重的数据

283
00:09:43,800 --> 00:09:45,566
B 呢是一个卷积的算子

284
00:09:45,566 --> 00:09:46,766
那内存空间呢

285
00:09:46,766 --> 00:09:48,299
已经开辟出来一个三角形

286
00:09:48,533 --> 00:09:49,499
C 去执行的时候

287
00:09:49,500 --> 00:09:51,266
因为这个三角形的内存空间

288
00:09:51,266 --> 00:09:53,333
跟上面的内存空间大小是不一样的

289
00:09:53,333 --> 00:09:54,966
而他们的计算也是不一样的

290
00:09:55,066 --> 00:09:55,866
这个时候呢

291
00:09:55,866 --> 00:09:57,199
他们之间的内存是

292
00:09:57,200 --> 00:09:58,700
不能做一个 sharing 的

293
00:09:58,900 --> 00:10:00,533
下面又有一个 E 的算子

294
00:10:00,533 --> 00:10:02,733
那这个算子的内存空间的大小

295
00:10:02,733 --> 00:10:05,066
假设啊相关的参数也是相同的

296
00:10:05,133 --> 00:10:07,399
这个时候他的内存空间大小相同

297
00:10:07,733 --> 00:10:10,166
那这种情况就可以直接去复用掉了

298
00:10:10,166 --> 00:10:11,999
因为上面跟下面之间呢

299
00:10:12,000 --> 00:10:13,700
没有太多的依赖关系

300
00:10:13,700 --> 00:10:15,800
而且在算 E 算子的时候呢

301
00:10:15,800 --> 00:10:17,566
B 算子已经算完了不需要了

302
00:10:17,566 --> 00:10:20,299
就可以对 B 算子的内存空间呢

303
00:10:20,300 --> 00:10:21,900
进行一个共享

304
00:10:23,300 --> 00:10:25,400
下面总体呢来看一下

305
00:10:25,400 --> 00:10:26,900
内存空间的优化方法

306
00:10:26,900 --> 00:10:28,500
就是刚才讲到的两种

307
00:10:28,566 --> 00:10:31,733
第一种就是 Inplace operation 内存替换

308
00:10:31,733 --> 00:10:33,666
第二种呢就是 Memory sharing

309
00:10:33,666 --> 00:10:35,499
内存的一个共享

310
00:10:35,800 --> 00:10:38,066
可以看到左边呢这是正向图

311
00:10:38,066 --> 00:10:39,533
像刚才举的例子呢

312
00:10:39,533 --> 00:10:41,133
只是一个简单的正向图

313
00:10:41,300 --> 00:10:44,900
右边呢是正向图跟反向图都有的

314
00:10:44,900 --> 00:10:48,066
那这个时候模型就变得复杂了

315
00:10:48,133 --> 00:10:50,966
如果要对正反向图都有的模型

316
00:10:50,966 --> 00:10:53,499
这么复杂的模型做一个内存的优化

317
00:10:53,500 --> 00:10:55,733
实际上啊他并不简单

318
00:10:55,733 --> 00:10:56,666
这时候呢

319
00:10:56,666 --> 00:10:59,866
就涉及到了很多新的算法的提出

320
00:11:00,333 --> 00:11:01,933
怎么样才能够正确的

321
00:11:01,933 --> 00:11:04,266
去分配具体的内存呢

322
00:11:04,533 --> 00:11:07,299
去分配 AI 计算图的内存呢

323
00:11:08,133 --> 00:11:09,966
那这个呢它跟

324
00:11:09,966 --> 00:11:11,933
传统的编译器的

325
00:11:12,066 --> 00:11:14,299
寄存器的一个内存的分配非常相似

326
00:11:14,300 --> 00:11:17,266
可以借鉴很多的思想

327
00:11:19,100 --> 00:11:21,533
现在呢简单的去解读一下

328
00:11:21,533 --> 00:11:23,066
那这个内存的优化方法呢

329
00:11:23,066 --> 00:11:24,066
其实跟传统的

330
00:11:24,066 --> 00:11:25,399
编译器的内存的优化方法

331
00:11:25,400 --> 00:11:26,266
有点像

332
00:11:26,300 --> 00:11:27,700
简单来看一下

333
00:11:27,766 --> 00:11:30,366
左边的这一个呢就是计算图

334
00:11:30,366 --> 00:11:31,399
中间的虚线呢

335
00:11:31,400 --> 00:11:33,600
就是计算图的一个数据的流布

336
00:11:33,666 --> 00:11:35,133
在初始化的时候呢

337
00:11:35,133 --> 00:11:37,499
去感知到每一个算子

338
00:11:37,600 --> 00:11:39,466
被使用被调用到多少次

339
00:11:39,466 --> 00:11:41,933
像这个 B 算子呢我就被调用到 2 次

340
00:11:41,933 --> 00:11:44,566
他有两个使用有两个依赖

341
00:11:44,566 --> 00:11:46,366
那其他算子呢基本上都是一

342
00:11:46,366 --> 00:11:49,266
那这个算子 G 算子呢就是最后的输出

343
00:11:49,266 --> 00:11:52,333
这是第 0 步需要做一个标记

344
00:11:54,533 --> 00:11:56,366
接着呢在 step 1 的时候呢

345
00:11:56,366 --> 00:11:58,166
会分配一个 tag

346
00:11:58,200 --> 00:12:00,200
对 B 算子分配一个 tag

347
00:12:00,333 --> 00:12:01,999
就像红色的这个小模块

348
00:12:02,466 --> 00:12:04,666
再分配一个新的 tag 呢

349
00:12:04,666 --> 00:12:06,166
分配一个新的 tag 的时候

350
00:12:06,266 --> 00:12:07,766
可以发现 C 的算子呢

351
00:12:07,766 --> 00:12:08,699
跟 B 算子啊

352
00:12:08,700 --> 00:12:11,200
其实不能做一个 Inplace 的操作的

353
00:12:11,266 --> 00:12:14,166
因为 B 的算子他的生命周期还在

354
00:12:14,166 --> 00:12:16,133
他没有变 0 他只是变成 1 了

355
00:12:16,133 --> 00:12:17,799
就是下面已经用了一次

356
00:12:17,800 --> 00:12:18,866
他的生命周期了

357
00:12:18,866 --> 00:12:20,333
但是这两个不能 Inplace

358
00:12:20,366 --> 00:12:21,599
因为他没有变 0

359
00:12:22,300 --> 00:12:25,100
接着呢再往下看第三步

360
00:12:25,266 --> 00:12:28,166
第三步呢去计算 F 算子的时候呢

361
00:12:28,166 --> 00:12:29,133
我分配一个 tag

362
00:12:29,133 --> 00:12:31,999
那这个时候 F 算子已经计算完了

363
00:12:32,333 --> 00:12:33,999
上面 B 算子呢可以释放出来

364
00:12:34,000 --> 00:12:36,566
所以释放到对应的一个队列里面

365
00:12:36,566 --> 00:12:38,199
队列里面呢就有一个 B 算子

366
00:12:38,200 --> 00:12:39,266
的空间

367
00:12:40,000 --> 00:12:42,666
那第四步呢继续往下去执行

368
00:12:42,666 --> 00:12:43,499
执行的时候呢

369
00:12:43,500 --> 00:12:46,100
其实这个 C 算子呢已经执行完了

370
00:12:46,300 --> 00:12:47,600
那 C 算子执行完了

371
00:12:47,600 --> 00:12:48,366
执行完之后呢

372
00:12:48,366 --> 00:12:49,333
他的内存空间呢

373
00:12:49,333 --> 00:12:51,933
也会放进来内存队列

374
00:12:52,800 --> 00:12:55,066
但这个时候呢我要执行一个 E 算子

375
00:12:55,066 --> 00:12:56,466
所以会把刚才的内存

376
00:12:56,466 --> 00:12:58,899
对列的那个红色的小方块呢

377
00:12:58,966 --> 00:13:00,366
把它 push 出来

378
00:13:00,366 --> 00:13:02,733
给到一个 E 算子

379
00:13:02,733 --> 00:13:05,599
去进行一个内存的 memory sharing

380
00:13:06,700 --> 00:13:09,066
而这里面对应的标记啊

381
00:13:09,066 --> 00:13:11,166
也会去进行修改的

382
00:13:11,600 --> 00:13:12,566
那在这个时候呢

383
00:13:12,566 --> 00:13:13,199
可以看到

384
00:13:13,200 --> 00:13:15,200
在进行下一个计算的时候呢

385
00:13:15,300 --> 00:13:17,400
假设他的内存空间都是相同的

386
00:13:17,766 --> 00:13:18,799
就可以 reuse

387
00:13:18,800 --> 00:13:20,100
相关的内存模块

388
00:13:20,100 --> 00:13:21,933
进行一个 Inplace 的操作

389
00:13:21,966 --> 00:13:23,533
那最后

390
00:13:23,533 --> 00:13:25,899
通过这种方式进行迭代下来

391
00:13:25,933 --> 00:13:28,966
看到最后的 Final Memory 的 Plan

392
00:13:28,966 --> 00:13:29,933
为啥叫 plan 呢

393
00:13:29,933 --> 00:13:31,399
因为它不是实际执行

394
00:13:31,500 --> 00:13:34,166
而是一个预执行预分配的工作

395
00:13:34,166 --> 00:13:35,499
可以看到这里面呢

396
00:13:35,500 --> 00:13:36,466
不同的颜色

397
00:13:36,466 --> 00:13:39,399
代表不同的内存的分配的模块

398
00:13:39,400 --> 00:13:40,866
和内存的分配的空间

399
00:13:41,133 --> 00:13:42,499
里面三个红色的

400
00:13:42,500 --> 00:13:45,133
就可以各自的去进行一个复用

401
00:13:45,133 --> 00:13:46,566
而中间的那两块呢

402
00:13:46,566 --> 00:13:48,399
确实就没办法进行复用了

403
00:13:48,400 --> 00:13:48,966
这个就是

404
00:13:48,966 --> 00:13:51,299
最原始的一个内存优化的方法

405
00:13:53,866 --> 00:13:55,466
哎 ZOMI 老师你好啊

406
00:13:55,666 --> 00:13:56,666
我有一个问题

407
00:13:56,666 --> 00:13:58,733
就是你刚才讲到的一些内存

408
00:13:58,733 --> 00:13:59,966
分配的方法呢

409
00:13:59,966 --> 00:14:01,333
都是串行的

410
00:14:01,666 --> 00:14:03,566
并行的时候怎么办呢

411
00:14:04,200 --> 00:14:07,700
哎小新同学这个还是个灵魂问题啊

412
00:14:07,866 --> 00:14:11,666
确实会遇到很多并行的操作

413
00:14:12,333 --> 00:14:14,566
假设现在有一个计算图啊

414
00:14:14,566 --> 00:14:18,499
这个计算图呢有 12345678 八个算子

415
00:14:18,500 --> 00:14:20,100
就从 A1 算到 A8

416
00:14:20,133 --> 00:14:22,199
如果我只是以串行的方式去运行

417
00:14:22,200 --> 00:14:24,566
我就左边运行一个右边运行一个

418
00:14:24,566 --> 00:14:26,199
左边运行一个右边运行一个

419
00:14:26,400 --> 00:14:27,166
这种方式呢

420
00:14:27,166 --> 00:14:28,966
这两种分配的方案呢

421
00:14:28,966 --> 00:14:31,266
或者刚才讲到 Inplace 和 Memory sharing

422
00:14:31,266 --> 00:14:34,466
两种方案都是能够去执行的

423
00:14:34,733 --> 00:14:37,099
但是呢这种方式引入了非常多的依赖

424
00:14:37,100 --> 00:14:38,333
例如可以看到

425
00:14:38,366 --> 00:14:41,266
基本上啊有很多的并行的依赖

426
00:14:41,266 --> 00:14:43,166
就左边跟右边是相同的

427
00:14:43,166 --> 00:14:44,733
有内存相关的依赖

428
00:14:44,766 --> 00:14:46,666
以刚才的一种算法去分配呢

429
00:14:46,666 --> 00:14:50,699
就很难做到一个并行的运行操作

430
00:14:50,700 --> 00:14:53,366
就假设我左边的这条分支给一个 NPU 核

431
00:14:53,700 --> 00:14:54,533
去运行

432
00:14:54,533 --> 00:14:55,566
右边的这个分支呢

433
00:14:55,566 --> 00:14:57,366
给另外一个 NPU 核去运行

434
00:14:57,400 --> 00:14:59,966
那这种方式呢其实是没办法去做的

435
00:14:59,966 --> 00:15:01,399
因为它有内存的依赖

436
00:15:02,466 --> 00:15:02,933
于是呢

437
00:15:02,933 --> 00:15:06,166
可以向右边的这种分配的方式

438
00:15:06,166 --> 00:15:07,533
我左边的分配

439
00:15:07,533 --> 00:15:09,399
以左边的这条分支作为分配

440
00:15:09,400 --> 00:15:11,966
右边的我以右边的这条分支

441
00:15:11,966 --> 00:15:14,366
自己做一个具体的分配

442
00:15:14,800 --> 00:15:15,400
那这种呢

443
00:15:15,400 --> 00:15:16,900
就是以左边的这条分支

444
00:15:16,900 --> 00:15:18,266
作为一个单独的内存分配

445
00:15:18,266 --> 00:15:20,866
右边的那个分支呢作为单独的分配

446
00:15:20,866 --> 00:15:22,999
就可以进行一个并行的操作了

447
00:15:24,700 --> 00:15:25,533
那在这里面呢

448
00:15:25,533 --> 00:15:27,599
我没有提到某一种具体的算法

449
00:15:27,600 --> 00:15:29,700
这里面我更希望大家去创造

450
00:15:29,700 --> 00:15:31,200
更多的内存的分配的算法

451
00:15:31,200 --> 00:15:33,166
和发表更多相关的论文

452
00:15:33,933 --> 00:15:34,733
在这里面呢

453
00:15:34,733 --> 00:15:36,499
有一条最基本的原则

454
00:15:36,500 --> 00:15:38,966
就是尽可能的允许更多的并行

455
00:15:39,066 --> 00:15:40,933
毕竟啊 NPU 或者 GPU

456
00:15:40,933 --> 00:15:42,566
一些 AI 的加速芯片呢

457
00:15:42,566 --> 00:15:44,866
是有大量的并行的一些能力的

458
00:15:44,900 --> 00:15:47,066
所以希望引入更多的并行

459
00:15:47,066 --> 00:15:48,999
而不是简单的串行的依赖

460
00:15:49,266 --> 00:15:51,266
这个时候呢怎么去分配呢

461
00:15:51,300 --> 00:15:52,900
一般的分配的过程当中呢

462
00:15:52,900 --> 00:15:55,866
就会去尝试找到每个图里面的

463
00:15:55,933 --> 00:15:57,399
最长的路径

464
00:15:57,400 --> 00:15:59,333
然后根据这个最长的路径呢

465
00:15:59,333 --> 00:16:01,533
进行一个简单的 Inplace 和

466
00:16:01,666 --> 00:16:02,999
share memory 的操作

467
00:16:05,200 --> 00:16:07,533
针对另外的独立的路径呢

468
00:16:07,533 --> 00:16:09,699
再独立的进行 Inplace

469
00:16:09,700 --> 00:16:10,933
和 memory 的操作

470
00:16:10,933 --> 00:16:11,866
那这种方式呢

471
00:16:11,866 --> 00:16:14,366
就可以最大程度的利用到系统的

472
00:16:14,366 --> 00:16:15,266
并行能力

473
00:16:17,000 --> 00:16:17,666
最后这个图呢

474
00:16:17,666 --> 00:16:19,499
就是 MXNet 的一个对比里面呢

475
00:16:19,500 --> 00:16:22,066
就用了 Inplace 和 share memory 的一个操作

476
00:16:22,066 --> 00:16:24,733
那 share memory 呢他这里面叫做 co-share

477
00:16:24,933 --> 00:16:27,499
呃其实都是一样的就是一个概念

478
00:16:27,500 --> 00:16:30,933
然后可以看到啊把两个算法结合起来

479
00:16:31,066 --> 00:16:32,799
对内存的优化

480
00:16:32,800 --> 00:16:34,533
和对内存的消耗啊

481
00:16:34,766 --> 00:16:36,733
确实有一个比较大的提升

482
00:16:36,733 --> 00:16:39,499
对原始的就 naive 的这种方式呢

483
00:16:39,566 --> 00:16:41,266
都有一个比较好的提升

484
00:16:41,266 --> 00:16:42,699
对不同的网络模型

485
00:16:46,300 --> 00:16:48,533
好了今天来一个总结

486
00:16:48,533 --> 00:16:50,966
就是现在呢 AI 编译器啊

487
00:16:51,066 --> 00:16:52,999
大部分呢都是用 Graph IR

488
00:16:53,000 --> 00:16:56,500
进行巧妙的对内存进行分配

489
00:16:56,566 --> 00:16:58,533
那今天的内容呢就到这里为止

490
00:16:58,533 --> 00:16:59,333
谢谢各位

491
00:16:59,600 --> 00:17:01,200
卷的不行了卷的不行了

492
00:17:01,200 --> 00:17:02,966
记得一键三连加关注哦

493
00:17:03,000 --> 00:17:04,333
所有的内容都会开源

494
00:17:04,333 --> 00:17:06,166
在下面这条链接里面

495
00:17:06,533 --> 00:17:07,499
拜了个拜

