1
00:00:00,000 --> 00:00:04,033
字幕生成：qiaokai  字幕校对：mkwei

2
00:00:05,200 --> 00:00:07,900
哈喽 大家晚上好 我是 ZOMI

3
00:00:07,933 --> 00:00:10,966
那今天呢来到一个新的内容

4
00:00:11,000 --> 00:00:13,200
AI 编译器之前端优化

5
00:00:13,200 --> 00:00:15,700
那这个呢就是一个全新的内容了

6
00:00:15,733 --> 00:00:19,533
讲到 AI 编译器肯定会有前端的一些优化

7
00:00:19,533 --> 00:00:22,199
前端的优化其实之前已经介绍过了

8
00:00:22,200 --> 00:00:25,300
前端主要是关注于图层的一些优化

9
00:00:25,333 --> 00:00:27,833
在图层的优化实际上有非常多

10
00:00:27,833 --> 00:00:29,733
这里面会简单的去给大家

11
00:00:29,766 --> 00:00:32,233
逐个地介绍相关的 Pass

12
00:00:32,233 --> 00:00:35,333
首先第一种呢把它分成左右两边

13
00:00:35,333 --> 00:00:37,733
左边这种我把它定义为

14
00:00:37,733 --> 00:00:42,333
真正的面向图层的一些 IR 或者图层的一些 Pass 的优化

15
00:00:42,333 --> 00:00:47,433
右边的这种面向于更传统的一些 DAG 图的一些优化

16
00:00:47,433 --> 00:00:48,633
让看看左边

17
00:00:48,633 --> 00:00:51,099
会讲到算子的融合  就把算子合起来

18
00:00:51,200 --> 00:00:53,900
第二个呢就是布局的转换

19
00:00:53,966 --> 00:00:57,833
这个布局转换更多的是指内存的布局内存的排布

20
00:00:57,933 --> 00:01:00,433
第三个就是内存的分配

21
00:01:00,433 --> 00:01:04,899
这个跟刚才上面布局是不一样的上面是指内存的数据

22
00:01:04,900 --> 00:01:10,166
这个就是指内存的地址和内存的空间进行一个分配

23
00:01:10,166 --> 00:01:12,999
接着会有一些常量折叠

24
00:01:13,000 --> 00:01:15,600
常量折叠主要是讲神经网络里面

25
00:01:15,600 --> 00:01:17,500
可能会出现一些常量

26
00:01:17,700 --> 00:01:20,033
这个时候把它预先地计算了

27
00:01:20,100 --> 00:01:25,166
另外下面的几个概念传统编译器 LLVM 里面也会有

28
00:01:25,166 --> 00:01:27,033
这里面简单地带过一下

29
00:01:27,033 --> 00:01:32,066
就是有公共子表达式消除 死代码消除

30
00:01:32,066 --> 00:01:34,966
另外还会有代数简化

31
00:01:34,966 --> 00:01:39,266
下面将会分为这些内容去给大家介绍

32
00:01:39,300 --> 00:01:44,166
现在看看前端的优化的一个整体的框图

33
00:01:44,166 --> 00:01:47,499
那首先最上面的就是 AI 框架

34
00:01:47,500 --> 00:01:50,366
TensorFlow、Pytorch、MindSpore

35
00:01:50,366 --> 00:01:52,233
还有国内的一些 AI 框架

36
00:01:52,233 --> 00:01:56,799
这些 AI 框架最重要的一个事情就是产生一个计算图

37
00:01:56,800 --> 00:02:00,666
拿到这个计算图就会把它变成一个 Graph IR

38
00:02:00,666 --> 00:02:03,199
传给编译器的前端

39
00:02:03,200 --> 00:02:05,966
当然了编译器有前端中间优化还有后端

40
00:02:05,966 --> 00:02:09,299
这个在 LLVM 里面详细地介绍过的

41
00:02:09,300 --> 00:02:12,533
而今天主要是聚焦于前端优化

42
00:02:12,533 --> 00:02:14,233
那就会有非常多的 Pass

43
00:02:14,233 --> 00:02:17,966
每个 Pass、Pass 的概念在前面其实介绍过了

44
00:02:18,000 --> 00:02:20,233
所以它会有非常多的 Pass

45
00:02:20,233 --> 00:02:23,933
不同的 Pass 执行不同的优化逻辑

46
00:02:23,933 --> 00:02:29,199
接着看看在整体的 AI 编译器里面现在所处在哪个位置

47
00:02:29,200 --> 00:02:32,566
在最上层用 Python 写了一些代码

48
00:02:32,566 --> 00:02:35,933
经过 AI 框架对这些 Python 代码进行一个解析

49
00:02:36,333 --> 00:02:38,399
最后变成 Graph IR

50
00:02:38,400 --> 00:02:43,133
就是计算图的 IR 或者计算图所衍生出来的一个 IR

51
00:02:43,133 --> 00:02:44,966
IR 是中间表达

52
00:02:44,966 --> 00:02:48,599
接着就会有一个前端的优化那前端的优化

53
00:02:48,600 --> 00:02:51,600
那前端的优化就是刚才讲到的一些优化

54
00:02:51,600 --> 00:02:53,566
那接下来下面的内容

55
00:02:53,566 --> 00:02:58,566
将会在比较往后的章节去给大家分享汇报

56
00:02:58,566 --> 00:03:01,733
那现在来看看前端优化都做了哪些内容

57
00:03:01,733 --> 00:03:02,666
其实很简单

58
00:03:02,666 --> 00:03:07,366
前端优化输进去是一个类似于 Graph 的 IR 就是图层的 IR

59
00:03:07,366 --> 00:03:11,133
然后经过常量折叠之后输出一个图的 IR

60
00:03:11,133 --> 00:03:13,933
接着这个图的 IR 重新丢给 Pass

61
00:03:13,933 --> 00:03:18,399
然后经过常量传播或者算子融合得到另外一个新的 IR

62
00:03:18,400 --> 00:03:20,800
接着又重新传回来

63
00:03:20,800 --> 00:03:24,333
每一次传它都是一个图的表达、图的形式

64
00:03:24,333 --> 00:03:27,633
所以叫做图层的 IR

65
00:03:27,633 --> 00:03:30,399
好了今天的内容呢到这里为止

66
00:03:30,400 --> 00:03:35,200
后面想要了解更多 AI 编译器的内容请关注这门课程

67
00:03:35,200 --> 00:03:36,200
谢谢各位

